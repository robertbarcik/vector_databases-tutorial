{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# LlamaIndex\n",
        "\n",
        "LlamaIndex is a framework designed to help you build applications powered by Large Language Models such as chatbots, AI assistants, and translation tools. One of its most valuable capabilities is enriching the knowledge of your LLM with **your own data**, enabling the model to answer questions about **personal, organizational, or domain-specific information** that it wasn’t originally trained on.\n",
        "\n",
        "> NOTE:\n",
        "**Before you run this notebook**\n",
        "- You need an OpenAI API key available as `OPENAI_API_KEY`.\n",
        "- Cells using LlamaParse require a LlamaCloud API key (`LLAMA_CLOUD_API_KEY`).\n",
        "- If you do not have these keys, skip the LlamaParse/OpenAI sections and follow the local examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# Setup: Installing Required Libraries\n",
        "\n",
        "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "9b942a22-b468-41b6-b9c8-10c837077fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ All libraries installed successfully!\n",
            "⚠️  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries with working versions\n",
        "!pip install -q llama-index-core==0.14.6 llama-index-embeddings-openai==0.5.1 \\\n",
        "    llama-index-llms-openai==0.6.6 openai==1.109.1 \\\n",
        "    chromadb==1.2.2 llama-index-vector-stores-chroma==0.5.3 \\\n",
        "    llama-index-readers-file llama-parse\n",
        "\n",
        "print(\"✅ All libraries installed successfully!\")\n",
        "print(\"⚠️  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "# 1. Data Connectors\n",
        "\n",
        "LlamaIndex uses data connectors to **ingest information** from a wide range of **structured and unstructured sources**.\n",
        "\n",
        "The simplest way to load the data is using `SimpleDirectoryReader` which supports various file types such as:\n",
        "\n",
        "- csv - comma-separated values\n",
        "- docx - Microsoft Word\n",
        "- ipynb - Jupyter Notebook\n",
        "- pdf - Portable Document Format\n",
        "- ppt, .pptm, .pptx - Microsoft PowerPoint\n",
        "- ...and many more.\n",
        "\n",
        "Data connector takes your data from these different formats and put them together in a uniform, organized way so they can be used within your LLM application.\n",
        "\n",
        "You can find all supported file types in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#simpledirectoryreader).\n",
        "\n",
        "Let's import `SimpleDirectoryReader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4",
        "outputId": "115471a4-829a-4657-b2b9-221b6b006607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key loaded from Colab secrets\n",
            "✅ Authentication configured!\n",
            "🤖 Selected Model: gpt-5-nano\n",
            "🧠 Embedding Model: text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Configure OpenAI API key\n",
        "OPENAI_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print('✅ API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY')\n",
        "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError('❌ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
        "    raise ValueError('❌ ERROR: No API key provided!')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print('✅ Authentication configured!')\n",
        "\n",
        "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
        "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n",
        "\n",
        "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
        "print(f'🧠 Embedding Model: {OPENAI_EMBED_MODEL}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "We will load the PDF file called \"charter.pdf\" (stored in \"data\" folder in notebook's directory) containing the Charter of Fundamental Rights of the European Union.  \n",
        "\n",
        "> NOTE: In this notebook we will use the asynchronous (async) versions of data connectors using `await` and `.aload_data()`. It helps everything run more smoothly and prevents technical errors with the notebook’s event loop. You don’t need to understand all the internals, just know that `await` is the keyword that tells Python \"this step might take a while, pause here until it’s done\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "# Generating documents\n",
        "documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "When this data connector processes a PDF, it doesn’t treat the whole file as a single block of text. Instead, it splits the PDF into pages and each page is returned as **document object**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9",
      "metadata": {
        "id": "9",
        "outputId": "dffbb575-1f94-4291-b70d-ab9db263a069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# The number of pages in the original PDF file == The number of document objects\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "Let's display the text of the second document where we can see the Table of Contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "11",
      "metadata": {
        "id": "11",
        "outputId": "912c039d-8fbf-463d-f1ef-6c667ef983e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Table of Contents  \n",
            "Page \n",
            "PREAMBLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393  \n",
            "TITLE I DIGNITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394  \n",
            "TITLE II FREEDOMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395  \n",
            "TITLE III EQUALITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397  \n",
            "TITLE IV SOLIDARITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399  \n",
            "TITLE V CITIZENS' RIGHTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401  \n",
            "TITLE VI JUSTICE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403  \n",
            "TITLE VII GENERAL PROVISIONS GOVERNING THE INTERPRETATION AND \n",
            "APPLICATION OF THE CHARTER . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n",
            "EN C 202/390 Official Journal of the European Union 7.6.2016\n"
          ]
        }
      ],
      "source": [
        "print(documents[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "Each document include metadata such as `file_name`, `file_type`, `creation_date`, etc.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13",
      "metadata": {
        "id": "13",
        "outputId": "4aeef865-9228-4281-93c2-b1be0b5e4af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page_label': '390',\n",
              " 'file_name': 'charter.pdf',\n",
              " 'file_path': 'data/charter.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 1049657,\n",
              " 'creation_date': '2025-10-28',\n",
              " 'last_modified_date': '2025-10-28'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "documents[1].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "### 📝 EXERCISE 1: Load and Explore Documents\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Check how many document objects were created from the PDF file\n",
        "2. Display the text content of the first document\n",
        "3. Print the metadata for the first document\n",
        "4. Think about: Why is the PDF split into multiple document objects? What does each object represent?\n",
        "\n",
        "**Hint:**\n",
        "- Use `len(documents)` to count documents\n",
        "- Access properties with `documents[0].text` and `documents[0].metadata`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "# 2. Creating the Index and Querying\n",
        "Next, we’ll build a vector database to store our embeddings. We'll use `VectorStoreIndex.from_documents()` which automatically **breaks each document into smaller pieces called nodes** based on length. Each node keeps the metadata of its parent document, so we don’t lose context. Once the nodes are created, they are passed to an embedding model - `text-embedding-ada-002` from OpenAI by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Creating the index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "Next, we’ll turn the index into a query engine so that we can ask questions.\n",
        "\n",
        "Behind the scenes, the workflow looks like this:\n",
        "1. **Query Embedding**: Our text query is embedded into a vector\n",
        "2. **Retriever**: Query vector is compared against the embeddings stored in the index and retriever returns the most relevant nodes - LlamaIndex uses **cosine** similarity by default\n",
        "3. **Response Syntethizer**: Combines the retrieved nodes with our query to generate a prompt, which is then passed to an LLM to produce an answer - LlamaIndex uses `gpt-3.5-turbo` from OpenAI by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "19",
      "metadata": {
        "id": "19",
        "outputId": "e2998ef9-8539-4275-9b3e-48d09735d3a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title I is about fundamental human rights and dignity, emphasizing the protection and respect for human life, integrity, and prohibiting practices such as torture, slavery, discrimination, and ensuring equality between men and women.\n"
          ]
        }
      ],
      "source": [
        "# Setting the index as query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Querying\n",
        "print(query_engine.query(\"What is Title 1 about?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "# 3. Making Data Persistent\n",
        "\n",
        "By default, `VectorStoreIndex` keeps all data in memory. However, LlamaIndex has its own built-in persistence mechanism.\n",
        "\n",
        "We will use `persist()` method that handle saving the index into \"my_storage\". In the code cell below, if folder \"my_storage\" does not exist yet the code will:\n",
        "- load PDF file from \"data\" folder\n",
        "- build a new index\n",
        "- persist that index to disk inside \"my storage\"\n",
        "\n",
        "If folder \"my_storage\" already exists, the code instead:\n",
        "- creates `StorageContext` object pointing to this folder\n",
        "- reload the previously saved index directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# A directory\n",
        "PERSIST_DIR = \"./my_storage\"\n",
        "\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # Loading the documents and creating the index\n",
        "    documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Storing\n",
        "    index.storage_context.persist(persist_dir = PERSIST_DIR)\n",
        "else:\n",
        "    # Reloading the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "Now we can start running queries against it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "25",
      "metadata": {
        "id": "25",
        "outputId": "8fa5f2e0-ba57-4a0b-fa94-e1f77f89a5fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title II focuses on various freedoms. It includes the right to liberty and security, respect for private and family life, protection of personal data, the right to marry and found a family, freedom of thought, conscience, and religion.\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Can you summarize Title 2?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "source": [
        "### 📝 EXERCISE 2: Query Your Index\n",
        "\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Think of a question about the EU Charter document (e.g., \"What rights do children have?\", \"What is Article 10 about?\", \"What freedoms are protected?\")\n",
        "2. Query the index using your question\n",
        "3. Print the response\n",
        "4. Try a second question and compare the answers\n",
        "5. Think about: How does the answer quality depend on your question phrasing?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "# 4. LlamaParse\n",
        "\n",
        "If your dataset includes different file types or documents with complex layouts (such as tables, multi-column text or embedded images), you can use `LlamaParse`. This parser is part of LlamaCloud and is designed to convert documents into structured outputs while preserving layout features far more accurately than generic readers.\n",
        "\n",
        "To use this parser, you’ll first need **LlamaCloud account**. Go to www.llamaindex.ai and sign-up. Then navigate to **API keys** section and click **Generate New Key**. Be sure to copy and store this secret key in a safe place. For security reasons, it will not be shown again in your account.\n",
        "\n",
        "> NOTE: You can also make your LlamaParse API key and base URL load automatically every time your terminal starts. This way, you don’t have to set them manually in every session. Open your terminal and edit your shell file - type `nano ~/.zshrc`. At the end of the file, add the following lines. Then run `source ~/.zshrc`.\n",
        ">\n",
        "> `export LLAMA_CLOUD_API_KEY=\"YOUR_EU_KEY\"`\n",
        ">\n",
        "> `export LLAMA_CLOUD_API_BASE=\"api.cloud.eu.llamaindex.ai\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configure LlamaParse API key\n",
        "LLAMA_CLOUD_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    LLAMA_CLOUD_API_KEY = userdata.get('LLAMA_CLOUD_API_KEY')\n",
        "    if LLAMA_CLOUD_API_KEY:\n",
        "        print('✅ LlamaParse API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY:\n",
        "    LLAMA_CLOUD_API_KEY = os.getenv('LLAMA_CLOUD_API_KEY')\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: LLAMA_CLOUD_API_KEY')\n",
        "        LLAMA_CLOUD_API_KEY = getpass('Enter your LlamaParse API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError(\n",
        "            '❌ ERROR: No LlamaParse API key provided! Set LLAMA_CLOUD_API_KEY as an environment variable or Colab secret.'\n",
        "        ) from exc\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY or LLAMA_CLOUD_API_KEY.strip() == '':\n",
        "    raise ValueError('❌ ERROR: No LlamaParse API key provided!')\n",
        "\n",
        "os.environ['LLAMA_CLOUD_API_KEY'] = LLAMA_CLOUD_API_KEY\n",
        "\n",
        "print('✅ LlamaParse authentication configured!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3E2TQY9_4Vc",
        "outputId": "2b6d0bb5-7d05-479c-9d43-c5dec87a22e0"
      },
      "id": "t3E2TQY9_4Vc",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LlamaParse API key loaded from Colab secrets\n",
            "✅ LlamaParse authentication configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type = \"text\",\n",
        "    base_url = \"https://api.cloud.eu.llamaindex.ai\",  # Calling the EU LlamaCloud endpoint\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "28",
      "metadata": {
        "id": "28",
        "outputId": "2c0769ca-d673-4d29-d62b-cc3f0880f7d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 91e05b9c-4295-4482-bfdd-70bc0e97fb05\n"
          ]
        }
      ],
      "source": [
        "documents = await parser.aload_data(\"./data/charter.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "Let's again display the text of the second document - the parser preserves layout features like headings better than a simple text extractor like `SimpleDirectoryReader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "30",
      "metadata": {
        "id": "30",
        "outputId": "9de46c9c-2e35-4069-8ef4-6f43a529da14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "C 202/390  EN    Official Journal of the European Union    7.6.2016\n",
            "\n",
            "                 Table of Contents\n",
            "\n",
            "                                                                                                                                   Page\n",
            "\n",
            "PREAMBLE      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        393\n",
            "\n",
            "TITLE I       DIGNITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              394\n",
            "\n",
            "TITLE II      FREEDOMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                 395\n",
            "\n",
            "TITLE III     EQUALITY              . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    397\n",
            "\n",
            "TITLE IV      SOLIDARITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                 399\n",
            "\n",
            "TITLE V       CITIZENS' RIGHTS                  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    401\n",
            "\n",
            "TITLE VI      JUSTICE           . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    403\n",
            "\n",
            "TITLE VII     GENERAL  PROVISIONS GOVERNING THE INTERPRETATION AND\n",
            "              APPLICATION OF THE CHARTER . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                 404\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(documents[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "## 4.1 Using LlamaParse - PDF with tables into Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "source": [
        "Now let’s try `LlamaParse` on PDF called \"livestock_poultry.pdf\" that contains not only the text but also **several tables**. `LlamaParse` will return the content in **Markdown format** which makes the document far easier for an LLM to interpret.\n",
        "\n",
        "In the code cell below, we initialize the parser that connects to the LlamaCloud API - we need to set `base_url` that specifies which regional LlamaCloud endpoint to use. In this case, we’re pointing to the EU server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": [],
      "source": [
        "# Parsing PDF\n",
        "parser = LlamaParse(\n",
        "    result_type = \"markdown\",\n",
        "    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "source": [
        "Now we can send a PDF file to the parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "37",
      "metadata": {
        "id": "37",
        "outputId": "1328cd49-9db4-4ab8-e160-3579fd7569c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 65273faa-d7c1-4248-93d5-f2cbc2251196\n",
            "."
          ]
        }
      ],
      "source": [
        "pdf_doc = await parser.aload_data(\"./data/livestock_poultry.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "Let's print the document with index 8. Compare this Markdown output with the original PDF (page 9). Notice how the layout is preserved. This is what makes `LlamaParse` valuable: instead of flattening tables into plain text, it captures structure in a way that downstream models can use effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "39",
      "metadata": {
        "id": "39",
        "outputId": "ab86476c-1263-488e-f6d4-776f2660df66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cattle Stocks - Top Countries Summary\n",
            "\n",
            "# (in 1,000 head)\n",
            "\n",
            "# 1. Total Cattle Beg. Stks\n",
            "\n",
            "| Country        | 2021    | 2022    | 2023    | 2024    | 2025    | 2025    |\n",
            "| -------------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
            "| India          | 305,500 | 306,700 | 307,400 | 307,420 | 307,490 | 307,490 |\n",
            "| Brazil         | 193,195 | 193,780 | 194,365 | 192,572 | 186,875 | 186,875 |\n",
            "| China          | 95,621  | 98,172  | 102,160 | 105,090 | 104,000 | 104,900 |\n",
            "| European Union\n"
          ]
        }
      ],
      "source": [
        "preview = pdf_doc[8].text[:500]\n",
        "print(preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "source": [
        "### 📝 EXERCISE 3: Compare SimpleDirectoryReader vs LlamaParse\n",
        "\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Look at the parsed output from LlamaParse for the livestock_poultry.pdf document\n",
        "2. Display a different page/document from the parsed results\n",
        "3. Observe how tables and structured data are represented\n",
        "4. Think about: When would you use LlamaParse instead of SimpleDirectoryReader?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "## 4.2 Parsing different file types\n",
        "\n",
        "In this section, we’ll see how to use LlamaParse to handle documents of different types, such as PDFs and Word files, and bring them into a single search workflow.\n",
        "\n",
        "Instead of writing separate code for each format, we can map file extensions to the same parser and let `SimpleDirectoryReader` automatically process everything in a folder.\n",
        "\n",
        "First, we'll initialize a parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "parser = LlamaParse(result_type = \"markdown\",\n",
        "                    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
        "                    verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "Next, we'll map file extensions to the parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "file_extractor = {\n",
        "    \".pdf\": parser,\n",
        "    \".docx\": parser\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "source": [
        "Now we can tell `SimpleDirectoryReader` to scan a folder with files \"charter.pdf\", \"livestock_poultry.pdf\" and \"vacation_policy.docx\". If it finds a `.pdf` or `.docx`, it will use our parser to process it. The result is a list of document objects where each page or section is stored as Markdown text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "45",
      "metadata": {
        "id": "45",
        "outputId": "a2177f54-2788-49b1-c76b-f87733d6909c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id fbed2a59-b90f-4127-8b0f-a7aa7a231cc4\n",
            "Started parsing the file under job_id 2a247618-1d15-4b85-b0b1-8fb0fcca6f50\n",
            "Started parsing the file under job_id 44ec92ec-99ab-4866-916b-8c215900f689\n"
          ]
        }
      ],
      "source": [
        "documents = await SimpleDirectoryReader(\n",
        "    input_dir = \"./data/\",\n",
        "    file_extractor = file_extractor\n",
        ").aload_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "source": [
        "Now we are going to create embeddings for our documents. As we already know, when we build `VectorStoreIndex`, it automatically splits text into chunks before embedding, but this uses default settings.\n",
        "\n",
        "However, we can use `SentenceSplitter` to gain explicit control over how that chunking happens:\n",
        "- `chunk_size`: sets the maximum length of each chunk (keeps chunks small enough to fit into the embedding model and LLM context window)\n",
        "- `chunk_overlap`: defines how much content is repeated between consecutive chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "\n",
        "**Chunk size matters**\n",
        "\n",
        "When we embed an entire document as a single vector, we are effectively averaging all of its topics into one point in space. For multi-topic articles this produces a diluted signal: a query about \"zero trust\" might rank poorly because the vector also carries equally strong signals for other sections such as cryptography or phishing. Chunking breaks the document into focused segments so each embedding represents one coherent idea, dramatically improving retrieval precision.\n",
        "\n",
        "**Trade-offs**\n",
        "- *Chunks that are too large (1000+ tokens)* keep the full context but blend unrelated concepts, reducing similarity scores and hurting recall.\n",
        "- *Chunks that are too small (50 tokens)* deliver crisp matches but may lose the surrounding context the LLM needs when generating an answer.\n",
        "\n",
        "**Mitigation strategies**\n",
        "1. Start with a balanced window (e.g., 256–512 tokens) and adjust based on your corpus.\n",
        "2. Introduce overlap (e.g., 10–20% of the chunk size) so important sentences near boundaries appear in both neighbouring chunks.\n",
        "3. During retrieval, fetch neighbouring chunks or stitch together the original document spans so the LLM receives enough context to respond reliably.\n",
        "\n",
        "This approach preserves the semantic focus needed for accurate vector search while still giving the downstream LLM the broader context it needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Split into nodes (chunks)\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size = 512,        # each chunk will be about 512 characters/tokens long\n",
        "    chunk_overlap = 50)      # the last 50 characters/tokens of one chunk will also appear at the start of the next\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "The next step is to build a vector index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "# Creating embeddings from \"nodes\"\n",
        "index = VectorStoreIndex.from_documents(nodes)\n",
        "\n",
        "# Wrapping the index in a query engine\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "source": [
        "In the code cell below, the question is converted into a vector embedding which is compared against all stored embeddings (nodes) in the vector index. The nodes whose embeddings are most similar (highest cosine score) are selected as \"relevant\" and combined with the query and passed to an LLM to generate the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "52",
      "metadata": {
        "id": "52",
        "outputId": "66fab814-126b-4d48-e39b-8ad00ebf7538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15 days that can be carried over into the next calendar year.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"How many days can be carried over into the next calendar year?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "53",
      "metadata": {
        "id": "53",
        "outputId": "c191c20f-e983-478b-e6e9-9fe8431a7d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China, Philippines, Chile, Japan, and Hong Kong.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"What are brazil top five pork export markets?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "54",
      "metadata": {
        "id": "54",
        "outputId": "e4ab9bf7-b928-47bf-fe57-5295f9caaa42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The citizens' rights include the right to vote and stand as a candidate at elections to the European Parliament, the right to vote and stand as a candidate at municipal elections, the right to good administration, the right of access to documents, the right to refer cases of maladministration to the European Ombudsman, the right to petition the European Parliament, and the freedom of movement and residence within the territory of the Member States.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"What are the citizens' rights?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "## 4.5 Using different LLM\n",
        "\n",
        "Up to now, we’ve built a vector index using the default embedding model and the default LLM. But both of these can be customized. By default, LlamaIndex uses OpenAI’s `text-embedding-ada-002` for embeddings and `gpt-3.5-turbo` for the LLM.\n",
        "\n",
        "In the example below, we’ll rebuild our index with a different embedding model - `text-embedding-3-small`, and then use a different LLM - `gpt-5-nano` to generate answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Building a new index + new embedding model\n",
        "pdf_index = VectorStoreIndex.from_documents(\n",
        "    pdf_doc,\n",
        "    embedding = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "57",
      "metadata": {
        "id": "57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd9d0b5-58d0-4d4a-f0a6-189d46bd04ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A 1% decrease.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Using new LLM\n",
        "query_engine = index.as_query_engine(llm = OpenAI(model=OPENAI_MODEL))\n",
        "response = query_engine.query(\"What is the forecasted percentage change of global export of pork between 2024 and 2025?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pk2eJHjCBbBL"
      },
      "id": "pk2eJHjCBbBL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}