{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# LlamaIndex\n",
        "\n",
        "LlamaIndex is a framework designed to help you build applications powered by Large Language Models such as chatbots, AI assistants, and translation tools. One of its most valuable capabilities is enriching the knowledge of your LLM with **your own data**, enabling the model to answer questions about **personal, organizational, or domain-specific information** that it wasn’t originally trained on.\n",
        "\n",
        "> NOTE:\n",
        "**Before running any code in this notebook, you need to complete three quick\n",
        "   setup steps**:\n",
        "\n",
        "Step 1: Store Your API Keys in Colab Secrets 🔑\n",
        "\n",
        "  This notebook requires two API keys. Follow these steps to securely store\n",
        "  them:\n",
        "\n",
        "  1. Click the 🔑 icon in the left sidebar of Colab (it says \"Secrets\" when\n",
        "  you hover over it)\n",
        "  2. Add your OpenAI API key:\n",
        "    - Click \"Add new secret\"\n",
        "    - Name: OPENAI_API_KEY\n",
        "    - Value: Paste your OpenAI API key\n",
        "\n",
        "  3. Add your LlamaParse API key:\n",
        "    - Click \"Add new secret\" again\n",
        "    - Name: LLAMA_CLOUD_API_KEY\n",
        "    - Value: Paste your LlamaParse API key\n",
        "\n",
        "\n",
        "  📌 Note: If you don't have these API keys yet:\n",
        "  - OpenAI API key: Get it from https://platform.openai.com/api-keys\n",
        "  - LlamaParse API key: Get it from https://cloud.llamaindex.ai (sign up,\n",
        "  then go to API Keys section)\n",
        "\n",
        "  ---\n",
        "  Step 2: Create a Data Folder 📁\n",
        "\n",
        "  1. In the Colab file browser (left sidebar), you'll see\n",
        "  your current files\n",
        "  2. Right-click in the empty space\n",
        "  3. Select \"New folder\"\n",
        "  4. Name it exactly: data\n",
        "\n",
        "  ---\n",
        "  Step 3: Upload Documents to the Data Folder 📄\n",
        "\n",
        "  You need to upload three PDF documents to the data folder:\n",
        "\n",
        "  1. Download these files (from course materials):\n",
        "    - charter.pdf - Charter of Fundamental Rights of the European Union\n",
        "    - livestock_poultry.pdf - Livestock and poultry data with tables\n",
        "    - vacation_policy.docx - Sample vacation policy document\n",
        "  2. Upload them to Colab:\n",
        "    - Click on the data folder you just created\n",
        "    - Click the upload icon at the top of the file browser\n",
        "    - Select all three files and upload them\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# Setup: Installing Required Libraries\n",
        "\n",
        "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "7b59c887-7b45-4ba1-e9e9-2618aa41e261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ All libraries installed successfully!\n",
            "⚠️  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries with working versions\n",
        "!pip install -q llama-index-core==0.14.6 llama-index-embeddings-openai==0.5.1 \\\n",
        "    llama-index-llms-openai==0.6.6 openai==1.109.1 \\\n",
        "    chromadb==1.2.2 llama-index-vector-stores-chroma==0.5.3 \\\n",
        "    llama-index-readers-file llama-parse\n",
        "\n",
        "print(\"✅ All libraries installed successfully!\")\n",
        "print(\"⚠️  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "# 1. Data Connectors\n",
        "\n",
        "LlamaIndex uses data connectors to **ingest information** from a wide range of **structured and unstructured sources**.\n",
        "\n",
        "The simplest way to load the data is using `SimpleDirectoryReader` which supports various file types such as:\n",
        "\n",
        "- csv - comma-separated values\n",
        "- docx - Microsoft Word\n",
        "- ipynb - Jupyter Notebook\n",
        "- pdf - Portable Document Format\n",
        "- ppt, .pptm, .pptx - Microsoft PowerPoint\n",
        "- ...and many more.\n",
        "\n",
        "Data connector takes your data from these different formats and put them together in a uniform, organized way so they can be used within your LLM application.\n",
        "\n",
        "You can find all supported file types in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#simpledirectoryreader).\n",
        "\n",
        "Let's import `SimpleDirectoryReader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4",
        "outputId": "0a469578-1040-40dd-d7b3-51807149df2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key loaded from Colab secrets\n",
            "✅ Authentication configured!\n",
            "🤖 Selected Model: gpt-5-nano\n",
            "🧠 Embedding Model: text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Configure OpenAI API key\n",
        "OPENAI_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print('✅ API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY')\n",
        "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError('❌ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
        "    raise ValueError('❌ ERROR: No API key provided!')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print('✅ Authentication configured!')\n",
        "\n",
        "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
        "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n",
        "\n",
        "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
        "print(f'🧠 Embedding Model: {OPENAI_EMBED_MODEL}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "We will load the PDF file called \"charter.pdf\" (stored in \"data\" folder in notebook's directory) containing the Charter of Fundamental Rights of the European Union.  \n",
        "\n",
        "> NOTE: In this notebook we will use the asynchronous (async) versions of data connectors using `await` and `.aload_data()`. It helps everything run more smoothly and prevents technical errors with the notebook’s event loop. You don’t need to understand all the internals, just know that `await` is the keyword that tells Python \"this step might take a while, pause here until it’s done\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "# Generating documents\n",
        "documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "When this data connector processes a PDF, it doesn’t treat the whole file as a single block of text. Instead, it splits the PDF into pages and each page is returned as **document object**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9",
      "metadata": {
        "id": "9",
        "outputId": "35c5cab2-87a6-4e07-941b-c9daeb8e9234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# The number of pages in the original PDF file == The number of document objects\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "Let's display the text of the second document where we can see the Table of Contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11",
      "metadata": {
        "id": "11",
        "outputId": "f422b77f-fbf4-42c2-b7ba-e185c9b4dd17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Table of Contents  \n",
            "Page \n",
            "PREAMBLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393  \n",
            "TITLE I DIGNITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394  \n",
            "TITLE II FREEDOMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395  \n",
            "TITLE III EQUALITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397  \n",
            "TITLE IV SOLIDARITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399  \n",
            "TITLE V CITIZENS' RIGHTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401  \n",
            "TITLE VI JUSTICE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403  \n",
            "TITLE VII GENERAL PROVISIONS GOVERNING THE INTERPRETATION AND \n",
            "APPLICATION OF THE CHARTER . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404\n",
            "EN C 202/390 Official Journal of the European Union 7.6.2016\n"
          ]
        }
      ],
      "source": [
        "print(documents[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "Each document include metadata such as `file_name`, `file_type`, `creation_date`, etc.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13",
      "metadata": {
        "id": "13",
        "outputId": "25c5a04a-1749-4658-cfc5-6923868000f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page_label': '390',\n",
              " 'file_name': 'charter.pdf',\n",
              " 'file_path': 'data/charter.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 1049657,\n",
              " 'creation_date': '2025-10-28',\n",
              " 'last_modified_date': '2025-10-28'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "documents[1].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "### 📝 EXERCISE 1: Load and Explore Documents\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Check how many document objects were created from the PDF file\n",
        "2. Display the text content of the first document\n",
        "3. Print the metadata for the first document\n",
        "4. Think about: Why is the PDF split into multiple document objects? What does each object represent?\n",
        "\n",
        "**Hint:**\n",
        "- Use `len(documents)` to count documents\n",
        "- Access properties with `documents[0].text` and `documents[0].metadata`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "# 2. Creating the Index and Querying\n",
        "Next, we’ll build a vector database to store our embeddings. We'll use `VectorStoreIndex.from_documents()` which automatically **breaks each document into smaller pieces called nodes** based on length. Each node keeps the metadata of its parent document, so we don’t lose context. Once the nodes are created, they are passed to an embedding model - `text-embedding-ada-002` from OpenAI by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Creating the index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "Next, we’ll turn the index into a query engine so that we can ask questions.\n",
        "\n",
        "Behind the scenes, the workflow looks like this:\n",
        "1. **Query Embedding**: Our text query is embedded into a vector\n",
        "2. **Retriever**: Query vector is compared against the embeddings stored in the index and retriever returns the most relevant nodes - LlamaIndex uses **cosine** similarity by default\n",
        "3. **Response Syntethizer**: Combines the retrieved nodes with our query to generate a prompt, which is then passed to an LLM to produce an answer - LlamaIndex uses `gpt-3.5-turbo` from OpenAI by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "19",
      "metadata": {
        "id": "19",
        "outputId": "a73c7251-b0f1-49dc-c95d-c20348e2e409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title I is about fundamental human rights and dignity, emphasizing the protection and respect for human life, integrity, and prohibiting practices such as torture, slavery, discrimination, and ensuring equality between men and women.\n"
          ]
        }
      ],
      "source": [
        "# Setting the index as query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Querying\n",
        "response = query_engine.query(\"What is Title 1 about?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zpr10bn1s9d",
      "source": [
        "## 2.1 Understanding Retrieval: What's Happening Behind the Scenes?\n",
        "\n",
        "In the previous example, we asked a question and received an answer. Let's understand this workflow:\n",
        "\n",
        "\n",
        "**1. Embedding the Query**\n",
        "Our question (\"What is Title 1 about?\") is converted into a numerical vector (embedding) using the same embedding model that was used to embed documents. This ensures the query and documents exist in the same semantic space and can be meaningfully compared.\n",
        "\n",
        "**2. Retrieval (The \"R\" in RAG)**\n",
        "LlamaIndex compares the query embedding against all the chunk embeddings stored in the index using cosine similarity. It then retrieves the most semantically similar chunks—these are the pieces of documents that are most likely to contain the answer to our question.\n",
        "\n",
        "By default, LlamaIndex retrieves the **top 2 most similar chunks**. These chunks become the \"context\" that will be sent to the LLM.\n",
        "\n",
        "**3. Generation (The \"G\" in RAG)**\n",
        "The retrieved chunks are combined with our original question and sent to the LLM in a prompt that essentially says: \"Here are some relevant document excerpts. Based ONLY on these excerpts, answer the following question.\"\n",
        "\n",
        "The LLM reads the context, synthesizes the information, and generates a natural language answer.\n",
        "\n",
        "\n",
        "\n",
        "Let's see this retrieval process in action."
      ],
      "metadata": {
        "id": "zpr10bn1s9d"
      }
    },
    {
      "cell_type": "code",
      "id": "os3lg8g447r",
      "source": [
        "# Query the index and inspect what was retrieved\n",
        "query_text = \"What is Title 1 about?\"\n",
        "response = query_engine.query(query_text)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUESTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(query_text)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL ANSWER FROM LLM:\")\n",
        "print(\"=\" * 80)\n",
        "print(response)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RETRIEVED CHUNKS (What the LLM actually saw as context):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Inspect the source nodes (retrieved chunks)\n",
        "for i, node in enumerate(response.source_nodes, 1):\n",
        "    print(f\"\\n📄 CHUNK {i}\")\n",
        "    print(f\"   Relevance Score: {node.score:.4f} (higher = more similar to query)\")\n",
        "    print(f\"   Source: {node.metadata.get('file_name', 'N/A')} | Page: {node.metadata.get('page_label', 'N/A')}\")\n",
        "    print(f\"   Text Preview (first 300 chars):\")\n",
        "    print(f\"   {node.text[:300]}...\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os3lg8g447r",
        "outputId": "f6d27404-2bd9-48e4-b277-c50c716496ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "QUESTION:\n",
            "================================================================================\n",
            "What is Title 1 about?\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FINAL ANSWER FROM LLM:\n",
            "================================================================================\n",
            "Title I is about fundamental human rights and dignity, emphasizing the protection and respect for human life, integrity, and prohibiting practices such as torture, slavery, discrimination, and ensuring equality between men and women.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RETRIEVED CHUNKS (What the LLM actually saw as context):\n",
            "================================================================================\n",
            "\n",
            "📄 CHUNK 1\n",
            "   Relevance Score: 0.7727 (higher = more similar to query)\n",
            "   Source: charter.pdf | Page: 394\n",
            "   Text Preview (first 300 chars):\n",
            "   TITLE I  \n",
            "DIGNITY \n",
            "Article 1  \n",
            "Human dignity  \n",
            "Human dignity is inviolable. It must be respected and protected.  \n",
            "Article 2  \n",
            "Right to life  \n",
            "1. Everyone has the right to life.  \n",
            "2. No one shall be condemned to the death penalty, or executed.  \n",
            "Article 3  \n",
            "Right to the integrity of the person  \n",
            "1. E...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "📄 CHUNK 2\n",
            "   Relevance Score: 0.7634 (higher = more similar to query)\n",
            "   Source: charter.pdf | Page: 398\n",
            "   Text Preview (first 300 chars):\n",
            "   Article 21  \n",
            "Non-discrimination \n",
            "1. Any discrimination based on any ground such as sex, race, colour, ethnic or social origin, \n",
            "genetic features, language, religion or belief, political or any other opinion, membership of a national \n",
            "minority, property, birth, disability, age or sexual orientation s...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unz8krtpnrr",
      "source": [
        "### Key Observations\n",
        "\n",
        "From the output above, notice several important things:\n",
        "\n",
        "1. **The LLM's answer is grounded in specific chunks** - The answer didn't come from the LLM's training data. It came from the chunks that were retrieved from your document.\n",
        "\n",
        "2. **Relevance scores guide retrieval** - Each chunk has a similarity score (typically between 0 and 1). Higher scores mean the chunk is more semantically similar to our query. LlamaIndex uses these scores to rank chunks and select the most relevant ones.\n",
        "\n",
        "3. **Metadata is preserved** - Each chunk remembers where it came from (file name, page number, etc.). This is crucial for citation and traceability.\n",
        "\n",
        "4. **Chunks are contextual excerpts** - Notice that each chunk is a portion of a document, not the entire document. This is why chunking strategy (which we'll explore more later) is so important: chunks must be large enough to contain meaningful information but small enough to be focused and relevant.\n",
        "\n"
      ],
      "metadata": {
        "id": "unz8krtpnrr"
      }
    },
    {
      "cell_type": "markdown",
      "id": "lfzaaapmtd8",
      "source": [
        "### Controlling Retrieval: The `similarity_top_k` Parameter\n",
        "\n",
        "By default, LlamaIndex retrieves the **top 2** most similar chunks. But you can control this behavior using the `similarity_top_k` parameter when creating your query engine.\n",
        "\n",
        "**Trade-offs:**\n",
        "- **Fewer chunks (k=1-2):** Faster, more focused, but might miss relevant information\n",
        "- **More chunks (k=5-10):** Better coverage, but more noise and higher LLM costs (more tokens to process)\n",
        "\n",
        "Let's experiment with different values:"
      ],
      "metadata": {
        "id": "lfzaaapmtd8"
      }
    },
    {
      "cell_type": "code",
      "id": "0vet0cppy1hp",
      "source": [
        "# Create a query engine that retrieves top 5 chunks instead of default 2\n",
        "query_engine_expanded = index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "query_text = \"What freedoms are protected in the EU Charter?\"\n",
        "response = query_engine_expanded.query(query_text)\n",
        "\n",
        "print(f\"QUESTION: {query_text}\\n\")\n",
        "print(f\"ANSWER: {response}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Retrieved {len(response.source_nodes)} chunks:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, node in enumerate(response.source_nodes, 1):\n",
        "    print(f\"\\nChunk {i} | Score: {node.score:.4f} | Page: {node.metadata.get('page_label', 'N/A')}\")\n",
        "    print(f\"Preview: {node.text[:150]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vet0cppy1hp",
        "outputId": "76733e98-e583-47f6-c6d6-879fd5339e19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: What freedoms are protected in the EU Charter?\n",
            "\n",
            "ANSWER: Freedom of expression, freedom of assembly and association, freedom of the arts and sciences, right to education, freedom to choose an occupation and right to engage in work, right to liberty and security, respect for private and family life, protection of personal data, right to marry and right to found a family, freedom of thought, conscience and religion.\n",
            "\n",
            "================================================================================\n",
            "Retrieved 5 chunks:\n",
            "================================================================================\n",
            "\n",
            "Chunk 1 | Score: 0.8797 | Page: 397\n",
            "Preview: Article 16  \n",
            "Freedom to conduct a business  \n",
            "The freedom to conduct a business in accordance with Union law and national laws and practices \n",
            "is recogn...\n",
            "\n",
            "Chunk 2 | Score: 0.8752 | Page: 393\n",
            "Preview: The European Parliament, the Council and the Commission solemnly proclaim the following text as \n",
            "the Charter of Fundamental Rights of the European Uni...\n",
            "\n",
            "Chunk 3 | Score: 0.8704 | Page: 405\n",
            "Preview: 5. The provisions of this Charter which contain principles may be implemented by legislative and \n",
            "executive acts taken by institutions, bodies, office...\n",
            "\n",
            "Chunk 4 | Score: 0.8665 | Page: 396\n",
            "Preview: Article 11  \n",
            "Freedom of expression and information  \n",
            "1. Everyone has the right to freedom of expression. This right shall include freedom to hold \n",
            "opi...\n",
            "\n",
            "Chunk 5 | Score: 0.8664 | Page: 395\n",
            "Preview: TITLE II  \n",
            "FREEDOMS \n",
            "Article 6  \n",
            "Right to liberty and security  \n",
            "Everyone has the right to liberty and security of person.  \n",
            "Article 7  \n",
            "Respect for p...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8o1j57j0wk",
      "source": [
        "**💡 Practical Tip:** Start with the default (`similarity_top_k=2`) and only increase it if you notice that answers are incomplete or missing information that you know exists in your documents. You can always inspect the retrieved chunks to diagnose whether retrieval is the problem."
      ],
      "metadata": {
        "id": "b8o1j57j0wk"
      }
    },
    {
      "cell_type": "markdown",
      "id": "n4iv9i6ol1g",
      "source": [
        "### 📝 EXERCISE 2.1: Inspect Retrieval for Your Own Query\n",
        "\n",
        "**What you'll practice:** Understanding the retrieval process by examining which chunks are selected for different queries.\n",
        "\n",
        "**Your task:**\n",
        "1. Create a query about something specific in the EU Charter (e.g., \"What are children's rights?\", \"What does Article 8 say?\")\n",
        "2. Use the query engine to get an answer\n",
        "3. Inspect the retrieved chunks using `response.source_nodes`\n",
        "4. Print the relevance scores and text previews for each chunk\n",
        "5. Think about: Do the retrieved chunks actually contain the information needed to answer your question? Are the scores reasonable?\n",
        "\n",
        "**Expected outcome:** You'll see exactly which parts of your documents were used to generate the answer, helping you understand whether the retrieval stage is working correctly."
      ],
      "metadata": {
        "id": "n4iv9i6ol1g"
      }
    },
    {
      "cell_type": "code",
      "id": "e4q77ci3l29",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "e4q77ci3l29"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "# 3. Making Data Persistent\n",
        "\n",
        "By default, `VectorStoreIndex` keeps all data in memory. However, LlamaIndex has its own built-in persistence mechanism.\n",
        "\n",
        "We will use `persist()` method that handle saving the index into \"my_storage\". In the code cell below, if folder \"my_storage\" does not exist yet the code will:\n",
        "- load PDF file from \"data\" folder\n",
        "- build a new index\n",
        "- persist that index to disk inside \"my storage\"\n",
        "\n",
        "If folder \"my_storage\" already exists, the code instead:\n",
        "- creates `StorageContext` object pointing to this folder\n",
        "- reload the previously saved index directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# A directory\n",
        "PERSIST_DIR = \"./my_storage\"\n",
        "\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # Loading the documents and creating the index\n",
        "    documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Storing\n",
        "    index.storage_context.persist(persist_dir = PERSIST_DIR)\n",
        "else:\n",
        "    # Reloading the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "Now we can start running queries against it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "25",
      "metadata": {
        "id": "25",
        "outputId": "a60dd569-b7b0-456c-ff31-e90d441c689d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title II focuses on freedoms. It includes articles on the right to liberty and security, respect for private and family life, protection of personal data, the right to marry and found a family, freedom of thought, conscience, and religion.\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Can you summarize Title 2?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "source": [
        "### 📝 EXERCISE 2: Query Your Index\n",
        "\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Think of a question about the EU Charter document (e.g., \"What rights do children have?\", \"What is Article 10 about?\", \"What freedoms are protected?\")\n",
        "2. Query the index using your question\n",
        "3. Print the response\n",
        "4. Try a second question and compare the answers\n",
        "5. Think about: How does the answer quality depend on your question phrasing?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "# 4. LlamaParse\n",
        "\n",
        "If your dataset includes different file types or documents with complex layouts (such as tables, multi-column text or embedded images), you can use `LlamaParse`. This parser is part of LlamaCloud and is designed to convert documents into structured outputs while preserving layout features far more accurately than generic readers.\n",
        "\n",
        "To use this parser, you’ll first need **LlamaCloud account**. Go to www.llamaindex.ai and sign-up. Then navigate to **API keys** section and click **Generate New Key**. Be sure to copy and store this secret key in a safe place. For security reasons, it will not be shown again in your account.\n",
        "\n",
        "> NOTE: You can also make your LlamaParse API key and base URL load automatically every time your terminal starts. This way, you don’t have to set them manually in every session. Open your terminal and edit your shell file - type `nano ~/.zshrc`. At the end of the file, add the following lines. Then run `source ~/.zshrc`.\n",
        ">\n",
        "> `export LLAMA_CLOUD_API_KEY=\"YOUR_EU_KEY\"`\n",
        ">\n",
        "> `export LLAMA_CLOUD_API_BASE=\"api.cloud.eu.llamaindex.ai\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configure LlamaParse API key\n",
        "LLAMA_CLOUD_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    LLAMA_CLOUD_API_KEY = userdata.get('LLAMA_CLOUD_API_KEY')\n",
        "    if LLAMA_CLOUD_API_KEY:\n",
        "        print('✅ LlamaParse API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY:\n",
        "    LLAMA_CLOUD_API_KEY = os.getenv('LLAMA_CLOUD_API_KEY')\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: LLAMA_CLOUD_API_KEY')\n",
        "        LLAMA_CLOUD_API_KEY = getpass('Enter your LlamaParse API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError(\n",
        "            '❌ ERROR: No LlamaParse API key provided! Set LLAMA_CLOUD_API_KEY as an environment variable or Colab secret.'\n",
        "        ) from exc\n",
        "\n",
        "if not LLAMA_CLOUD_API_KEY or LLAMA_CLOUD_API_KEY.strip() == '':\n",
        "    raise ValueError('❌ ERROR: No LlamaParse API key provided!')\n",
        "\n",
        "os.environ['LLAMA_CLOUD_API_KEY'] = LLAMA_CLOUD_API_KEY\n",
        "\n",
        "print('✅ LlamaParse authentication configured!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3E2TQY9_4Vc",
        "outputId": "3c61afc4-7128-44ec-83f9-0cc27ff1269e"
      },
      "id": "t3E2TQY9_4Vc",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LlamaParse API key loaded from Colab secrets\n",
            "✅ LlamaParse authentication configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type = \"text\",\n",
        "    base_url = \"https://api.cloud.eu.llamaindex.ai\",  # Calling the EU LlamaCloud endpoint\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "28",
      "metadata": {
        "id": "28",
        "outputId": "092342b6-224e-43af-eb47-276306607f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id e1d5779b-b7f3-4dd1-90b3-e3b0daea563d\n"
          ]
        }
      ],
      "source": [
        "documents = await parser.aload_data(\"./data/charter.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "Let's again display the text of the second document - the parser preserves layout features like headings better than a simple text extractor like `SimpleDirectoryReader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "30",
      "metadata": {
        "id": "30",
        "outputId": "11274867-cb8d-4656-dfa6-17d500762a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "C 202/390  EN    Official Journal of the European Union    7.6.2016\n",
            "\n",
            "                 Table of Contents\n",
            "\n",
            "                                                                                                                                   Page\n",
            "\n",
            "PREAMBLE      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        393\n",
            "\n",
            "TITLE I       DIGNITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              394\n",
            "\n",
            "TITLE II      FREEDOMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                 395\n",
            "\n",
            "TITLE III     EQUALITY              . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    397\n",
            "\n",
            "TITLE IV      SOLIDARITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                 399\n",
            "\n",
            "TITLE V       CITIZENS' RIGHTS                  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    401\n",
            "\n",
            "TITLE VI      JUSTICE           . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    403\n",
            "\n",
            "TITLE VII     GENERAL  PROVISIONS GOVERNING THE INTERPRETATION AND\n",
            "              APPLICATION OF THE CHARTER . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                 404\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(documents[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "## 4.1 Using LlamaParse - PDF with tables into Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "source": [
        "Now let’s try `LlamaParse` on PDF called \"livestock_poultry.pdf\" that contains not only the text but also **several tables**. `LlamaParse` will return the content in **Markdown format** which makes the document far easier for an LLM to interpret.\n",
        "\n",
        "In the code cell below, we initialize the parser that connects to the LlamaCloud API - we need to set `base_url` that specifies which regional LlamaCloud endpoint to use. In this case, we’re pointing to the EU server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": [],
      "source": [
        "# Parsing PDF\n",
        "parser = LlamaParse(\n",
        "    result_type = \"markdown\",\n",
        "    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "source": [
        "Now we can send a PDF file to the parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "37",
      "metadata": {
        "id": "37",
        "outputId": "11ccac1c-23fa-481c-8887-fda711fff411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 4723152b-c798-4c7d-8b81-b7daebae5538\n"
          ]
        }
      ],
      "source": [
        "pdf_doc = await parser.aload_data(\"./data/livestock_poultry.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "Let's print the document with index 8. Compare this Markdown output with the original PDF (page 9). Notice how the layout is preserved. This is what makes `LlamaParse` valuable: instead of flattening tables into plain text, it captures structure in a way that downstream models can use effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "39",
      "metadata": {
        "id": "39",
        "outputId": "15821491-4d5b-4e27-c6d3-94fd2aa75fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cattle Stocks - Top Countries Summary\n",
            "\n",
            "# (in 1,000 head)\n",
            "\n",
            "# 1. Total Cattle Beg. Stks\n",
            "\n",
            "| Country        | 2021    | 2022    | 2023    | 2024    | 2025    | 2025    |\n",
            "| -------------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
            "| India          | 305,500 | 306,700 | 307,400 | 307,420 | 307,490 | 307,490 |\n",
            "| Brazil         | 193,195 | 193,780 | 194,365 | 192,572 | 186,875 | 186,875 |\n",
            "| China          | 95,621  | 98,172  | 102,160 | 105,090 | 104,000 | 104,900 |\n",
            "| European Union\n"
          ]
        }
      ],
      "source": [
        "preview = pdf_doc[8].text[:500]\n",
        "print(preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "source": [
        "### 📝 EXERCISE 3: Compare SimpleDirectoryReader vs LlamaParse\n",
        "\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Look at the parsed output from LlamaParse for the livestock_poultry.pdf document\n",
        "2. Display a different page/document from the parsed results\n",
        "3. Observe how tables and structured data are represented\n",
        "4. Think about: When would you use LlamaParse instead of SimpleDirectoryReader?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "## 4.2 Parsing different file types\n",
        "\n",
        "In this section, we’ll see how to use LlamaParse to handle documents of different types, such as PDFs and Word files, and bring them into a single search workflow.\n",
        "\n",
        "Instead of writing separate code for each format, we can map file extensions to the same parser and let `SimpleDirectoryReader` automatically process everything in a folder.\n",
        "\n",
        "First, we'll initialize a parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "parser = LlamaParse(result_type = \"markdown\",\n",
        "                    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
        "                    verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "Next, we'll map file extensions to the parser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "file_extractor = {\n",
        "    \".pdf\": parser,\n",
        "    \".docx\": parser\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "source": [
        "Now we can tell `SimpleDirectoryReader` to scan a folder with files \"charter.pdf\", \"livestock_poultry.pdf\" and \"vacation_policy.docx\". If it finds a `.pdf` or `.docx`, it will use our parser to process it. The result is a list of document objects where each page or section is stored as Markdown text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "45",
      "metadata": {
        "id": "45",
        "outputId": "a26bf849-256d-4ec2-8036-7670b801c8b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 4fb5fe16-bb08-4eeb-a63c-5faa2db84e87\n",
            "Started parsing the file under job_id 8268e283-29e2-4fae-bbcc-26d5a1e2d04e\n",
            "Started parsing the file under job_id 5fcf4d7a-f68f-4930-a594-1c14640b68c7\n"
          ]
        }
      ],
      "source": [
        "documents = await SimpleDirectoryReader(\n",
        "    input_dir = \"./data/\",\n",
        "    file_extractor = file_extractor\n",
        ").aload_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "source": [
        "Now we are going to create embeddings for our documents. As we already know, when we build `VectorStoreIndex`, it automatically splits text into chunks before embedding, but this uses default settings.\n",
        "\n",
        "However, we can use `SentenceSplitter` to gain explicit control over how that chunking happens:\n",
        "- `chunk_size`: sets the maximum length of each chunk (keeps chunks small enough to fit into the embedding model and LLM context window)\n",
        "- `chunk_overlap`: defines how much content is repeated between consecutive chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "\n",
        "**Chunk size matters**\n",
        "\n",
        "When we embed an entire document as a single vector, we are effectively averaging all of its topics into one point in space. For multi-topic articles this produces a diluted signal: a query about \"zero trust\" might rank poorly because the vector also carries equally strong signals for other sections such as cryptography or phishing. Chunking breaks the document into focused segments so each embedding represents one coherent idea, dramatically improving retrieval precision.\n",
        "\n",
        "**Trade-offs**\n",
        "- *Chunks that are too large (1000+ tokens)* keep the full context but blend unrelated concepts, reducing similarity scores and hurting recall.\n",
        "- *Chunks that are too small (50 tokens)* deliver crisp matches but may lose the surrounding context the LLM needs when generating an answer.\n",
        "\n",
        "**Mitigation strategies**\n",
        "1. Start with a balanced window (e.g., 256–512 tokens) and adjust based on your corpus.\n",
        "2. Introduce overlap (e.g., 10–20% of the chunk size) so important sentences near boundaries appear in both neighbouring chunks.\n",
        "3. During retrieval, fetch neighbouring chunks or stitch together the original document spans so the LLM receives enough context to respond reliably.\n",
        "\n",
        "This approach preserves the semantic focus needed for accurate vector search while still giving the downstream LLM the broader context it needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Split into nodes (chunks)\n",
        "splitter = SentenceSplitter(\n",
        "    chunk_size = 512,        # each chunk will be about 512 characters/tokens long\n",
        "    chunk_overlap = 50)      # the last 50 characters/tokens of one chunk will also appear at the start of the next\n",
        "\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "The next step is to build a vector index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "# Creating embeddings from \"nodes\"\n",
        "index = VectorStoreIndex.from_documents(nodes)\n",
        "\n",
        "# Wrapping the index in a query engine\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "source": [
        "In the code cell below, the question is converted into a vector embedding which is compared against all stored embeddings (nodes) in the vector index. The nodes whose embeddings are most similar (highest cosine score) are selected as \"relevant\" and combined with the query and passed to an LLM to generate the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "52",
      "metadata": {
        "id": "52",
        "outputId": "d70cb924-9294-4231-8d20-31d85756505d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of days that can be carried over into the next calendar year is 15 days.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"How many days can be carried over into the next calendar year?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "53",
      "metadata": {
        "id": "53",
        "outputId": "80fc8d67-1310-4245-b166-6ece74cbbc58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China, Philippines, Chile, Japan, and Hong Kong.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"What are brazil top five pork export markets?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "54",
      "metadata": {
        "id": "54",
        "outputId": "96b4e6c4-9446-4965-f611-17212b804d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The citizens' rights include the right to vote and stand as a candidate at elections to the European Parliament and municipal elections, the right to good administration, the right of access to documents, the right to refer cases of maladministration to the European Ombudsman, the right to petition the European Parliament, and the freedom of movement and residence within the territory of the Member States.\n"
          ]
        }
      ],
      "source": [
        "# Running the query\n",
        "print(query_engine.query(\"What are the citizens' rights?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "## 4.5 Using different LLM\n",
        "\n",
        "Up to now, we’ve built a vector index using the default embedding model and the default LLM. But both of these can be customized. By default, LlamaIndex uses OpenAI’s `text-embedding-ada-002` for embeddings and `gpt-3.5-turbo` for the LLM.\n",
        "\n",
        "In the example below, we’ll rebuild our index with a different embedding model - `text-embedding-3-small`, and then use a different LLM - `gpt-5-nano` to generate answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Building a new index + new embedding model\n",
        "pdf_index = VectorStoreIndex.from_documents(\n",
        "    pdf_doc,\n",
        "    embedding = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "57",
      "metadata": {
        "id": "57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deba6d50-dbbf-4fe8-c8ac-60fff8e9fa65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1% (a decline of about one percent from 2024 to 2025).\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Using new LLM\n",
        "query_engine = index.as_query_engine(llm = OpenAI(model=OPENAI_MODEL))\n",
        "response = query_engine.query(\"What is the forecasted percentage change of global export of pork between 2024 and 2025?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pk2eJHjCBbBL"
      },
      "id": "pk2eJHjCBbBL",
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}