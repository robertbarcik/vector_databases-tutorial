{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "57a550dd-cadd-4895-9967-9e42c3a849c3"
   },
   "source": [
    "# LlamaIndex\n",
    "\n",
    "LlamaIndex is a framework designed to help you build applications powered by Large Language Models such as chatbots, AI assistants, and translation tools. One of its most valuable capabilities is enriching the knowledge of your LLM with **your own data**, enabling the model to answer questions about **personal, organizational, or domain-specific information** that it wasn’t originally trained on.\n",
    "\n",
    "::: note\n",
    "**Before you run this notebook**\n",
    "- You need an OpenAI API key available as `OPENAI_API_KEY`.\n",
    "- Cells using LlamaParse require a LlamaCloud API key (`LLAMA_CLOUD_API_KEY`).\n",
    "- If you do not have these keys, skip the LlamaParse/OpenAI sections and follow the local examples.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "733ee605-2d9c-4de0-990a-f039e812513c"
   },
   "source": [
    "# 1. Data Connectors\n",
    "\n",
    "LlamaIndex uses data connectors to **ingest information** from a wide range of **structured and unstructured sources**.\n",
    "\n",
    "The simplest way to load the data is using `SimpleDirectoryReader` which supports various file types such as:\n",
    "\n",
    "- csv - comma-separated values\n",
    "- docx - Microsoft Word\n",
    "- ipynb - Jupyter Notebook\n",
    "- pdf - Portable Document Format\n",
    "- ppt, .pptm, .pptx - Microsoft PowerPoint\n",
    "- ...and many more.\n",
    "\n",
    "Data connector takes your data from these different formats and put them together in a uniform, organized way so they can be used within your LLM application.\n",
    "\n",
    "You can find all supported file types in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#simpledirectoryreader).\n",
    "\n",
    "Let's import `SimpleDirectoryReader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('✅ API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('❌ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('❌ ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('✅ Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'🧠 Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3cf0f9d3-0997-485f-8423-d15a81803388"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "17628016-069e-4ba8-b631-c02b0cebcf58"
   },
   "source": [
    "We will load the PDF file called \"charter.pdf\" (stored in \"data\" folder in notebook's directory) containing the Charter of Fundamental Rights of the European Union.  \n",
    "\n",
    "> NOTE: In this notebook we will use the asynchronous (async) versions of data connectors using `await` and `.aload_data()`. It helps everything run more smoothly and prevents technical errors with the notebook’s event loop. You don’t need to understand all the internals, just know that `await` is the keyword that tells Python \"this step might take a while, pause here until it’s done\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "e099daaa-3f6a-4ec7-bf14-07b98c924fa9"
   },
   "outputs": [],
   "source": [
    "# Generating documents\n",
    "documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "2af3a19a-770a-4d0a-8ff7-9ea41b9773a8"
   },
   "source": [
    "When this data connector processes a PDF, it doesn’t treat the whole file as a single block of text. Instead, it splits the PDF into pages and each page is returned as **document object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "7b81ecc4-0800-4708-acb9-d226c51c644e",
    "outputId": "4c428aa3-29b6-4c5f-b8d4-d8e5db41ac86"
   },
   "outputs": [],
   "source": [
    "# The number of pages in the original PDF file == The number of document objects\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "5b4ca01e-80fc-4b27-b0fb-62df1231dbd6"
   },
   "source": [
    "Let's display the text of the second document where we can see the Table of Contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "3bfe1297-608b-48b5-87bd-82b75fe4b90a",
    "outputId": "83ff777c-9aa5-4713-d2c8-4f3bf5f5311f"
   },
   "outputs": [],
   "source": [
    "print(documents[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "281e13be-433d-4da1-b332-d7bf5c252024"
   },
   "source": [
    "Each document include metadata such as `file_name`, `file_type`, `creation_date`, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "27457d8d-7ca6-4ebf-bff2-fb0f7c985caf",
    "outputId": "85df09f5-b2e6-45de-eafb-bb4a21b4a01a"
   },
   "outputs": [],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "64e04732-f224-4008-9835-d89e61c6396f"
   },
   "source": [
    "# 2. Creating the Index and Querying\n",
    "Next, we’ll build a vector database to store our embeddings. We'll use `VectorStoreIndex.from_documents()` which automatically **breaks each document into smaller pieces called nodes** based on length. Each node keeps the metadata of its parent document, so we don’t lose context. Once the nodes are created, they are passed to an embedding model - `text-embedding-ada-002` from OpenAI by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "88396a76-7874-431e-86aa-ddc4b51ca3f4",
    "outputId": "6484d7c3-1953-442c-9959-6aebfbd38e90"
   },
   "outputs": [],
   "source": [
    "# Creating the index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "b7744064-5b4e-4637-8afb-1868db1b3deb"
   },
   "source": [
    "Next, we’ll turn the index into a query engine so that we can ask questions.\n",
    "\n",
    "Behind the scenes, the workflow looks like this:\n",
    "1. **Query Embedding**: Our text query is embedded into a vector\n",
    "2. **Retriever**: Query vector is compared against the embeddings stored in the index and retriever returns the most relevant nodes - LlamaIndex uses **cosine** similarity by default\n",
    "3. **Response Syntethizer**: Combines the retrieved nodes with our query to generate a prompt, which is then passed to an LLM to produce an answer - LlamaIndex uses `gpt-3.5-turbo` from OpenAI by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "44e4cafa-0e35-43be-b796-c45bdfafa681",
    "outputId": "b9800c4c-21fd-414b-b269-a0db06e276c8"
   },
   "outputs": [],
   "source": [
    "# Setting the index as query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Querying\n",
    "print(query_engine.query(\"What is Title 1 about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "238e0789-edaf-407b-a511-83865378d43a"
   },
   "source": [
    "# 3. Making Data Persistent\n",
    "\n",
    "By default, `VectorStoreIndex` keeps all data in memory. However, LlamaIndex has its own built-in persistence mechanism.\n",
    "\n",
    "We will use `persist()` method that handle saving the index into \"my_storage\". In the code cell below, if folder \"my_storage\" does not exist yet the code will:\n",
    "- load PDF file from \"data\" folder\n",
    "- build a new index\n",
    "- persist that index to disk inside \"my storage\"\n",
    "\n",
    "If folder \"my_storage\" already exists, the code instead:\n",
    "- creates `StorageContext` object pointing to this folder\n",
    "- reload the previously saved index directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "db1e515c-786f-4720-9e5b-828bcbc05a80",
    "outputId": "73a0d749-f04f-4042-e144-a70c5b77c0a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# A directory\n",
    "PERSIST_DIR = \"./my_storage\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # Loading the documents and creating the index\n",
    "    documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # Storing\n",
    "    index.storage_context.persist(persist_dir = PERSIST_DIR)\n",
    "else:\n",
    "    # Reloading the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "8317c4d8-8841-45e2-b182-0f0234fd7d37"
   },
   "source": [
    "Now we can start running queries against it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "28cb3f47-6391-447c-92b2-6a9cefbd6a24",
    "outputId": "1bd26224-7b92-49d1-da13-35c560554e76"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Can you summarize Title 2?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "eef28516-0636-47b7-93b1-b6858d50d9e2"
   },
   "source": [
    "# 4. LlamaParse\n",
    "\n",
    "If your dataset includes different file types or documents with complex layouts (such as tables, multi-column text or embedded images), you can use `LlamaParse`. This parser is part of LlamaCloud and is designed to convert documents into structured outputs while preserving layout features far more accurately than generic readers.\n",
    "\n",
    "To use this parser, you’ll first need **LlamaCloud account**. Go to www.llamaindex.ai and sign-up. Then navigate to **API keys** section and click **Generate New Key**. Be sure to copy and store this secret key in a safe place. For security reasons, it will not be shown again in your account.\n",
    "\n",
    "> NOTE: You can also make your LlamaParse API key and base URL load automatically every time your terminal starts. This way, you don’t have to set them manually in every session. Open your terminal and edit your shell file - type `nano ~/.zshrc`. At the end of the file, add the following lines. Then run `source ~/.zshrc`.\n",
    ">\n",
    "> `export LLAMA_CLOUD_API_KEY=\"YOUR_EU_KEY\"`\n",
    ">\n",
    "> `export LLAMA_CLOUD_API_BASE=\"api.cloud.eu.llamaindex.ai\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "1308eb87-edd2-408b-a553-4ab9fb3eb155"
   },
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type = \"text\",\n",
    "    base_url = \"https://api.cloud.eu.llamaindex.ai\",  # Calling the EU LlamaCloud endpoint\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "82c5e62b-b049-48bc-8838-f55d3f537af8",
    "outputId": "51fb71fa-2dab-493f-b8ec-a3d8aef820d5"
   },
   "outputs": [],
   "source": [
    "documents = await parser.aload_data(\"./data/charter.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "48985328-aa5c-4b53-8ea8-654aba43978c"
   },
   "source": [
    "Let's again display the text of the second document - the parser preserves layout features like headings better than a simple text extractor like `SimpleDirectoryReader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "35139e0d-ef03-4847-a604-033d633b723a",
    "outputId": "532b9e84-6899-431d-d8d0-940d21e3aef9"
   },
   "outputs": [],
   "source": [
    "print(documents[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "2f3c2a32-6f7d-4ad4-aa51-774c83e06a3d"
   },
   "source": [
    "## 4.1 Using LlamaParse - PDF with tables into Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "321c3210-e505-442b-923f-932461353ff7"
   },
   "source": [
    "Now let’s try `LlamaParse` on PDF called \"livestock_poultry.pdf\" that contains not only the text but also **several tables**. `LlamaParse` will return the content in **Markdown format** which makes the document far easier for an LLM to interpret.\n",
    "\n",
    "In the code cell below, we initialize the parser that connects to the LlamaCloud API - we need to set `base_url` that specifies which regional LlamaCloud endpoint to use. In this case, we’re pointing to the EU server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "59530382-d441-4f6f-929d-d278f352b748"
   },
   "outputs": [],
   "source": [
    "# Parsing PDF\n",
    "parser = LlamaParse(\n",
    "    result_type = \"markdown\",\n",
    "    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "3191c952-183d-406e-b6dc-8db637bcc6d3"
   },
   "source": [
    "Now we can send a PDF file to the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "a2dfbe2a-102b-47d2-b687-d36cc26519a1",
    "outputId": "c7c4895c-803d-432a-bf77-f340f3194d56"
   },
   "outputs": [],
   "source": [
    "pdf_doc = await parser.aload_data(\"./data/livestock_poultry.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "069e7d49-c696-40ff-9cc0-7d005f6ae210"
   },
   "source": [
    "Let's print the document with index 8. Compare this Markdown output with the original PDF (page 9). Notice how the layout is preserved. This is what makes `LlamaParse` valuable: instead of flattening tables into plain text, it captures structure in a way that downstream models can use effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "770e1f2d-25d6-49c6-ae7a-49c425514027",
    "outputId": "d9d9f48d-9705-4610-ea6c-61550fc75b4e"
   },
   "outputs": [],
   "source": [
    "preview = pdf_doc[8].text[:500]\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "82296aeb-b730-44b1-8b92-d0a3c3f4ebf7"
   },
   "source": [
    "## 4.2 Parsing different file types\n",
    "\n",
    "In this section, we’ll see how to use LlamaParse to handle documents of different types, such as PDFs and Word files, and bring them into a single search workflow.\n",
    "\n",
    "Instead of writing separate code for each format, we can map file extensions to the same parser and let `SimpleDirectoryReader` automatically process everything in a folder.\n",
    "\n",
    "First, we'll initialize a parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "id": "2872fa69-834c-47b6-863a-1305840b275e"
   },
   "outputs": [],
   "source": [
    "parser = LlamaParse(result_type = \"markdown\",\n",
    "                    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
    "                    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "d60acec6-275e-4ff5-948c-6329180398ae"
   },
   "source": [
    "Next, we'll map file extensions to the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "909ca439-32d8-4f6f-ab65-1b1b0d261022"
   },
   "outputs": [],
   "source": [
    "file_extractor = {\n",
    "    \".pdf\": parser,\n",
    "    \".docx\": parser\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "4c18225d-f46b-4b7d-8f19-3eec78bc09b7"
   },
   "source": [
    "Now we can tell `SimpleDirectoryReader` to scan a folder with files \"charter.pdf\", \"livestock_poultry.pdf\" and \"vacation_policy.docx\". If it finds a `.pdf` or `.docx`, it will use our parser to process it. The result is a list of document objects where each page or section is stored as Markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "09339a6a-c221-485b-bf3b-9ec7453f501d",
    "outputId": "c54c235f-030b-4d0f-88a8-cc69b398be76"
   },
   "outputs": [],
   "source": [
    "documents = await SimpleDirectoryReader(\n",
    "    input_dir = \"./data/\",\n",
    "    file_extractor = file_extractor\n",
    ").aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "abf3d766-db07-496a-970c-81862c3c8ace"
   },
   "source": [
    "Now we are going to create embeddings for our documents. As we already know, when we build `VectorStoreIndex`, it automatically splits text into chunks before embedding, but this uses default settings.\n",
    "\n",
    "However, we can use `SentenceSplitter` to gain explicit control over how that chunking happens:\n",
    "- `chunk_size`: sets the maximum length of each chunk (keeps chunks small enough to fit into the embedding model and LLM context window)\n",
    "- `chunk_overlap`: defines how much content is repeated between consecutive chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "2c55b6fe-1b88-46f3-bfd1-c73d5575cbab"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Split into nodes (chunks)\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size = 512,        # each chunk will be about 512 characters/tokens long\n",
    "    chunk_overlap = 50)      # the last 50 characters/tokens of one chunk will also appear at the start of the next\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "5a935fea-6afc-4166-a2cd-b21c7558fff4"
   },
   "source": [
    "The next step is to build a vector index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "d25eafd0-9a8f-458e-8aaa-612e8032aee5"
   },
   "outputs": [],
   "source": [
    "# Creating embeddings from \"nodes\"\n",
    "index = VectorStoreIndex.from_documents(nodes)\n",
    "\n",
    "# Wrapping the index in a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "50fa25c2-0fe6-4333-959d-7bc2cf91d615"
   },
   "source": [
    "In the code cell below, the question is converted into a vector embedding which is compared against all stored embeddings (nodes) in the vector index. The nodes whose embeddings are most similar (highest cosine score) are selected as \"relevant\" and combined with the query and passed to an LLM to generate the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "1dfaa2a2-85e4-4683-b860-7116d3563250",
    "outputId": "8616b2d0-770b-48e1-91ed-6d2d89989f79"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"How many days can be carried over into the next calendar year?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "60ddadc8-0e90-4ebf-a006-782c8fdb0549",
    "outputId": "30200111-1bb0-496a-a80b-2fd93fd885b1"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"What are brazil top five pork export markets?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "da364cab-afa1-4f43-bb5c-a8bca167523d",
    "outputId": "8d99b7d0-49f2-4965-8e55-5ad6aa5392c8"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"What are the citizens' rights?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "5335c92d-a988-41af-afd7-aea53b68fae1"
   },
   "source": [
    "## 4.5 Using different LLM\n",
    "\n",
    "Up to now, we’ve built a vector index using the default embedding model and the default LLM. But both of these can be customized. By default, LlamaIndex uses OpenAI’s `text-embedding-ada-002` for embeddings and `gpt-3.5-turbo` for the LLM.\n",
    "\n",
    "In the example below, we’ll rebuild our index with a different embedding model - `text-embedding-3-small`, and then use a different LLM - `gpt-5-nano` to generate answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "fa706ddf-d0f2-4a8e-a9f1-4f1d03d527b1"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Building a new index + new embedding model\n",
    "pdf_index = VectorStoreIndex.from_documents(\n",
    "    pdf_doc,\n",
    "    embedding = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "id": "324b30fc-6068-45eb-ae39-b90dd3df316c"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Using new LLM\n",
    "query_engine = index.as_query_engine(llm = OpenAI(model=OPENAI_MODEL))\n",
    "response = query_engine.query(\"What is the forecasted percentage change of global export of pork between 2024 and 2025?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
