{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "57a550dd-cadd-4895-9967-9e42c3a849c3"
   },
   "source": [
    "# LlamaIndex\n",
    "\n",
    "LlamaIndex is a framework designed to help you build applications powered by Large Language Models such as chatbots, AI assistants, and translation tools. One of its most valuable capabilities is enriching the knowledge of your LLM with **your own data**, enabling the model to answer questions about **personal, organizational, or domain-specific information** that it wasn‚Äôt originally trained on.\n",
    "\n",
    "::: note\n",
    "**Before you run this notebook**\n",
    "- You need an OpenAI API key available as `OPENAI_API_KEY`.\n",
    "- Cells using LlamaParse require a LlamaCloud API key (`LLAMA_CLOUD_API_KEY`).\n",
    "- If you do not have these keys, skip the LlamaParse/OpenAI sections and follow the local examples.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries with working versions\n!pip install -q llama-index-core==0.14.6 llama-index-embeddings-openai==0.5.1 \\\n    llama-index-llms-openai==0.6.6 openai==1.109.1 \\\n    chromadb==1.2.2 llama-index-vector-stores-chroma==0.5.3 \\\n    llama-index-readers-file llama-parse\n\nprint(\"‚úÖ All libraries installed successfully!\")\nprint(\"‚ö†Ô∏è  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "733ee605-2d9c-4de0-990a-f039e812513c"
   },
   "source": [
    "# 1. Data Connectors\n",
    "\n",
    "LlamaIndex uses data connectors to **ingest information** from a wide range of **structured and unstructured sources**.\n",
    "\n",
    "The simplest way to load the data is using `SimpleDirectoryReader` which supports various file types such as:\n",
    "\n",
    "- csv - comma-separated values\n",
    "- docx - Microsoft Word\n",
    "- ipynb - Jupyter Notebook\n",
    "- pdf - Portable Document Format\n",
    "- ppt, .pptm, .pptx - Microsoft PowerPoint\n",
    "- ...and many more.\n",
    "\n",
    "Data connector takes your data from these different formats and put them together in a uniform, organized way so they can be used within your LLM application.\n",
    "\n",
    "You can find all supported file types in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/#simpledirectoryreader).\n",
    "\n",
    "Let's import `SimpleDirectoryReader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('‚úÖ API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('‚ùå ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('‚ùå ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('‚úÖ Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'ü§ñ Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'üß† Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "3cf0f9d3-0997-485f-8423-d15a81803388"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "17628016-069e-4ba8-b631-c02b0cebcf58"
   },
   "source": [
    "We will load the PDF file called \"charter.pdf\" (stored in \"data\" folder in notebook's directory) containing the Charter of Fundamental Rights of the European Union.  \n",
    "\n",
    "> NOTE: In this notebook we will use the asynchronous (async) versions of data connectors using `await` and `.aload_data()`. It helps everything run more smoothly and prevents technical errors with the notebook‚Äôs event loop. You don‚Äôt need to understand all the internals, just know that `await` is the keyword that tells Python \"this step might take a while, pause here until it‚Äôs done\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "e099daaa-3f6a-4ec7-bf14-07b98c924fa9"
   },
   "outputs": [],
   "source": [
    "# Generating documents\n",
    "documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "2af3a19a-770a-4d0a-8ff7-9ea41b9773a8"
   },
   "source": [
    "When this data connector processes a PDF, it doesn‚Äôt treat the whole file as a single block of text. Instead, it splits the PDF into pages and each page is returned as **document object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "7b81ecc4-0800-4708-acb9-d226c51c644e",
    "outputId": "4c428aa3-29b6-4c5f-b8d4-d8e5db41ac86"
   },
   "outputs": [],
   "source": [
    "# The number of pages in the original PDF file == The number of document objects\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "5b4ca01e-80fc-4b27-b0fb-62df1231dbd6"
   },
   "source": [
    "Let's display the text of the second document where we can see the Table of Contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "3bfe1297-608b-48b5-87bd-82b75fe4b90a",
    "outputId": "83ff777c-9aa5-4713-d2c8-4f3bf5f5311f"
   },
   "outputs": [],
   "source": [
    "print(documents[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "281e13be-433d-4da1-b332-d7bf5c252024"
   },
   "source": [
    "Each document include metadata such as `file_name`, `file_type`, `creation_date`, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "27457d8d-7ca6-4ebf-bff2-fb0f7c985caf",
    "outputId": "85df09f5-b2e6-45de-eafb-bb4a21b4a01a"
   },
   "outputs": [],
   "source": [
    "documents[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 1: Load and Explore Documents (5-7 minutes)\n",
    "\n",
    "**What you'll practice:** Using SimpleDirectoryReader to load documents and inspect their properties.\n",
    "\n",
    "**Your task:**\n",
    "1. Check how many document objects were created from the PDF file\n",
    "2. Display the text content of the first document (index 0)\n",
    "3. Print the metadata for the first document\n",
    "4. Think about: Why is the PDF split into multiple document objects? What does each object represent?\n",
    "\n",
    "**Hint:** \n",
    "- Use `len(documents)` to count documents\n",
    "- Access properties with `documents[0].text` and `documents[0].metadata`\n",
    "\n",
    "**Expected outcome:** You'll see that each document object corresponds to one page of the PDF, keeping the content organized and making it easier to track where information comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# \n",
    "# print(f\"Total documents loaded: {len(documents)}\")\n",
    "# print(f\"\\nFirst document text (first 300 characters):\")\n",
    "# print(documents[0].text[:300])\n",
    "# print(f\"\\nFirst document metadata:\")\n",
    "# print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "64e04732-f224-4008-9835-d89e61c6396f"
   },
   "source": [
    "# 2. Creating the Index and Querying\n",
    "Next, we‚Äôll build a vector database to store our embeddings. We'll use `VectorStoreIndex.from_documents()` which automatically **breaks each document into smaller pieces called nodes** based on length. Each node keeps the metadata of its parent document, so we don‚Äôt lose context. Once the nodes are created, they are passed to an embedding model - `text-embedding-ada-002` from OpenAI by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "88396a76-7874-431e-86aa-ddc4b51ca3f4",
    "outputId": "6484d7c3-1953-442c-9959-6aebfbd38e90"
   },
   "outputs": [],
   "source": [
    "# Creating the index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "b7744064-5b4e-4637-8afb-1868db1b3deb"
   },
   "source": [
    "Next, we‚Äôll turn the index into a query engine so that we can ask questions.\n",
    "\n",
    "Behind the scenes, the workflow looks like this:\n",
    "1. **Query Embedding**: Our text query is embedded into a vector\n",
    "2. **Retriever**: Query vector is compared against the embeddings stored in the index and retriever returns the most relevant nodes - LlamaIndex uses **cosine** similarity by default\n",
    "3. **Response Syntethizer**: Combines the retrieved nodes with our query to generate a prompt, which is then passed to an LLM to produce an answer - LlamaIndex uses `gpt-3.5-turbo` from OpenAI by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "44e4cafa-0e35-43be-b796-c45bdfafa681",
    "outputId": "b9800c4c-21fd-414b-b269-a0db06e276c8"
   },
   "outputs": [],
   "source": [
    "# Setting the index as query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Querying\n",
    "print(query_engine.query(\"What is Title 1 about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "238e0789-edaf-407b-a511-83865378d43a"
   },
   "source": [
    "# 3. Making Data Persistent\n",
    "\n",
    "By default, `VectorStoreIndex` keeps all data in memory. However, LlamaIndex has its own built-in persistence mechanism.\n",
    "\n",
    "We will use `persist()` method that handle saving the index into \"my_storage\". In the code cell below, if folder \"my_storage\" does not exist yet the code will:\n",
    "- load PDF file from \"data\" folder\n",
    "- build a new index\n",
    "- persist that index to disk inside \"my storage\"\n",
    "\n",
    "If folder \"my_storage\" already exists, the code instead:\n",
    "- creates `StorageContext` object pointing to this folder\n",
    "- reload the previously saved index directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "db1e515c-786f-4720-9e5b-828bcbc05a80",
    "outputId": "73a0d749-f04f-4042-e144-a70c5b77c0a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# A directory\n",
    "PERSIST_DIR = \"./my_storage\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # Loading the documents and creating the index\n",
    "    documents = await SimpleDirectoryReader(input_files = [\"data/charter.pdf\"]).aload_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # Storing\n",
    "    index.storage_context.persist(persist_dir = PERSIST_DIR)\n",
    "else:\n",
    "    # Reloading the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "8317c4d8-8841-45e2-b182-0f0234fd7d37"
   },
   "source": [
    "Now we can start running queries against it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 2: Query Your Index (7-10 minutes)\n",
    "\n",
    "**What you'll practice:** Querying a vector index and understanding how LlamaIndex retrieves information.\n",
    "\n",
    "**Your task:**\n",
    "1. Think of a question about the EU Charter document (e.g., \"What rights do children have?\", \"What is Article 10 about?\", \"What freedoms are protected?\")\n",
    "2. Query the index using your question\n",
    "3. Print the response\n",
    "4. Try a second question and compare the answers\n",
    "5. Think about: How does the answer quality depend on your question phrasing?\n",
    "\n",
    "**Hint:** Use the query engine that was created from the persistent index:\n",
    "```python\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Your question\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Expected outcome:** You should get relevant answers based on the document content. More specific questions typically yield better, more focused answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# \n",
    "# question1 = \"Your first question here\"\n",
    "# response1 = query_engine.query(question1)\n",
    "# print(f\"Question 1: {question1}\")\n",
    "# print(f\"Answer: {response1}\\n\")\n",
    "# \n",
    "# question2 = \"Your second question here\"\n",
    "# response2 = query_engine.query(question2)\n",
    "# print(f\"Question 2: {question2}\")\n",
    "# print(f\"Answer: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "28cb3f47-6391-447c-92b2-6a9cefbd6a24",
    "outputId": "1bd26224-7b92-49d1-da13-35c560554e76"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Can you summarize Title 2?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "eef28516-0636-47b7-93b1-b6858d50d9e2"
   },
   "source": [
    "# 4. LlamaParse\n",
    "\n",
    "If your dataset includes different file types or documents with complex layouts (such as tables, multi-column text or embedded images), you can use `LlamaParse`. This parser is part of LlamaCloud and is designed to convert documents into structured outputs while preserving layout features far more accurately than generic readers.\n",
    "\n",
    "To use this parser, you‚Äôll first need **LlamaCloud account**. Go to www.llamaindex.ai and sign-up. Then navigate to **API keys** section and click **Generate New Key**. Be sure to copy and store this secret key in a safe place. For security reasons, it will not be shown again in your account.\n",
    "\n",
    "> NOTE: You can also make your LlamaParse API key and base URL load automatically every time your terminal starts. This way, you don‚Äôt have to set them manually in every session. Open your terminal and edit your shell file - type `nano ~/.zshrc`. At the end of the file, add the following lines. Then run `source ~/.zshrc`.\n",
    ">\n",
    "> `export LLAMA_CLOUD_API_KEY=\"YOUR_EU_KEY\"`\n",
    ">\n",
    "> `export LLAMA_CLOUD_API_BASE=\"api.cloud.eu.llamaindex.ai\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "1308eb87-edd2-408b-a553-4ab9fb3eb155"
   },
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type = \"text\",\n",
    "    base_url = \"https://api.cloud.eu.llamaindex.ai\",  # Calling the EU LlamaCloud endpoint\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "82c5e62b-b049-48bc-8838-f55d3f537af8",
    "outputId": "51fb71fa-2dab-493f-b8ec-a3d8aef820d5"
   },
   "outputs": [],
   "source": [
    "documents = await parser.aload_data(\"./data/charter.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "48985328-aa5c-4b53-8ea8-654aba43978c"
   },
   "source": [
    "Let's again display the text of the second document - the parser preserves layout features like headings better than a simple text extractor like `SimpleDirectoryReader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "35139e0d-ef03-4847-a604-033d633b723a",
    "outputId": "532b9e84-6899-431d-d8d0-940d21e3aef9"
   },
   "outputs": [],
   "source": [
    "print(documents[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "2f3c2a32-6f7d-4ad4-aa51-774c83e06a3d"
   },
   "source": [
    "## 4.1 Using LlamaParse - PDF with tables into Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 3: Compare SimpleDirectoryReader vs LlamaParse (10-12 minutes)\n",
    "\n",
    "**What you'll practice:** Understanding the differences between basic and advanced document parsing.\n",
    "\n",
    "**Your task:**\n",
    "1. Look at the parsed output from LlamaParse for the livestock_poultry.pdf document\n",
    "2. Display a different page/document from the parsed results (try index 5 or 6)\n",
    "3. Observe how tables and structured data are represented\n",
    "4. Think about: When would you use LlamaParse instead of SimpleDirectoryReader?\n",
    "\n",
    "**Key differences to notice:**\n",
    "- **SimpleDirectoryReader**: Fast, simple, treats all text uniformly\n",
    "- **LlamaParse**: Preserves structure (tables, headings, lists) as Markdown, better for complex layouts\n",
    "\n",
    "**Hint:** Access different pages with `pdf_doc[index].text` where index is 0 to len(pdf_doc)-1\n",
    "\n",
    "**Expected outcome:** You'll see that LlamaParse preserves table structure and formatting that would be lost with basic text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# \n",
    "# print(f\"Total pages/documents in PDF: {len(pdf_doc)}\")\n",
    "# \n",
    "# # Display a different page\n",
    "# page_index = 5  # Try different numbers\n",
    "# print(f\"\\nContent from page/document {page_index}:\")\n",
    "# print(pdf_doc[page_index].text[:600])  # Show first 600 characters\n",
    "# \n",
    "# # Analyze the structure\n",
    "# print(\"\\nObservations:\")\n",
    "# print(\"- Are tables preserved?\")\n",
    "# print(\"- Are headings clearly marked?\")\n",
    "# print(\"- Is the layout structure maintained?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "321c3210-e505-442b-923f-932461353ff7"
   },
   "source": [
    "Now let‚Äôs try `LlamaParse` on PDF called \"livestock_poultry.pdf\" that contains not only the text but also **several tables**. `LlamaParse` will return the content in **Markdown format** which makes the document far easier for an LLM to interpret.\n",
    "\n",
    "In the code cell below, we initialize the parser that connects to the LlamaCloud API - we need to set `base_url` that specifies which regional LlamaCloud endpoint to use. In this case, we‚Äôre pointing to the EU server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "59530382-d441-4f6f-929d-d278f352b748"
   },
   "outputs": [],
   "source": [
    "# Parsing PDF\n",
    "parser = LlamaParse(\n",
    "    result_type = \"markdown\",\n",
    "    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "3191c952-183d-406e-b6dc-8db637bcc6d3"
   },
   "source": [
    "Now we can send a PDF file to the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "a2dfbe2a-102b-47d2-b687-d36cc26519a1",
    "outputId": "c7c4895c-803d-432a-bf77-f340f3194d56"
   },
   "outputs": [],
   "source": [
    "pdf_doc = await parser.aload_data(\"./data/livestock_poultry.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "069e7d49-c696-40ff-9cc0-7d005f6ae210"
   },
   "source": [
    "Let's print the document with index 8. Compare this Markdown output with the original PDF (page 9). Notice how the layout is preserved. This is what makes `LlamaParse` valuable: instead of flattening tables into plain text, it captures structure in a way that downstream models can use effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "770e1f2d-25d6-49c6-ae7a-49c425514027",
    "outputId": "d9d9f48d-9705-4610-ea6c-61550fc75b4e"
   },
   "outputs": [],
   "source": [
    "preview = pdf_doc[8].text[:500]\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "82296aeb-b730-44b1-8b92-d0a3c3f4ebf7"
   },
   "source": [
    "## 4.2 Parsing different file types\n",
    "\n",
    "In this section, we‚Äôll see how to use LlamaParse to handle documents of different types, such as PDFs and Word files, and bring them into a single search workflow.\n",
    "\n",
    "Instead of writing separate code for each format, we can map file extensions to the same parser and let `SimpleDirectoryReader` automatically process everything in a folder.\n",
    "\n",
    "First, we'll initialize a parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "2872fa69-834c-47b6-863a-1305840b275e"
   },
   "outputs": [],
   "source": [
    "parser = LlamaParse(result_type = \"markdown\",\n",
    "                    base_url = \"https://api.cloud.eu.llamaindex.ai\",\n",
    "                    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "d60acec6-275e-4ff5-948c-6329180398ae"
   },
   "source": [
    "Next, we'll map file extensions to the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "909ca439-32d8-4f6f-ab65-1b1b0d261022"
   },
   "outputs": [],
   "source": [
    "file_extractor = {\n",
    "    \".pdf\": parser,\n",
    "    \".docx\": parser\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "4c18225d-f46b-4b7d-8f19-3eec78bc09b7"
   },
   "source": [
    "Now we can tell `SimpleDirectoryReader` to scan a folder with files \"charter.pdf\", \"livestock_poultry.pdf\" and \"vacation_policy.docx\". If it finds a `.pdf` or `.docx`, it will use our parser to process it. The result is a list of document objects where each page or section is stored as Markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "09339a6a-c221-485b-bf3b-9ec7453f501d",
    "outputId": "c54c235f-030b-4d0f-88a8-cc69b398be76"
   },
   "outputs": [],
   "source": [
    "documents = await SimpleDirectoryReader(\n",
    "    input_dir = \"./data/\",\n",
    "    file_extractor = file_extractor\n",
    ").aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "abf3d766-db07-496a-970c-81862c3c8ace"
   },
   "source": [
    "Now we are going to create embeddings for our documents. As we already know, when we build `VectorStoreIndex`, it automatically splits text into chunks before embedding, but this uses default settings.\n",
    "\n",
    "However, we can use `SentenceSplitter` to gain explicit control over how that chunking happens:\n",
    "- `chunk_size`: sets the maximum length of each chunk (keeps chunks small enough to fit into the embedding model and LLM context window)\n",
    "- `chunk_overlap`: defines how much content is repeated between consecutive chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "::: info\n",
    "**Why chunk size matters**\n",
    "\n",
    "When we embed an entire document as a single vector, we are effectively averaging all of its topics into one point in space. For multi-topic articles this produces a diluted signal: a query about \"zero trust\" might rank poorly because the vector also carries equally strong signals for other sections such as cryptography or phishing. Chunking breaks the document into focused segments so each embedding represents one coherent idea, dramatically improving retrieval precision.\n",
    "\n",
    "**Trade-offs**\n",
    "- *Chunks that are too large (1000+ tokens)* keep the full context but blend unrelated concepts, reducing similarity scores and hurting recall.\n",
    "- *Chunks that are too small (50 tokens)* deliver crisp matches but may lose the surrounding context the LLM needs when generating an answer.\n",
    "\n",
    "**Mitigation strategies**\n",
    "1. Start with a balanced window (e.g., 256‚Äì512 tokens) and adjust based on your corpus.\n",
    "2. Introduce overlap (e.g., 10‚Äì20% of the chunk size) so important sentences near boundaries appear in both neighbouring chunks.\n",
    "3. During retrieval, fetch neighbouring chunks or stitch together the original document spans so the LLM receives enough context to respond reliably.\n",
    "\n",
    "This approach preserves the semantic focus needed for accurate vector search while still giving the downstream LLM the broader context it needs.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "id": "2c55b6fe-1b88-46f3-bfd1-c73d5575cbab"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Split into nodes (chunks)\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size = 512,        # each chunk will be about 512 characters/tokens long\n",
    "    chunk_overlap = 50)      # the last 50 characters/tokens of one chunk will also appear at the start of the next\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "5a935fea-6afc-4166-a2cd-b21c7558fff4"
   },
   "source": [
    "The next step is to build a vector index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "d25eafd0-9a8f-458e-8aaa-612e8032aee5"
   },
   "outputs": [],
   "source": [
    "# Creating embeddings from \"nodes\"\n",
    "index = VectorStoreIndex.from_documents(nodes)\n",
    "\n",
    "# Wrapping the index in a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "50fa25c2-0fe6-4333-959d-7bc2cf91d615"
   },
   "source": [
    "In the code cell below, the question is converted into a vector embedding which is compared against all stored embeddings (nodes) in the vector index. The nodes whose embeddings are most similar (highest cosine score) are selected as \"relevant\" and combined with the query and passed to an LLM to generate the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "1dfaa2a2-85e4-4683-b860-7116d3563250",
    "outputId": "8616b2d0-770b-48e1-91ed-6d2d89989f79"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"How many days can be carried over into the next calendar year?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "60ddadc8-0e90-4ebf-a006-782c8fdb0549",
    "outputId": "30200111-1bb0-496a-a80b-2fd93fd885b1"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"What are brazil top five pork export markets?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "id": "da364cab-afa1-4f43-bb5c-a8bca167523d",
    "outputId": "8d99b7d0-49f2-4965-8e55-5ad6aa5392c8"
   },
   "outputs": [],
   "source": [
    "# Running the query\n",
    "print(query_engine.query(\"What are the citizens' rights?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "5335c92d-a988-41af-afd7-aea53b68fae1"
   },
   "source": [
    "## 4.5 Using different LLM\n",
    "\n",
    "Up to now, we‚Äôve built a vector index using the default embedding model and the default LLM. But both of these can be customized. By default, LlamaIndex uses OpenAI‚Äôs `text-embedding-ada-002` for embeddings and `gpt-3.5-turbo` for the LLM.\n",
    "\n",
    "In the example below, we‚Äôll rebuild our index with a different embedding model - `text-embedding-3-small`, and then use a different LLM - `gpt-5-nano` to generate answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "id": "fa706ddf-d0f2-4a8e-a9f1-4f1d03d527b1"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Building a new index + new embedding model\n",
    "pdf_index = VectorStoreIndex.from_documents(\n",
    "    pdf_doc,\n",
    "    embedding = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "id": "324b30fc-6068-45eb-ae39-b90dd3df316c"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Using new LLM\n",
    "query_engine = index.as_query_engine(llm = OpenAI(model=OPENAI_MODEL))\n",
    "response = query_engine.query(\"What is the forecasted percentage change of global export of pork between 2024 and 2025?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}