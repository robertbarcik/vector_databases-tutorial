{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXbp5qlRnw7r"
   },
   "source": [
    "# Introduction\n",
    "In this exercise, we will learn how to **enrich documents with metadata** before indexing them for RAG. So instead of only storing raw text, weâ€™ll add useful information such as titles, summaries or example questions/answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtblxeGJgfqu"
   },
   "source": [
    "# 1. Loading the data\n",
    "\n",
    "We are going to work with the PDF file \"why-language-models-hallucinate.pdf\" (a recent OpenAI research piece that explores the statistical reasons behind model hallucinations) and load it using `SimpleDirectoryReader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('âœ… API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('ðŸ’¡ To use Colab secrets: Go to ðŸ”‘ (left sidebar) â†’ Add new secret â†’ Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('âŒ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('âŒ ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('âœ… Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'ðŸ¤– Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'ðŸ§  Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew6PmzeJcgZ-"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = await SimpleDirectoryReader(input_files=[\"./data/why-language-models-hallucinate.pdf\"]).aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-cmMrDGnw7v"
   },
   "source": [
    "The file has 36 pages, so the data connector created 36 document objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSMVG6Slnw7v"
   },
   "outputs": [],
   "source": [
    "print(\"Number of document objects:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dd7Hqehnw7w"
   },
   "source": [
    "# 2. Controlling Metadata Visibility\n",
    "\n",
    "Before we enrich our documents, we first need to understand what information is already stored in them.\n",
    "\n",
    "\n",
    "\n",
    "We already know that each document carries not just the text but also the metadata. Additionally, there are 2 parameters that control what parts of that metadata will be visible to the embedding model and to the LLM at query time:\n",
    "\n",
    "`excluded_embed_metadata_keys`:\n",
    "- tells LlamaIndex which metadata **should not be included when creating embeddings**\n",
    "- embeddings are designed to capture the semantic meaning of content, and technical details such as file size or last modified date do not add any useful meaning.\n",
    "\n",
    "`excluded_llm_metadata_keys`:\n",
    "- tells LlamaIndex which metadata **should not be sent to the LLM when the document is retrieved at query time**\n",
    "- the reason for controlling this is that some metadata, like we can add the author, can provide valuable context to the LLM when generating an answer, while other metadata would only distract the model and reduce the clarity of its response\n",
    "\n",
    "These two parameters give us **control over what information flows into embeddings and into the LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axrU95QwntZH"
   },
   "outputs": [],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgT73ub5nw7w"
   },
   "source": [
    "## 2.1 Manually Constructed Documents\n",
    "\n",
    "Most of the time we let LlamaIndex create documents automatically when we load files. But sometimes we want to **build a Document manually**. This is useful when:\n",
    "1. Our data doesnâ€™t come from a file (e.g., from a database or an API)\n",
    "2. We want to attach custom metadata up front\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0YeIV1Vnw7x"
   },
   "source": [
    "The example below (adapted from the documentation) shows how we can construct a custom Document and explicitly control what metadata is included when building embeddings.\n",
    "\n",
    "In this case, we give the document some metadata (\"file_name\", \"category\" and \"author\"). But notice that we tell LlamaIndex to **exclude \"file_name\"** from embeddings by setting `excluded_embed_metadata_keys`. This makes sense, because the actual file name is not semantically meaningful and would only add noise to the embedding space. The category (\"finance\") and author (\"LlamaIndex\"), however, may carry useful meaning for semantic search, so we leave them in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YbC8JS_nw7x"
   },
   "outputs": [],
   "source": [
    "# What the Embedding model will see\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    excluded_embed_metadata_keys=[\"file_name\"]\n",
    ")\n",
    "\n",
    "print(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PJ1RK_1nw7x"
   },
   "source": [
    "Just like we control which metadata flows into the embedding model, we can also **decide which metadata the LLM will receive when it is asked to answer a query**. This is important because the LLM doesnâ€™t only use the raw text of a chunk. It can also use metadata as extra context to generate a better answer.\n",
    "\n",
    "In the example below, we create a custom Document with the same metadata fields. This time, however, we tell LlamaIndex to exclude the category from what the LLM sees. That means when the document is retrieved later, the model will still see the file name (so it knows the source) and the author (which may add credibility). In this case, the category is redundant - the text already makes it clear that the topic is finance, so it likely wonâ€™t affect the LLMâ€™s response and only takes up prompt space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmbdVZ_mpvcH"
   },
   "outputs": [],
   "source": [
    "# What the LLM model will see\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    excluded_llm_metadata_keys=[\"category\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzVYLJlEnw7x"
   },
   "source": [
    "Some metadata is more useful for embeddings, some is more useful for the LLM, and some works for both. **Whether we exclude/add a certain information depends on our use case**: are we trying to keep embeddings clean, or give the LLM more context? In real-world projects, this is a design choice you make depending on how much metadata adds value versus noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD8NeBVnnw7y"
   },
   "source": [
    "By default, LlamaIndex already takes care of formatting metadata in a clean way, and in practice you usually donâ€™t need to change it. However, you can **customize the formatting** if you want more readable prompts for the LLM or if you want the metadata formatted in a certain style to match your companyâ€™s pipelines or prompt style.\n",
    "\n",
    "Optional parameters:\n",
    "- `metadata_seperator` - Sets the character(s) between different pieces of metadata. The default is a newline (`\"\\n\"`).\n",
    "- `metadata_template` - Defines how each key-value pair is shown. Both `{key}` and `{value}` must be included.\n",
    "- `text_template` - takes two variables: `metadata_str` and `content`\n",
    "\n",
    "This doesnâ€™t change what information is sent, only how it is displayed. For example, for the LLM a cleaner format can sometimes help it parse metadata more naturally.\n",
    "\n",
    "In our exercise, weâ€™ll try a custom format just to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5NctF0anw7y"
   },
   "outputs": [],
   "source": [
    "# Formatting\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    metadata_seperator=\", \",\n",
    "    metadata_template=\"{key}:{value}\",\n",
    "    text_template=\"Metadata:\\n{metadata_str}\\n------\\nContent:\\n{content}\",\n",
    ")\n",
    "\n",
    "print(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1MA7MLhnw7y"
   },
   "source": [
    "Now letâ€™s return to the real documents created automatically when we loaded our PDF. Each of these documents already comes with some metadata attached.\n",
    "\n",
    "- \"file_path\" can be useful (it tells us where the chunk came from)\n",
    "- \"page_label\" usually does not add much value for embeddings (handy to keep for LLM if you want the reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpfv9Auvn_1q"
   },
   "outputs": [],
   "source": [
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJJ8rkX9nw7y"
   },
   "source": [
    "Let's exclude page labels - we can loop through all documents, adjust their formatting template and tell LlamaIndex not to include \"page_label\" in the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bmx1ZCisotj"
   },
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    # Defining the content/metadata template\n",
    "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "\n",
    "    # Excluding page label from embedding\n",
    "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9twn8ILKnw7z"
   },
   "source": [
    "Let's check the transformation - page label should not be included in the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8hDb9WztOHm"
   },
   "outputs": [],
   "source": [
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GchJEAIHnw7z"
   },
   "source": [
    "# 3. Building a RAG Pipeline with Metadata Enrichment\n",
    "\n",
    "Now weâ€™re getting to the interesting and fun part of the notebook. Up to this point, our documents only carried basic metadata like file names and page labels. Thatâ€™s useful for organizing files, but it doesnâ€™t really help a RAG system retrieve more accurate answers. So this is what weâ€™re going to do: make our RAG pipeline smarter by **enriching each document chunk with additional context** such as short titles, summaries and example Q&As. To do so, we'll **use a language model  `gpt-5-nano` to generate this metadata**.\n",
    "\n",
    "To really test whether this helps, weâ€™ll build **3 different versions of our nodes** (chunks of text):\n",
    "- Baseline nodes (`nodes_0`) - only the basic metadata\n",
    "- Title-enriched nodes (`nodes_1`) â€“ chunks labeled with short, descriptive titles.\n",
    "- Fully enriched nodes (`nodes_2`) â€“ chunks augmented with titles, summaries and example Q&A pairs.\n",
    "\n",
    "Weâ€™ll follow three main steps in this experiment:\n",
    "1. **Splitting the data**: we'll break the PDF into smaller, manageable chunks\n",
    "2. **Creating three versions of nodes**\n",
    "3. **Building and testing RAG indexes**: weâ€™ll run the same queries against each node set and compare the results to see how much metadata enrichment improves retrieval and answers\n",
    "\n",
    "> NOTE: This setup is inspired by the official LlamaIndex metadata extraction [example](https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MetadataExtraction_LLMSurvey/#automated-metadata-extraction-for-better-retrieval-synthesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLHF-yzYnw7z"
   },
   "source": [
    "**OpenAI's Language model for transformations**\n",
    "\n",
    "We will use OpenAIâ€™s `gpt-5-nano` model which is fast, affordable, and accurate enough for our metadata extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvgSCHy0nw7z"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Language model\n",
    "llm_transformations = OpenAI(\n",
    "    model = OPENAI_MODEL,\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bezsWTqhnw7z"
   },
   "source": [
    "## 3.1 Splitting the data\n",
    "\n",
    "First, we need to prepare our documents for transformation by splitting them into smaller chunks. Large documents cannot be processed effectively all at once. We'll use `SentenceSplitter` which splits the content into 1024 tokens and also adds an overlap of 128 tokens. The overlap ensures that if important information appears at the boundary of one chunk, it is also present in the next chunk, so nothing is lost. Parameter `separator` simply tells the splitter to break text along spaces (keeping words intact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4Aywinmnw7z"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    separator = \" \",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExPsw0bunw7z"
   },
   "source": [
    "## 3.2 Creating three versions of Nodes\n",
    "\n",
    "Weâ€™ll now create three parallel versions of our corpus so we can run a fair comparison later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwUFyAUynw70"
   },
   "source": [
    "### 3.2.1 Creating baseline nodes (split only)\n",
    "First, we create the baseline: chunks produced by the splitter with no metadata enrichment. This gives us a control group. Any improvement we see later can be attributed to the extra metadata, not to changes in chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppzlrWoOnw70"
   },
   "outputs": [],
   "source": [
    "# Baseline nodes\n",
    "baseline_nodes = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRaL6CUJnw70"
   },
   "outputs": [],
   "source": [
    "baseline_nodes[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKuo_n8Cnw70"
   },
   "source": [
    "### 3.2.2 Enriched nodes (titles extraction)\n",
    "\n",
    "Next, we'll add short, descriptive titles for each chunk of text. These will be labels for each chunk and often help the retriever match user intent to the right passage.\n",
    "\n",
    "To do this, we use `TitleExtractor` which takes an LLM and generates a title for each node. We also set the parameter `nodes = 5` so that up to 5 chunks are processed in one request, making the process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRa9xlUInw70"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import TitleExtractor\n",
    "\n",
    "title_extractor = TitleExtractor(llm = llm_transformations, nodes = 5)\n",
    "title_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnIoWvKLnw70"
   },
   "source": [
    "Next, we run this transformation using `IngestionPipeline`. The pipeline executes a sequence of transformations, in our case, splitting the text into chunks and then adding titles. We set `in_place=False` to make sure we donâ€™t overwrite our baseline nodes. Instead, we produce a separate list (stored in \"nodes_1\") for A/B test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFPRF_Tgnw70"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline_titles = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miG_qQZHnw71"
   },
   "outputs": [],
   "source": [
    "# Running the pipeline\n",
    "nodes_1 = pipeline_titles.run(\n",
    "    documents = documents,\n",
    "    in_place = False,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_OuscTtnw71"
   },
   "source": [
    "### 3.2.3 Fully enriched nodes (titles + Q&A + summary extraction)\n",
    "\n",
    "For the richest version of our nodes, weâ€™ll go beyond titles and also add example Q&A pairs and short summaries.\n",
    "\n",
    "**Q&A pairs simulate how a real user might query the system and what kind of response a chunk could provide**. This makes the retrieverâ€™s job easier because each chunk carries hints about the kinds of questions it can answer. In practice, adding Q&A metadata often improves recall (finding the right chunk) and helps the system produce more useful answers. Weâ€™ll use `QuestionsAnsweredExtractor` and set `questions = 3`, which asks the LLM to generate three realistic Q&A pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h61G6wSAnw71"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import QuestionsAnsweredExtractor\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(llm = llm_transformations, questions = 3)\n",
    "qa_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0FJrzY7nw71"
   },
   "source": [
    "**Summaries capture the core ideas of each chunk in a compact form**. They provide another layer of metadata thatâ€™s especially helpful when users ask broader or high-level questions. Weâ€™ll use `SummaryExtractor` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmn8ppZznw72"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import SummaryExtractor\n",
    "\n",
    "summary_extractor = SummaryExtractor(llm = llm_transformations)\n",
    "summary_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYIXHSQRnw72"
   },
   "source": [
    "Both transformations run inside the same pipeline, along with the SentenceSplitter and TitleExtractor, so each chunk ends up with a title, a short summary and 3 example Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVS6y7X1nw72"
   },
   "outputs": [],
   "source": [
    "# titles + Q&A + summary\n",
    "pipeline_rich = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor,\n",
    "        qa_extractor,\n",
    "        summary_extractor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbxODGHLnw72"
   },
   "outputs": [],
   "source": [
    "# Running the pipeline\n",
    "nodes_2 = pipeline_rich.run(\n",
    "    documents = documents,\n",
    "    in_place = False,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-My1ucKZnw73"
   },
   "outputs": [],
   "source": [
    "print(nodes_2[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGeORF84nw73"
   },
   "source": [
    "**Splicing Baseline and Enriched Nodes**\n",
    "\n",
    "To fairly test the effect of metadata enrichment, we donâ€™t want to rebuild our dataset in three completely separate ways. That would make it difficult to know if differences in answers are due to enrichment or simply because the data was reprocessed differently. Instead, we keep most of the dataset identical and **replace only a small slice of nodes with enriched versions**.\n",
    "\n",
    "This creates a controlled experiment:\n",
    "- All three indexes contain the same core content.\n",
    "- The only difference is that in \"index1\" and \"index2\", a chosen section of the document is enriched with new metadata (titles, or titles + Q&A + summaries).\n",
    "- If the enriched versions produce better answers, we can be confident the improvement comes from the metadata itself, not from unrelated differences.\n",
    "\n",
    "  \n",
    "First let's check the number of nodes in the baseline split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGJNcX3anw73"
   },
   "outputs": [],
   "source": [
    "print(len(baseline_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMQR8Jeznw73"
   },
   "source": [
    "When deciding which nodes to replace, we need to balance two things:\n",
    "1. Keep enough baseline nodes so the indexes are mostly identical.\n",
    "2. Pick a meaningful section of the paper (not just references, etc.).\n",
    "   \n",
    "In our case, the baseline split produced **39 nodes**. A good rule of thumb is to replace about 20â€“25% of the nodes. Thatâ€™s large enough to see an effect, but small enough that the rest of the dataset remains constant. We chose the range 15â€“25, which corresponds to the middle of the paper.\n",
    "\n",
    "We will create the helper function that replaces the baseline slice [15:25] with enriched nodes from \"nodes_1\" or \"nodes_2\". The rest of the baseline stays intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwcRj7j6nw73"
   },
   "outputs": [],
   "source": [
    "def splice(orig, replacement, start=15, end=25):\n",
    "    # keep same length, swap slice [start:end] with enriched nodes\n",
    "    return orig[:start] + replacement[start:end] + orig[end:]\n",
    "\n",
    "# mostly baseline nodes, with titles added in positions 15â€“25\n",
    "nodes_for_index_1 = splice(baseline_nodes, nodes_1, 15, 25)\n",
    "\n",
    "# mostly baseline nodes, with titles added in positions 15â€“25\n",
    "nodes_for_index_2 = splice(baseline_nodes, nodes_2, 15, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNfXO5z-nw73"
   },
   "source": [
    "**Creating Embeddings**\n",
    "\n",
    "Now weâ€™ll embed and index each one with the same embedding model `\"text-embedding-3-small\"`. Creating a `VectorStoreIndex` from nodes automatically computes embeddings for those nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAILnWoWnw73"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=OPENAI_EMBED_MODEL)\n",
    "\n",
    "# baseline only\n",
    "index_0 = VectorStoreIndex(baseline_nodes, embed_model=embed_model, show_progress=True)\n",
    "\n",
    "# baseline with the slice replaced by titles\n",
    "index_1 = VectorStoreIndex(nodes_for_index_1, embed_model=embed_model, show_progress=True)\n",
    "\n",
    "# baseline with the slice replaced by titles + Q&A + summary\n",
    "index_2 = VectorStoreIndex(nodes_for_index_2, embed_model=embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHfk3w6Qnw73"
   },
   "source": [
    "**Querying**\n",
    "\n",
    "Next, we'll create three query engines with identical parameter `similarity_top_k=1`, so each returns the single most relevant node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQQEidLNnw73"
   },
   "outputs": [],
   "source": [
    "query = \"What metrics are commonly used to evaluate text generation quality, and what are their limitations according to the paper?\"\n",
    "\n",
    "query_engine_0 = index_0.as_query_engine(similarity_top_k=1)\n",
    "query_engine_1 = index_1.as_query_engine(similarity_top_k=1)\n",
    "query_engine_2 = index_2.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-KI3QjUnw73"
   },
   "source": [
    "Each index is queried with the exact same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdJFyB0Xnw73"
   },
   "outputs": [],
   "source": [
    "response_0 = query_engine_0.query(query)\n",
    "response_1 = query_engine_1.query(query)\n",
    "response_2 = query_engine_2.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVltIL6nw74"
   },
   "source": [
    "Let's observe the results:\n",
    "\n",
    "- **BASELINE**:\n",
    "In the baseline setup, the system produced an answer mentioning BLEU, ROUGE, METEOR, and perplexity. However, the retrieved source was from page 20, which only contains references. This means the model did not actually ground its answer in the paper, but instead pulled in \"classic NLP metrics\" from its prior knowledge. The result looks plausible on the surface but is ultimately a hallucination because the metrics are not discussed in that section of the PDF.\n",
    "\n",
    "\n",
    "- **TITLES**:\n",
    "With titles added as metadata, the answer changed noticeably. This time, the system retrieved content from **page 34**, where the paper discusses evaluation benchmarks such as HELM, MMLU-Pro, and GPQA. The answer now highlighted evaluation methods like mathematical equivalence, instruction following, and user chat assessments, as well as the problem of benchmarks discouraging \"I donâ€™t know\" responses. This aligns much more closely with the original text in Appendix F, making the response both faithful and relevant.\n",
    "\n",
    "\n",
    "- **TITLES + Q&A + SUMMARY**:\n",
    "This version performed similarly to the titles-only case. It also pointed to page 34 and generated an answer that covered mathematical equivalence, instruction following, penalties for abstention, and hallucination risks. While this was still grounded in the correct part of the paper and avoided the hallucination seen in the baseline, it did not provide a substantial improvement over the titles-only approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dY7Oc5Ncnw74"
   },
   "outputs": [],
   "source": [
    "print(\"\\n[BASELINE]\\n\", response_0.response)\n",
    "print(\"\\n[TITLES]\\n\", response_1.response)\n",
    "print(\"\\n[TITLES + Q&A + SUMMARY]\\n\", response_2.response)\n",
    "\n",
    "def show_sources(resp, k=1):\n",
    "    for i, sn in enumerate(resp.source_nodes[:k], 1):\n",
    "        md = sn.node.metadata or {}\n",
    "        print(f\"\\nSource {i} | page={md.get('page_label')} | title={md.get('document_title')}\")\n",
    "        print(sn.node.get_content(metadata_mode=MetadataMode.NONE)[:400], \"\\n---------------\")\n",
    "\n",
    "print(\"\\n SOURCES: BASELINE\")\n",
    "show_sources(response_0)\n",
    "\n",
    "print(\"\\n SOURCES: TITLES\")\n",
    "show_sources(response_1)\n",
    "\n",
    "print(\"\\n SOURCES: TITLES + Q&A + SUMMARY\")\n",
    "show_sources(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Nj-WX5Dnw74"
   },
   "source": [
    "In our experiment, the document included all pages, even those containing references. That meant **the retriever could sometimes pull from irrelevant sections**, as we saw with the baseline answer. Metadata enrichment helped correct this problem by steering retrieval toward more meaningful content.\n",
    "\n",
    "In practice, however, we should **pre-process documents and exclude irrelevant sections such as references or bibliographies**. By removing these, we reduce noise in the retrieval stage and make it more likely that answers are grounded in the substantive parts of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQMILxH8_VJl"
   },
   "source": [
    "# 4. Persistent Storage\n",
    "\n",
    "Once weâ€™ve decided which pages to keep and which metadata to enrich (e.g., titles only, pages with references removed), we can persist that final node set, for example, in ChromaDB. We will take the enriched nodes stored in \"nodes_1\", embed them with the same embedding model (`text-embedding-3-small`), and write those vectors into a Chroma collection we can reopen in future notebook's sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Eb4mZFNnw74"
   },
   "outputs": [],
   "source": [
    "from chromadb import PersistentClient\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "embed_model = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygwt43vmnw74"
   },
   "source": [
    "In this code below, we connect our pipeline to ChromaDB. We start by opening (or creating) a Chroma database on disk, then define a collection called \"LLM_titles_only_v1\" where our vectors will be stored. We'll build a `VectorStoreIndex` from our enriched nodes using the embedding model and route them into Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aJwLQa9nw74"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "CHROMA_PATH = \"./chroma_database\"\n",
    "client = PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
    "\n",
    "# Routing vectors into Chroma via StorageContext\n",
    "vector_store = ChromaVectorStore(chroma_collection = collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes_1,\n",
    "    storage_context = storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63g3UiALnw74"
   },
   "source": [
    "When we come back in a new session, we just need to wrap the existing Chroma collection and set the index as query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw3rll0znw74"
   },
   "outputs": [],
   "source": [
    "client = PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,  # query-time embeddings must match!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOPUt2Jgnw74"
   },
   "outputs": [],
   "source": [
    "qe = index.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1koAFwPnw74"
   },
   "outputs": [],
   "source": [
    "query = \"What are the conclusions about hallucinations of language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlCd3ke7nw74"
   },
   "outputs": [],
   "source": [
    "response = qe.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSiZNpHznw74"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
