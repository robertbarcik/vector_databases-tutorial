{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Introduction\n",
    "In this exercise, we will learn how to **enrich documents with metadata** before indexing them for RAG. So instead of only storing raw text, we’ll add useful information such as titles, summaries or example questions/answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "# Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook.\n",
    "\n",
    "Also please, upload the file called why-language-models-hallucinate.pdf to the Data Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Install required libraries with working versions\n",
    "!pip install -q llama-index-core==0.14.6 llama-index-embeddings-openai==0.5.1 \\\n",
    "    llama-index-llms-openai==0.6.6 openai==1.109.1 \\\n",
    "    chromadb==1.2.2 llama-index-vector-stores-chroma==0.5.3 \\\n",
    "    llama-index-readers-file llama-parse\n",
    "\n",
    "print(\"✅ All libraries installed successfully!\")\n",
    "print(\"⚠️  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "source": [
    "# 1. Loading the data\n",
    "\n",
    "We are going to work with the PDF file \"why-language-models-hallucinate.pdf\" (a recent OpenAI research piece that explores the statistical reasons behind model hallucinations) and load it using `SimpleDirectoryReader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('✅ API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('❌ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('❌ ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('✅ Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'🧠 Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = await SimpleDirectoryReader(input_files=[\"why-language-models-hallucinate.pdf\"]).aload_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7rdi0pe82wk",
   "source": "## 1.1 Preparing Metadata for Numeric Filtering\n\nBy default, `SimpleDirectoryReader` stores `page_label` as a **string** (`'1'`, `'2'`, etc.). However, when we want to use **comparison operators** like \"greater than or equal\" (`>=`) or \"less than or equal\" (`<=`) for filtering, vector databases like ChromaDB require **numeric values** (int or float).\n\nThis is a common data type issue you'll encounter when building RAG systems. The solution is simple: convert `page_label` to integers immediately after loading documents.\n\n**Why this matters:**\n- ✅ Enables numeric range filtering (e.g., \"pages 12-15\", \"pages >= 30\")\n- ✅ Prevents type mismatch errors during queries\n- ✅ Standard practice in production RAG systems\n\nLet's convert the page labels now:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "omco1yi0par",
   "source": "# Convert page_label from string to int for numeric filtering\n# NOTE: ChromaDB requires int/float for comparison operators (>=, <=, >, <)\nfor doc in documents:\n    if 'page_label' in doc.metadata:\n        try:\n            doc.metadata['page_label'] = int(doc.metadata['page_label'])\n        except (ValueError, TypeError):\n            pass  # Keep original if conversion fails\n\nprint(\"✓ Converted page_label to integers for numeric filtering\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "source": [
    "The file has 36 pages, so the data connector created 36 document objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "7"
   },
   "outputs": [],
   "source": [
    "print(\"Number of document objects:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "# 2. Controlling Metadata Visibility\n",
    "\n",
    "Before we enrich our documents, we first need to understand what information is already stored in them.\n",
    "\n",
    "\n",
    "\n",
    "We already know that each document carries not just the text but also the metadata. Additionally, there are 2 parameters that control what parts of that metadata will be visible to the embedding model and to the LLM at query time:\n",
    "\n",
    "`excluded_embed_metadata_keys`:\n",
    "- tells LlamaIndex which metadata **should not be included when creating embeddings**\n",
    "- embeddings are designed to capture the semantic meaning of content, and technical details such as file size or last modified date do not add any useful meaning.\n",
    "\n",
    "`excluded_llm_metadata_keys`:\n",
    "- tells LlamaIndex which metadata **should not be sent to the LLM when the document is retrieved at query time**\n",
    "- the reason for controlling this is that some metadata, like we can add the author, can provide valuable context to the LLM when generating an answer, while other metadata would only distract the model and reduce the clarity of its response\n",
    "\n",
    "These two parameters give us **control over what information flows into embeddings and into the LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "## 2.1 Manually Constructed Documents\n",
    "\n",
    "Most of the time we let LlamaIndex create documents automatically when we load files. But sometimes we want to **build a Document manually**. This is useful when:\n",
    "1. Our data doesn’t come from a file (e.g., from a database or an API)\n",
    "2. We want to attach custom metadata up front\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "source": [
    "The example below (adapted from the documentation) shows how we can construct a custom Document and explicitly control what metadata is included when building embeddings.\n",
    "\n",
    "In this case, we give the document some metadata (\"file_name\", \"category\" and \"author\"). But notice that we tell LlamaIndex to **exclude \"file_name\"** from embeddings by setting `excluded_embed_metadata_keys`. This makes sense, because the actual file name is not semantically meaningful and would only add noise to the embedding space. The category (\"finance\") and author (\"LlamaIndex\"), however, may carry useful meaning for semantic search, so we leave them in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "# What the Embedding model will see\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    excluded_embed_metadata_keys=[\"file_name\"]\n",
    ")\n",
    "\n",
    "print(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "13"
   },
   "source": [
    "Just like we control which metadata flows into the embedding model, we can also **decide which metadata the LLM will receive when it is asked to answer a query**. This is important because the LLM doesn’t only use the raw text of a chunk. It can also use metadata as extra context to generate a better answer.\n",
    "\n",
    "In the example below, we create a custom Document with the same metadata fields. This time, however, we tell LlamaIndex to exclude the category from what the LLM sees. That means when the document is retrieved later, the model will still see the file name (so it knows the source) and the author (which may add credibility). In this case, the category is redundant - the text already makes it clear that the topic is finance, so it likely won’t affect the LLM’s response and only takes up prompt space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "# What the LLM model will see\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    excluded_llm_metadata_keys=[\"category\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "15"
   },
   "source": [
    "Some metadata is more useful for embeddings, some is more useful for the LLM, and some works for both. **Whether we exclude/add a certain information depends on our use case**: are we trying to keep embeddings clean, or give the LLM more context? In real-world projects, this is a design choice you make depending on how much metadata adds value versus noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "16"
   },
   "source": [
    "By default, LlamaIndex already takes care of formatting metadata in a clean way, and in practice you usually don’t need to change it. However, you can **customize the formatting** if you want more readable prompts for the LLM or if you want the metadata formatted in a certain style to match your company’s pipelines or prompt style.\n",
    "\n",
    "Optional parameters:\n",
    "- `metadata_seperator` - Sets the character(s) between different pieces of metadata. The default is a newline (`\"\\n\"`).\n",
    "- `metadata_template` - Defines how each key-value pair is shown. Both `{key}` and `{value}` must be included.\n",
    "- `text_template` - takes two variables: `metadata_str` and `content`\n",
    "\n",
    "This doesn’t change what information is sent, only how it is displayed. For example, for the LLM a cleaner format can sometimes help it parse metadata more naturally.\n",
    "\n",
    "In our exercise, we’ll try a custom format just to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "17"
   },
   "outputs": [],
   "source": [
    "# Formatting\n",
    "document = Document(\n",
    "    text=\"This is a short snippet of a super-customized document that will go to the model\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    metadata_seperator=\", \",\n",
    "    metadata_template=\"{key}:{value}\",\n",
    "    text_template=\"Metadata:\\n{metadata_str}\\n------\\nContent:\\n{content}\",\n",
    ")\n",
    "\n",
    "print(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "18"
   },
   "source": [
    "Now let’s return to the real documents created automatically when we loaded our PDF. Each of these documents already comes with some metadata attached.\n",
    "\n",
    "- \"file_path\" can be useful (it tells us where the chunk came from)\n",
    "- \"page_label\" usually does not add much value for embeddings (handy to keep for LLM if you want the reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "19"
   },
   "outputs": [],
   "source": [
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "20"
   },
   "source": [
    "Let's exclude page labels - we can loop through all documents, adjust their formatting template and tell LlamaIndex not to include \"page_label\" in the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "21"
   },
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    # Defining the content/metadata template\n",
    "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "\n",
    "    # Excluding page label from embedding\n",
    "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "22"
   },
   "source": [
    "Let's check the transformation - page label should not be included in the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "23"
   },
   "outputs": [],
   "source": [
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "24"
   },
   "source": [
    "# 3. Building a RAG Pipeline with Metadata Enrichment\n",
    "\n",
    "Now we’re getting to the interesting and fun part of the notebook. Up to this point, our documents only carried basic metadata like file names and page labels. That’s useful for organizing files, but it doesn’t really help a RAG system retrieve more accurate answers. So this is what we’re going to do: make our RAG pipeline smarter by **enriching each document chunk with additional context** such as short titles, summaries and example Q&As. To do so, we'll **use a language model  `gpt-5-nano` to generate this metadata**.\n",
    "\n",
    "To really test whether this helps, we’ll build **3 different versions of our nodes** (chunks of text):\n",
    "- Baseline nodes (`nodes_0`) - only the basic metadata\n",
    "- Title-enriched nodes (`nodes_1`) – chunks labeled with short, descriptive titles.\n",
    "- Fully enriched nodes (`nodes_2`) – chunks augmented with titles, summaries and example Q&A pairs.\n",
    "\n",
    "We’ll follow three main steps in this experiment:\n",
    "1. **Splitting the data**: we'll break the PDF into smaller, manageable chunks\n",
    "2. **Creating three versions of nodes**\n",
    "3. **Building and testing RAG indexes**: we’ll run the same queries against each node set and compare the results to see how much metadata enrichment improves retrieval and answers\n",
    "\n",
    "> NOTE: This setup is inspired by the official LlamaIndex metadata extraction [example](https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MetadataExtraction_LLMSurvey/#automated-metadata-extraction-for-better-retrieval-synthesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "25"
   },
   "source": [
    "**OpenAI's Language model for transformations**\n",
    "\n",
    "We will use OpenAI’s `gpt-5-nano` model which is fast, affordable, and accurate enough for our metadata extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "26"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Language model\n",
    "llm_transformations = OpenAI(\n",
    "    model = OPENAI_MODEL,\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "27"
   },
   "source": [
    "## 3.1 Splitting the data\n",
    "\n",
    "First, we need to prepare our documents for transformation by splitting them into smaller chunks. Large documents cannot be processed effectively all at once. We'll use `SentenceSplitter` which splits the content into 1024 tokens and also adds an overlap of 128 tokens. The overlap ensures that if important information appears at the boundary of one chunk, it is also present in the next chunk, so nothing is lost. Parameter `separator` simply tells the splitter to break text along spaces (keeping words intact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "28"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    separator = \" \",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "29"
   },
   "source": [
    "## 3.2 Creating three versions of Nodes\n",
    "\n",
    "We’ll now create three parallel versions of our corpus so we can run a fair comparison later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "30"
   },
   "source": [
    "### 3.2.1 Creating baseline nodes (split only)\n",
    "First, we create the baseline: chunks produced by the splitter with no metadata enrichment. This gives us a control group. Any improvement we see later can be attributed to the extra metadata, not to changes in chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "31"
   },
   "outputs": [],
   "source": [
    "# Baseline nodes\n",
    "baseline_nodes = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "32"
   },
   "outputs": [],
   "source": [
    "baseline_nodes[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "33"
   },
   "source": [
    "### 3.2.2 Enriched nodes (titles extraction)\n",
    "\n",
    "Next, we'll add short, descriptive titles for each chunk of text. These will be labels for each chunk and often help the retriever match user intent to the right passage.\n",
    "\n",
    "To do this, we use `TitleExtractor` which takes an LLM and generates a title for each node. We also set the parameter `nodes = 5` so that up to 5 chunks are processed in one request, making the process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "34"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import TitleExtractor\n",
    "\n",
    "title_extractor = TitleExtractor(llm = llm_transformations, nodes = 5)\n",
    "title_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "35"
   },
   "source": [
    "Next, we run this transformation using `IngestionPipeline`. The pipeline executes a sequence of transformations, in our case, splitting the text into chunks and then adding titles. We set `in_place=False` to make sure we don’t overwrite our baseline nodes. Instead, we produce a separate list (stored in \"nodes_1\") for A/B test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "36"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline_titles = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "37"
   },
   "outputs": [],
   "source": [
    "# Running the pipeline\n",
    "nodes_1 = pipeline_titles.run(\n",
    "    documents = documents,\n",
    "    in_place = False,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "38"
   },
   "source": [
    "### 📝 EXERCISE 1: Explore Metadata Extraction\n",
    "\n",
    "\n",
    "**Your task:**\n",
    "1. Compare a baseline node (without metadata) to an enriched node (with title extraction)\n",
    "2. Display the content of `baseline_nodes[5]` using `.get_content()`\n",
    "3. Display the content of `nodes_1[5]` (with title metadata) using `.get_content(metadata_mode=MetadataMode.LLM)`\n",
    "4. Observe: What additional information does the title provide? How might this help retrieval?\n",
    "\n",
    "\n",
    "**Hint:** Use `MetadataMode.LLM` to see what the language model receives, including metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "39"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "40"
   },
   "source": [
    "### 3.2.3 Fully enriched nodes (titles + Q&A + summary extraction)\n",
    "\n",
    "For the richest version of our nodes, we’ll go beyond titles and also add example Q&A pairs and short summaries.\n",
    "\n",
    "**Q&A pairs simulate how a real user might query the system and what kind of response a chunk could provide**. This makes the retriever’s job easier because each chunk carries hints about the kinds of questions it can answer. In practice, adding Q&A metadata often improves recall (finding the right chunk) and helps the system produce more useful answers. We’ll use `QuestionsAnsweredExtractor` and set `questions = 3`, which asks the LLM to generate three realistic Q&A pairs per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "41"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import QuestionsAnsweredExtractor\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(llm = llm_transformations, questions = 3)\n",
    "qa_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "42"
   },
   "source": [
    "**Summaries capture the core ideas of each chunk in a compact form**. They provide another layer of metadata that’s especially helpful when users ask broader or high-level questions. We’ll use `SummaryExtractor` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "43"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import SummaryExtractor\n",
    "\n",
    "summary_extractor = SummaryExtractor(llm = llm_transformations)\n",
    "summary_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "44"
   },
   "source": [
    "Both transformations run inside the same pipeline, along with the SentenceSplitter and TitleExtractor, so each chunk ends up with a title, a short summary and 3 example Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "45"
   },
   "outputs": [],
   "source": [
    "# titles + Q&A + summary\n",
    "pipeline_rich = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor,\n",
    "        qa_extractor,\n",
    "        summary_extractor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "46"
   },
   "outputs": [],
   "source": [
    "# Running the pipeline\n",
    "nodes_2 = pipeline_rich.run(\n",
    "    documents = documents,\n",
    "    in_place = False,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "47"
   },
   "outputs": [],
   "source": [
    "print(nodes_2[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "48"
   },
   "source": [
    "**Splicing Baseline and Enriched Nodes**\n",
    "\n",
    "To fairly test the effect of metadata enrichment, we don’t want to rebuild our dataset in three completely separate ways. That would make it difficult to know if differences in answers are due to enrichment or simply because the data was reprocessed differently. Instead, we keep most of the dataset identical and **replace only a small slice of nodes with enriched versions**.\n",
    "\n",
    "This creates a controlled experiment:\n",
    "- All three indexes contain the same core content.\n",
    "- The only difference is that in \"index1\" and \"index2\", a chosen section of the document is enriched with new metadata (titles, or titles + Q&A + summaries).\n",
    "- If the enriched versions produce better answers, we can be confident the improvement comes from the metadata itself, not from unrelated differences.\n",
    "\n",
    "  \n",
    "First let's check the number of nodes in the baseline split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "49"
   },
   "outputs": [],
   "source": [
    "print(len(baseline_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "50"
   },
   "source": [
    "When deciding which nodes to replace, we need to balance two things:\n",
    "1. Keep enough baseline nodes so the indexes are mostly identical.\n",
    "2. Pick a meaningful section of the paper (not just references, etc.).\n",
    "   \n",
    "In our case, the baseline split produced **39 nodes**. A good rule of thumb is to replace about 20–25% of the nodes. That’s large enough to see an effect, but small enough that the rest of the dataset remains constant. We chose the range 15–25, which corresponds to the middle of the paper.\n",
    "\n",
    "We will create the helper function that replaces the baseline slice [15:25] with enriched nodes from \"nodes_1\" or \"nodes_2\". The rest of the baseline stays intact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "id": "51"
   },
   "outputs": [],
   "source": [
    "def splice(orig, replacement, start=15, end=25):\n",
    "    # keep same length, swap slice [start:end] with enriched nodes\n",
    "    return orig[:start] + replacement[start:end] + orig[end:]\n",
    "\n",
    "# mostly baseline nodes, with titles added in positions 15–25\n",
    "nodes_for_index_1 = splice(baseline_nodes, nodes_1, 15, 25)\n",
    "\n",
    "# mostly baseline nodes, with titles added in positions 15–25\n",
    "nodes_for_index_2 = splice(baseline_nodes, nodes_2, 15, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "52"
   },
   "source": [
    "**Creating Embeddings**\n",
    "\n",
    "Now we’ll embed and index each one with the same embedding model `\"text-embedding-3-small\"`. Creating a `VectorStoreIndex` from nodes automatically computes embeddings for those nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "53"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=OPENAI_EMBED_MODEL)\n",
    "\n",
    "# baseline only\n",
    "index_0 = VectorStoreIndex(baseline_nodes, embed_model=embed_model, show_progress=True)\n",
    "\n",
    "# baseline with the slice replaced by titles\n",
    "index_1 = VectorStoreIndex(nodes_for_index_1, embed_model=embed_model, show_progress=True)\n",
    "\n",
    "# baseline with the slice replaced by titles + Q&A + summary\n",
    "index_2 = VectorStoreIndex(nodes_for_index_2, embed_model=embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "54"
   },
   "source": [
    "**Querying**\n",
    "\n",
    "Next, we'll create three query engines with identical parameter `similarity_top_k=1`, so each returns the single most relevant node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "id": "55"
   },
   "outputs": [],
   "source": [
    "query = \"What metrics are commonly used to evaluate text generation quality, and what are their limitations according to the paper?\"\n",
    "\n",
    "query_engine_0 = index_0.as_query_engine(similarity_top_k=1)\n",
    "query_engine_1 = index_1.as_query_engine(similarity_top_k=1)\n",
    "query_engine_2 = index_2.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "id": "56"
   },
   "source": [
    "Each index is queried with the exact same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "id": "57"
   },
   "outputs": [],
   "source": [
    "response_0 = query_engine_0.query(query)\n",
    "response_1 = query_engine_1.query(query)\n",
    "response_2 = query_engine_2.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "58"
   },
   "source": [
    "Let's observe the results:\n",
    "\n",
    "- **BASELINE**:\n",
    "In the baseline setup, the system produced an answer mentioning BLEU, ROUGE, METEOR, and perplexity. However, the retrieved source was from page 20, which only contains references. This means the model did not actually ground its answer in the paper, but instead pulled in \"classic NLP metrics\" from its prior knowledge. The result looks plausible on the surface but is ultimately a hallucination because the metrics are not discussed in that section of the PDF.\n",
    "\n",
    "\n",
    "- **TITLES**:\n",
    "With titles added as metadata, the answer changed noticeably. This time, the system retrieved content from **page 34**, where the paper discusses evaluation benchmarks such as HELM, MMLU-Pro, and GPQA. The answer now highlighted evaluation methods like mathematical equivalence, instruction following, and user chat assessments, as well as the problem of benchmarks discouraging \"I don’t know\" responses. This aligns much more closely with the original text in Appendix F, making the response both faithful and relevant.\n",
    "\n",
    "\n",
    "- **TITLES + Q&A + SUMMARY**:\n",
    "This version performed similarly to the titles-only case. It also pointed to page 34 and generated an answer that covered mathematical equivalence, instruction following, penalties for abstention, and hallucination risks. While this was still grounded in the correct part of the paper and avoided the hallucination seen in the baseline, it did not provide a substantial improvement over the titles-only approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "59"
   },
   "outputs": [],
   "source": [
    "print(\"\\n[BASELINE]\\n\", response_0.response)\n",
    "print(\"\\n[TITLES]\\n\", response_1.response)\n",
    "print(\"\\n[TITLES + Q&A + SUMMARY]\\n\", response_2.response)\n",
    "\n",
    "def show_sources(resp, k=1):\n",
    "    for i, sn in enumerate(resp.source_nodes[:k], 1):\n",
    "        md = sn.node.metadata or {}\n",
    "        print(f\"\\nSource {i} | page={md.get('page_label')} | title={md.get('document_title')}\")\n",
    "        print(sn.node.get_content(metadata_mode=MetadataMode.NONE)[:400], \"\\n---------------\")\n",
    "\n",
    "print(\"\\n SOURCES: BASELINE\")\n",
    "show_sources(response_0)\n",
    "\n",
    "print(\"\\n SOURCES: TITLES\")\n",
    "show_sources(response_1)\n",
    "\n",
    "print(\"\\n SOURCES: TITLES + Q&A + SUMMARY\")\n",
    "show_sources(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "60"
   },
   "source": [
    "In our experiment, the document included all pages, even those containing references. That meant **the retriever could sometimes pull from irrelevant sections**, as we saw with the baseline answer. Metadata enrichment helped correct this problem by steering retrieval toward more meaningful content.\n",
    "\n",
    "In practice, however, we should **pre-process documents and exclude irrelevant sections such as references or bibliographies**. By removing these, we reduce noise in the retrieval stage and make it more likely that answers are grounded in the substantive parts of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "61"
   },
   "source": [
    "# 4. Persistent Storage\n",
    "\n",
    "Once we’ve decided which pages to keep and which metadata to enrich (e.g., titles only, pages with references removed), we can persist that final node set, for example, in ChromaDB. We will take the enriched nodes stored in \"nodes_1\", embed them with the same embedding model (`text-embedding-3-small`), and write those vectors into a Chroma collection we can reopen in future notebook's sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "64"
   },
   "outputs": [],
   "source": [
    "from chromadb import PersistentClient\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "embed_model = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "id": "65"
   },
   "source": [
    "In this code below, we connect our pipeline to ChromaDB. We start by opening (or creating) a Chroma database on disk, then define a collection called \"LLM_titles_only_v1\" where our vectors will be stored. We'll build a `VectorStoreIndex` from our enriched nodes using the embedding model and route them into Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "id": "66"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "CHROMA_PATH = \"./chroma_database\"\n",
    "client = PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
    "\n",
    "# Routing vectors into Chroma via StorageContext\n",
    "vector_store = ChromaVectorStore(chroma_collection = collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes_1,\n",
    "    storage_context = storage_context,\n",
    "    embed_model=embed_model,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "67"
   },
   "source": [
    "When we come back in a new session, we just need to wrap the existing Chroma collection and set the index as query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "id": "68"
   },
   "outputs": [],
   "source": [
    "client = PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model,  # query-time embeddings must match!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "id": "69"
   },
   "outputs": [],
   "source": [
    "qe = index.as_query_engine(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "id": "70"
   },
   "outputs": [],
   "source": [
    "query = \"What are the conclusions about hallucinations of language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "id": "71"
   },
   "outputs": [],
   "source": [
    "response = qe.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "72"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7xu0prno0yq",
   "source": [
    "# 5. Using Metadata Filters in Queries\n",
    "\n",
    "In the previous sections, we enriched our documents with metadata like titles, summaries, and Q&A pairs. But we haven't yet shown you how to actually **USE** that metadata to filter your queries. This is one of the most powerful features of metadata enrichment.\n",
    "\n",
    "## Why Filter with Metadata?\n",
    "\n",
    "Imagine you have thousands of documents from different sources, topics, or time periods. Sometimes you don't want to search through ALL of them—you want to search only within:\n",
    "- A specific document or set of documents\n",
    "- A particular category or topic\n",
    "- Content from a certain time period\n",
    "- Documents by a specific author\n",
    "\n",
    "**Metadata filtering lets you do exactly this.** It combines semantic search (finding similar content) with structured filtering (like SQL WHERE clauses).\n",
    "\n",
    "## How It Works\n",
    "\n",
    "LlamaIndex allows you to add filters to your queries using the `MetadataFilters` class. You can filter by:\n",
    "- **Exact match**: `key == value`\n",
    "- **In list**: `key IN [value1, value2, ...]`\n",
    "- **Greater than / Less than**: `key > value`, `key < value`\n",
    "- **Not equal**: `key != value`\n",
    "\n",
    "Let's see this in action with our enriched nodes."
   ],
   "metadata": {
    "id": "7xu0prno0yq"
   }
  },
  {
   "cell_type": "markdown",
   "id": "q5pyh1up1cb",
   "source": "## 5.1 Example: Filter by Page Range\n\nLet's say you remember reading something about confidence targets in the post-training section, and you think it was somewhere around pages 12-15. Instead of searching the entire document, you can filter to only retrieve from that page range.\n\n**Important Note:** This works because we converted `page_label` to integers in Section 1.1. ChromaDB requires numeric values (int or float) for comparison operators like `>=` and `<=`. If you skip that conversion step, you'll get a `ValueError` saying \"Expected operand value to be an int or a float\".\n\n**How this helps:**\n- ✅ Faster retrieval (fewer nodes to search)\n- ✅ More focused results\n- ✅ Avoid pulling irrelevant content from other sections",
   "metadata": {
    "id": "q5pyh1up1cb"
   }
  },
  {
   "cell_type": "code",
   "id": "4hutczknf68",
   "source": "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n\n# Create a filter: only retrieve from pages 12-15\n# NOTE: We use integer values (converted in Section 1.1) for comparison operators\npage_filters = MetadataFilters(\n    filters=[\n        MetadataFilter(key=\"page_label\", value=12, operator=FilterOperator.GTE),  # >= 12\n        MetadataFilter(key=\"page_label\", value=15, operator=FilterOperator.LTE),  # <= 15\n    ]\n)\n\n# Create a query engine with filters\nfiltered_query_engine = index.as_query_engine(\n    similarity_top_k=2,  # Still retrieve top 2, but only from filtered pages\n    filters=page_filters\n)\n\n# Query with filter applied\nquery = \"What are confidence targets?\"\nresponse = filtered_query_engine.query(query)\n\nprint(\"ANSWER:\")\nprint(response.response)\nprint(\"\\n\" + \"=\"*80)\nprint(\"RETRIEVED SOURCES (should only be from pages 12-15):\")\nprint(\"=\"*80)\n\nfor i, node in enumerate(response.source_nodes, 1):\n    print(f\"\\nSource {i}:\")\n    print(f\"  Page: {node.metadata.get('page_label', 'N/A')}\")\n    print(f\"  Title: {node.metadata.get('document_title', 'N/A')}\")\n    print(f\"  Score: {node.score:.4f}\")\n    print(f\"  Preview: {node.text[:200]}...\")",
   "metadata": {
    "id": "4hutczknf68"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4maxlb0di6b",
   "source": [
    "**What happened here?**\n",
    "\n",
    "1. We created filters that say \"only look at pages between 12 and 15\"\n",
    "2. The retriever searched ONLY within those pages (ignoring everything else)\n",
    "3. Even though the entire document is indexed, we get results only from the filtered range\n",
    "\n",
    "**When is this useful?**\n",
    "- You have a multi-document collection and want to search within one document\n",
    "- You want to exclude certain sections (like references or appendices)\n",
    "- You're looking for information from a specific time period or category"
   ],
   "metadata": {
    "id": "4maxlb0di6b"
   }
  },
  {
   "cell_type": "markdown",
   "id": "0oicld0izs6",
   "source": [
    "## 5.2 Example: Filter by File Name (Multi-Document Collections)\n",
    "\n",
    "In real-world applications, you often have multiple documents indexed together. Metadata filtering becomes even more powerful here—you can search across all documents OR narrow down to specific ones.\n",
    "\n",
    "Let's demonstrate this concept. Even though we only have one PDF in this notebook, imagine you had indexed multiple research papers. You could filter by `file_name` to search within just one paper."
   ],
   "metadata": {
    "id": "0oicld0izs6"
   }
  },
  {
   "cell_type": "code",
   "id": "686v1sexmc8",
   "source": [
    "# Filter to search only within a specific file\n",
    "file_filter = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"file_name\",\n",
    "            value=\"why-language-models-hallucinate.pdf\",\n",
    "            operator=FilterOperator.EQ  # Exact match\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create query engine with file filter\n",
    "file_filtered_qe = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    filters=file_filter\n",
    ")\n",
    "\n",
    "response = file_filtered_qe.query(\"What are the main conclusions about hallucinations?\")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(response.response)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SOURCES (all from the filtered file):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    print(f\"\\nSource {i}: {node.metadata.get('file_name', 'N/A')} (Page {node.metadata.get('page_label', 'N/A')})\")"
   ],
   "metadata": {
    "id": "686v1sexmc8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n8qipecq7f",
   "source": [
    "**Real-world use case:**\n",
    "\n",
    "Imagine you're building a research assistant that has indexed 100 academic papers. A user asks:\n",
    "> \"What does the 2024 OpenAI paper say about hallucinations?\"\n",
    "\n",
    "Without filtering, the system might retrieve chunks from ANY of the 100 papers. With metadata filtering:\n",
    "```python\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"author\", value=\"OpenAI\", operator=FilterOperator.EQ),\n",
    "        MetadataFilter(key=\"year\", value=\"2024\", operator=FilterOperator.EQ)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Now the search is constrained to ONLY the relevant paper, dramatically improving answer quality."
   ],
   "metadata": {
    "id": "n8qipecq7f"
   }
  },
  {
   "cell_type": "markdown",
   "id": "8u9bci8ki8b",
   "source": [
    "## 5.3 Key Takeaways: Metadata Filtering\n",
    "\n",
    "**When to use it:**\n",
    "- ✅ Multi-document collections (search within specific documents)\n",
    "- ✅ Excluding sections (skip references, appendices, etc.)\n",
    "- ✅ Time-based queries (recent documents only)\n",
    "- ✅ Category-based queries (only technical docs, only legal docs, etc.)\n",
    "\n",
    "**Available filter operators:**\n",
    "- `FilterOperator.EQ` - Equal to (==)\n",
    "- `FilterOperator.NE` - Not equal to (!=)\n",
    "- `FilterOperator.GT` - Greater than (>)\n",
    "- `FilterOperator.LT` - Less than (<)\n",
    "- `FilterOperator.GTE` - Greater than or equal (>=)\n",
    "- `FilterOperator.LTE` - Less than or equal (<=)\n",
    "- `FilterOperator.IN` - Value in list\n",
    "- `FilterOperator.NIN` - Value not in list\n",
    "\n",
    "**Remember:** Metadata filtering works TOGETHER with semantic search. The retriever:\n",
    "1. First applies your filters (narrows down candidates)\n",
    "2. Then does semantic similarity search within those filtered candidates\n",
    "3. Returns the most relevant chunks that match both criteria"
   ],
   "metadata": {
    "id": "8u9bci8ki8b"
   }
  },
  {
   "cell_type": "markdown",
   "id": "c141cl5hpo4",
   "source": [
    "# 6. Response Synthesis Modes\n",
    "\n",
    "So far, we've focused heavily on the **retrieval** side of RAG—how to find the right chunks using semantic search, metadata enrichment, and filtering. But there's another critical component: **how the LLM synthesizes those retrieved chunks into a final answer**.\n",
    "\n",
    "This is called **response synthesis**, and LlamaIndex offers several different modes for doing this. Each mode has different trade-offs in terms of answer quality, context handling, and API cost.\n",
    "\n",
    "## The Problem: What Happens with Multiple Retrieved Chunks?\n",
    "\n",
    "When you set `similarity_top_k=5`, the retriever returns 5 chunks. But how does the LLM use them?\n",
    "\n",
    "**Three challenges:**\n",
    "1. **Context length**: If chunks are long, they might exceed the LLM's context window\n",
    "2. **Information synthesis**: Should chunks be combined, compared, or processed one-by-one?\n",
    "3. **Relevance**: Not all retrieved chunks are equally useful—some might be noise\n",
    "\n",
    "Different response modes solve these challenges in different ways."
   ],
   "metadata": {
    "id": "c141cl5hpo4"
   }
  },
  {
   "cell_type": "markdown",
   "id": "zw8qqygzmer",
   "source": [
    "## 6.1 Response Mode: `compact` (Default)\n",
    "\n",
    "This is the default mode in LlamaIndex.\n",
    "\n",
    "**How it works:**\n",
    "1. Retrieves multiple chunks (e.g., top 5)\n",
    "2. **Concatenates them into a single context string**\n",
    "3. Sends the concatenated context + query to the LLM in ONE request\n",
    "4. LLM generates answer based on all chunks at once\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ **Fast**: Only one LLM call\n",
    "- ✅ **Cheap**: Minimal API cost\n",
    "- ✅ **Good for short chunks**: Works well when all chunks fit in context\n",
    "- ❌ **Context limit risk**: If chunks are too large, might exceed LLM's context window\n",
    "- ❌ **No refinement**: LLM sees everything at once, can't iteratively improve\n",
    "\n",
    "**When to use:**\n",
    "- Default choice for most queries\n",
    "- Short to medium-length chunks\n",
    "- Fast prototyping"
   ],
   "metadata": {
    "id": "zw8qqygzmer"
   }
  },
  {
   "cell_type": "code",
   "id": "ngvv3vtzku",
   "source": [
    "# Example: Compact mode (default)\n",
    "query_engine_compact = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\"  # This is actually the default\n",
    ")\n",
    "\n",
    "query = \"What are the main causes of hallucinations in language models?\"\n",
    "response_compact = query_engine_compact.query(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESPONSE MODE: COMPACT\")\n",
    "print(\"=\"*80)\n",
    "print(response_compact.response)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Number of chunks used: {len(response_compact.source_nodes)}\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "id": "ngvv3vtzku"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bznb0ec0jk6",
   "source": [
    "## 6.2 Response Mode: `refine`\n",
    "\n",
    "This mode uses an **iterative refinement** approach.\n",
    "\n",
    "**How it works:**\n",
    "1. Retrieves multiple chunks (e.g., top 5)\n",
    "2. Sends **chunk 1** to LLM → Get initial answer\n",
    "3. Sends **chunk 2 + previous answer** to LLM → \"Refine the answer based on new context\"\n",
    "4. Sends **chunk 3 + refined answer** to LLM → \"Refine again\"\n",
    "5. Continues until all chunks are processed\n",
    "6. Returns the final refined answer\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ **Better quality**: Iteratively improves the answer with each chunk\n",
    "- ✅ **Handles long contexts**: Processes chunks one at a time, avoids context limits\n",
    "- ✅ **More comprehensive**: Can incorporate information from many chunks sequentially\n",
    "- ❌ **Slower**: Makes N LLM calls (where N = number of chunks)\n",
    "- ❌ **More expensive**: Each refinement costs API tokens\n",
    "- ❌ **Later chunks matter more**: Information from later chunks might overshadow earlier ones\n",
    "\n",
    "**When to use:**\n",
    "- Complex questions requiring information from multiple sources\n",
    "- Long documents where chunks are large\n",
    "- When answer quality is more important than speed/cost"
   ],
   "metadata": {
    "id": "bznb0ec0jk6"
   }
  },
  {
   "cell_type": "code",
   "id": "wanvmoob16g",
   "source": [
    "# Example: Refine mode\n",
    "query_engine_refine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"refine\"\n",
    ")\n",
    "\n",
    "response_refine = query_engine_refine.query(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESPONSE MODE: REFINE\")\n",
    "print(\"=\"*80)\n",
    "print(response_refine.response)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Number of chunks used: {len(response_refine.source_nodes)}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Note: This made 3 LLM calls (one per chunk) to iteratively refine the answer\")"
   ],
   "metadata": {
    "id": "wanvmoob16g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "li9dfdg7qa",
   "source": [
    "## 6.3 Response Mode: `tree_summarize`\n",
    "\n",
    "This mode uses a **hierarchical summarization** approach.\n",
    "\n",
    "**How it works:**\n",
    "1. Retrieves multiple chunks (e.g., top 8)\n",
    "2. Groups them into pairs or small batches\n",
    "3. Summarizes each batch → Creates intermediate summaries\n",
    "4. Groups those summaries and summarizes again\n",
    "5. Repeats until one final summary remains\n",
    "\n",
    "\n",
    "```\n",
    "Chunk1, Chunk2 → Summary A\n",
    "Chunk3, Chunk4 → Summary B\n",
    "Chunk5, Chunk6 → Summary C\n",
    "Chunk7, Chunk8 → Summary D\n",
    "\n",
    "Summary A, B → Summary AB\n",
    "Summary C, D → Summary CD\n",
    "\n",
    "Summary AB, CD → Final Answer\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ **Handles many chunks**: Can process dozens of chunks efficiently\n",
    "- ✅ **Balanced processing**: All chunks contribute equally (no recency bias)\n",
    "- ✅ **Good for summarization**: Excellent for \"summarize this document\" type queries\n",
    "- ❌ **Multiple LLM calls**: log(N) calls where N = number of chunks\n",
    "- ❌ **Loss of detail**: Hierarchical summarization can lose fine-grained details\n",
    "- ❌ **Slower**: More calls than compact, fewer than refine\n",
    "\n",
    "**When to use:**\n",
    "- Large number of retrieved chunks (10+)\n",
    "- Summarization tasks\n",
    "- When you want balanced consideration of all chunks"
   ],
   "metadata": {
    "id": "li9dfdg7qa"
   }
  },
  {
   "cell_type": "code",
   "id": "xgrzlk20wss",
   "source": [
    "# Example: Tree Summarize mode\n",
    "query_engine_tree = index.as_query_engine(\n",
    "    similarity_top_k=4,  # Use 4 chunks to show tree structure\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "response_tree = query_engine_tree.query(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESPONSE MODE: TREE_SUMMARIZE\")\n",
    "print(\"=\"*80)\n",
    "print(response_tree.response)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Number of chunks used: {len(response_tree.source_nodes)}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n💡 Note: This used hierarchical summarization (pairs of chunks → intermediate summaries → final answer)\")"
   ],
   "metadata": {
    "id": "xgrzlk20wss"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h40x3x3qk96",
   "source": [
    "## 6.4 Comparing Response Modes Side-by-Side\n",
    "\n",
    "Let's compare all three modes on the same query to see the differences:"
   ],
   "metadata": {
    "id": "h40x3x3qk96"
   }
  },
  {
   "cell_type": "code",
   "id": "zvkjc4v26o",
   "source": [
    "# Compare all three modes\n",
    "comparison_query = \"Summarize the paper's main findings about why language models hallucinate\"\n",
    "\n",
    "modes = [\"compact\", \"refine\", \"tree_summarize\"]\n",
    "responses = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"QUERY: {comparison_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for mode in modes:\n",
    "    qe = index.as_query_engine(similarity_top_k=3, response_mode=mode)\n",
    "    response = qe.query(comparison_query)\n",
    "    responses[mode] = response\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODE: {mode.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(response.response)\n",
    "    print(f\"\\nChunks used: {len(response.source_nodes)}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "zvkjc4v26o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gt4f704rgn",
   "source": [
    "## 6.5 Key Takeaways: Response Synthesis Modes\n",
    "\n",
    "### **Quick Reference Table:**\n",
    "\n",
    "| Mode | LLM Calls | Speed | Cost | Quality | Best For |\n",
    "|------|-----------|-------|------|---------|----------|\n",
    "| **compact** | 1 | Fast | Cheap | ⭐⭐ Good | Default choice, short chunks |\n",
    "| **refine** | N (per chunk) | Slow | Expensive | ⭐⭐⭐ Best | Complex queries, detail-oriented |\n",
    "| **tree_summarize** | log(N) | Medium |  Medium | ⭐⭐ Good | Many chunks, summarization |\n",
    "\n",
    "### **Decision Guide:**\n",
    "\n",
    "**Use `compact` when:**\n",
    "- ✅ You have 2-5 short chunks\n",
    "- ✅ Fast response time is important\n",
    "- ✅ You're prototyping or testing\n",
    "- ✅ Cost is a concern\n",
    "\n",
    "**Use `refine` when:**\n",
    "- ✅ Answer quality is paramount\n",
    "- ✅ You need comprehensive answers from multiple sources\n",
    "- ✅ Chunks contain complementary information\n",
    "- ✅ You can afford the extra API calls\n",
    "\n",
    "**Use `tree_summarize` when:**\n",
    "- ✅ You have many chunks (10+)\n",
    "- ✅ You're doing summarization\n",
    "- ✅ You want balanced treatment of all chunks\n",
    "- ✅ Moderate cost/quality trade-off is acceptable\n",
    "\n",
    "### **Pro Tips:**\n",
    "\n",
    "1. **Start with `compact`** - It's the default for a reason. Only switch if you have a specific need.\n",
    "\n",
    "2. **Monitor token usage** - In production, track how many tokens each mode uses. `refine` can get expensive quickly!\n",
    "\n",
    "3. **Test with your data** - The \"best\" mode depends on your specific documents and queries. Run experiments!\n",
    "\n",
    "4. **Consider hybrid approaches** - You can use `compact` for simple queries and `refine` for complex ones.\n",
    "\n",
    "5. **Remember: Good retrieval matters more** - A perfect synthesis mode can't fix bad retrieval. Focus on metadata, chunking, and filtering first!\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "gt4f704rgn"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bnQZedEJ4TnT"
   },
   "id": "bnQZedEJ4TnT",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}