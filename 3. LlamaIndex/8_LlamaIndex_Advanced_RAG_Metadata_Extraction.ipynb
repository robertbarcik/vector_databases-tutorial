{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Introduction\n",
        "In this exercise, we will learn how to **enrich documents with metadata** before indexing them for RAG. So instead of only storing raw text, we‚Äôll add useful information such as titles, summaries or example questions/answers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# Setup: Installing Required Libraries\n",
        "\n",
        "Step 1: Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook.\n",
        "\n",
        "Step 2: Upload the file called why-language-models-hallucinate.pdf to Files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "# Install required libraries with working versions\n",
        "# If you see dependency conflict warnings during installation, you can ignore them - they won't affect this notebook.\n",
        "# Always restart your runtime after installation! (Runtime ‚Üí Restart runtime)\n",
        "!pip install -q llama-index-core==0.14.6 llama-index-embeddings-openai==0.5.1 \\\n",
        "    llama-index-llms-openai==0.6.6 openai==1.109.1 \\\n",
        "    chromadb==1.2.2 llama-index-vector-stores-chroma==0.5.3 \\\n",
        "    llama-index-readers-file llama-parse\n",
        "\n",
        "print(\"‚úÖ All libraries installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "source": [
        "# 1. Loading the data\n",
        "\n",
        "We are going to work with the PDF file \"why-language-models-hallucinate.pdf\" (a recent OpenAI research piece that explores the statistical reasons behind model hallucinations) and load it using `SimpleDirectoryReader`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Configure OpenAI API key\n",
        "OPENAI_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print('‚úÖ API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY')\n",
        "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError('‚ùå ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
        "    raise ValueError('‚ùå ERROR: No API key provided!')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print('‚úÖ Authentication configured!')\n",
        "\n",
        "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
        "print(f'ü§ñ Selected Model: {OPENAI_MODEL}')\n",
        "\n",
        "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
        "print(f'üß† Embedding Model: {OPENAI_EMBED_MODEL}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = await SimpleDirectoryReader(input_files=[\"why-language-models-hallucinate.pdf\"]).aload_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "The file has 36 pages, so the data connector created 36 document objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "print(\"Number of document objects:\", len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "# 2. Controlling Metadata Visibility\n",
        "\n",
        "Before we enrich our documents, we first need to understand what information is already stored in them.\n",
        "\n",
        "\n",
        "\n",
        "We already know that each document carries not just the text but also the metadata. Additionally, there are 2 parameters that control what parts of that metadata will be visible to the embedding model and to the LLM at query time:\n",
        "\n",
        "`excluded_embed_metadata_keys`:\n",
        "- tells LlamaIndex which metadata **should not be included when creating embeddings**\n",
        "- embeddings are designed to capture the semantic meaning of content, and technical details such as file size or last modified date do not add any useful meaning.\n",
        "\n",
        "`excluded_llm_metadata_keys`:\n",
        "- tells LlamaIndex which metadata **should not be sent to the LLM when the document is retrieved at query time**\n",
        "- the reason for controlling this is that some metadata, like we can add the author, can provide valuable context to the LLM when generating an answer, while other metadata would only distract the model and reduce the clarity of its response\n",
        "\n",
        "These two parameters give us **control over what information flows into embeddings and into the LLM**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "documents[0].__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "## 2.1 Manually Constructed Documents\n",
        "\n",
        "Most of the time we let LlamaIndex create documents automatically when we load files. But sometimes we want to **build a Document manually**. This is useful when:\n",
        "1. Our data doesn‚Äôt come from a file (e.g., from a database or an API)\n",
        "2. We want to attach custom metadata up front\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "The example below (adapted from the documentation) shows how we can construct a custom Document and explicitly control what metadata is included when building embeddings.\n",
        "\n",
        "In this case, we give the document some metadata (\"file_name\", \"category\" and \"author\"). But notice that we tell LlamaIndex to **exclude \"file_name\"** from embeddings by setting `excluded_embed_metadata_keys`. This makes sense, because the actual file name is not semantically meaningful and would only add noise to the embedding space. The category (\"finance\") and author (\"LlamaIndex\"), however, may carry useful meaning for semantic search, so we leave them in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "# What the Embedding model will see\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    excluded_embed_metadata_keys=[\"file_name\"]\n",
        ")\n",
        "\n",
        "print(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "source": [
        "Just like we control which metadata flows into the embedding model, we can also **decide which metadata the LLM will receive when it is asked to answer a query**. This is important because the LLM doesn‚Äôt only use the raw text of a chunk. It can also use metadata as extra context to generate a better answer.\n",
        "\n",
        "In the example below, we create a custom Document with the same metadata fields. This time, however, we tell LlamaIndex to exclude the category from what the LLM sees. That means when the document is retrieved later, the model will still see the file name (so it knows the source) and the author (which may add credibility). In this case, the category is redundant - the text already makes it clear that the topic is finance, so it likely won‚Äôt affect the LLM‚Äôs response and only takes up prompt space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "# What the LLM model will see\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a short snippet of a super-customized document that will go to the embedding model\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    excluded_llm_metadata_keys=[\"category\"],\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "source": [
        "Some metadata is more useful for embeddings, some is more useful for the LLM, and some works for both. **Whether we exclude/add a certain information depends on our use case**: are we trying to keep embeddings clean, or give the LLM more context? In real-world projects, this is a design choice you make depending on how much metadata adds value versus noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "By default, LlamaIndex already takes care of formatting metadata in a clean way, and in practice you usually don‚Äôt need to change it. However, you can **customize the formatting** if you want more readable prompts for the LLM or if you want the metadata formatted in a certain style to match your company‚Äôs pipelines or prompt style.\n",
        "\n",
        "Optional parameters:\n",
        "- `metadata_seperator` - Sets the character(s) between different pieces of metadata. The default is a newline (`\"\\n\"`).\n",
        "- `metadata_template` - Defines how each key-value pair is shown. Both `{key}` and `{value}` must be included.\n",
        "- `text_template` - takes two variables: `metadata_str` and `content`\n",
        "\n",
        "This doesn‚Äôt change what information is sent, only how it is displayed. For example, for the LLM a cleaner format can sometimes help it parse metadata more naturally.\n",
        "\n",
        "In our exercise, we‚Äôll try a custom format just to see how this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Formatting\n",
        "document = Document(\n",
        "    text=\"This is a short snippet of a super-customized document that will go to the model\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    metadata_seperator=\", \",\n",
        "    metadata_template=\"{key}:{value}\",\n",
        "    text_template=\"Metadata:\\n{metadata_str}\\n------\\nContent:\\n{content}\",\n",
        ")\n",
        "\n",
        "print(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "Now let‚Äôs return to the real documents created automatically when we loaded our PDF. Each of these documents already comes with some metadata attached.\n",
        "\n",
        "- \"file_path\" can be useful (it tells us where the chunk came from)\n",
        "- \"page_label\" usually does not add much value for embeddings (handy to keep for LLM if you want the reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "Let's exclude page labels - we can loop through all documents, adjust their formatting template and tell LlamaIndex not to include \"page_label\" in the embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "for doc in documents:\n",
        "    # Defining the content/metadata template\n",
        "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
        "\n",
        "    # Excluding page label from embedding\n",
        "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
        "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "Let's check the transformation - page label should not be included in the metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "outputs": [],
      "source": [
        "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "# 3. Building a RAG Pipeline with Metadata Enrichment\n",
        "\n",
        "Now we‚Äôre getting to the interesting and fun part of the notebook. Up to this point, our documents only carried basic metadata like file names and page labels. That‚Äôs useful for organizing files, but it doesn‚Äôt really help a RAG system retrieve more accurate answers. So this is what we‚Äôre going to do: make our RAG pipeline smarter by **enriching each document chunk with additional context** such as short titles, summaries and example Q&As. To do so, we'll **use a language model  `gpt-5-nano` to generate this metadata**.\n",
        "\n",
        "To really test whether this helps, we‚Äôll build **3 different versions of our nodes** (chunks of text):\n",
        "- Baseline nodes (`nodes_0`) - only the basic metadata\n",
        "- Title-enriched nodes (`nodes_1`) ‚Äì chunks labeled with short, descriptive titles.\n",
        "- Fully enriched nodes (`nodes_2`) ‚Äì chunks augmented with titles, summaries and example Q&A pairs.\n",
        "\n",
        "We‚Äôll follow three main steps in this experiment:\n",
        "1. **Splitting the data**: we'll break the PDF into smaller, manageable chunks\n",
        "2. **Creating three versions of nodes**\n",
        "3. **Building and testing RAG indexes**: we‚Äôll run the same queries against each node set and compare the results to see how much metadata enrichment improves retrieval and answers\n",
        "\n",
        "> NOTE: This setup is inspired by the official LlamaIndex metadata extraction [example](https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MetadataExtraction_LLMSurvey/#automated-metadata-extraction-for-better-retrieval-synthesis)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "source": [
        "**OpenAI's Language model for transformations**\n",
        "\n",
        "We will use OpenAI‚Äôs `gpt-5-nano` model which is fast, affordable, and accurate enough for our metadata extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Language model\n",
        "llm_transformations = OpenAI(\n",
        "    model = OPENAI_MODEL,\n",
        "    temperature = 0.0,\n",
        "    max_tokens = 512\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "source": [
        "## 3.1 Splitting the data\n",
        "\n",
        "First, we need to prepare our documents for transformation by splitting them into smaller chunks. Large documents cannot be processed effectively all at once. We'll use `SentenceSplitter` which splits the content into 1024 tokens and also adds an overlap of 128 tokens. The overlap ensures that if important information appears at the boundary of one chunk, it is also present in the next chunk, so nothing is lost. Parameter `separator` simply tells the splitter to break text along spaces (keeping words intact)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "text_splitter = SentenceSplitter(\n",
        "    separator = \" \",\n",
        "    chunk_size = 1024,\n",
        "    chunk_overlap = 128\n",
        ")\n",
        "text_splitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "## 3.2 Creating three versions of Nodes\n",
        "\n",
        "We‚Äôll now create three parallel versions of our corpus so we can run a fair comparison later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "source": [
        "### 3.2.1 Creating baseline nodes (split only)\n",
        "First, we create the baseline: chunks produced by the splitter with no metadata enrichment. This gives us a control group. Any improvement we see later can be attributed to the extra metadata, not to changes in chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "outputs": [],
      "source": [
        "# Baseline nodes\n",
        "baseline_nodes = text_splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "baseline_nodes[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "source": [
        "### 3.2.2 Enriched nodes (titles extraction)\n",
        "\n",
        "Next, we'll add short, descriptive titles for each chunk of text. These will be labels for each chunk and often help the retriever match user intent to the right passage.\n",
        "\n",
        "To do this, we use `TitleExtractor` which takes an LLM and generates a title for each node. We also set the parameter `nodes = 5` so that up to 5 chunks are processed in one request, making the process more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.extractors import TitleExtractor\n",
        "\n",
        "title_extractor = TitleExtractor(llm = llm_transformations, nodes = 5)\n",
        "title_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "Next, we run this transformation using `IngestionPipeline`. The pipeline executes a sequence of transformations, in our case, splitting the text into chunks and then adding titles. We set `in_place=False` to make sure we don‚Äôt overwrite our baseline nodes. Instead, we produce a separate list (stored in \"nodes_1\") for A/B test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline_titles = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        title_extractor\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "# Running the pipeline\n",
        "nodes_1 = pipeline_titles.run(\n",
        "    documents = documents,\n",
        "    in_place = False,\n",
        "    show_progress = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "### üìù EXERCISE 1: Explore Metadata Extraction\n",
        "\n",
        "\n",
        "**Your task:**\n",
        "1. Compare a baseline node (without metadata) to an enriched node (with title extraction)\n",
        "2. Display the content of `baseline_nodes[5]` using `.get_content()`\n",
        "3. Display the content of `nodes_1[5]` (with title metadata) using `.get_content(metadata_mode=MetadataMode.LLM)`\n",
        "4. Observe: What additional information does the title provide? How might this help retrieval?\n",
        "\n",
        "\n",
        "**Hint:** Use `MetadataMode.LLM` to see what the language model receives, including metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "### 3.2.3 Fully enriched nodes (titles + Q&A + summary extraction)\n",
        "\n",
        "For the richest version of our nodes, we‚Äôll go beyond titles and also add example Q&A pairs and short summaries.\n",
        "\n",
        "**Q&A pairs simulate how a real user might query the system and what kind of response a chunk could provide**. This makes the retriever‚Äôs job easier because each chunk carries hints about the kinds of questions it can answer. In practice, adding Q&A metadata often improves recall (finding the right chunk) and helps the system produce more useful answers. We‚Äôll use `QuestionsAnsweredExtractor` and set `questions = 3`, which asks the LLM to generate three realistic Q&A pairs per chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.extractors import QuestionsAnsweredExtractor\n",
        "\n",
        "qa_extractor = QuestionsAnsweredExtractor(llm = llm_transformations, questions = 3)\n",
        "qa_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "**Summaries capture the core ideas of each chunk in a compact form**. They provide another layer of metadata that‚Äôs especially helpful when users ask broader or high-level questions. We‚Äôll use `SummaryExtractor` for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.extractors import SummaryExtractor\n",
        "\n",
        "summary_extractor = SummaryExtractor(llm = llm_transformations)\n",
        "summary_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "source": [
        "Both transformations run inside the same pipeline, along with the SentenceSplitter and TitleExtractor, so each chunk ends up with a title, a short summary and 3 example Q&A pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "outputs": [],
      "source": [
        "# titles + Q&A + summary\n",
        "pipeline_rich = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        title_extractor,\n",
        "        qa_extractor,\n",
        "        summary_extractor\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": [],
      "source": [
        "# Running the pipeline\n",
        "nodes_2 = pipeline_rich.run(\n",
        "    documents = documents,\n",
        "    in_place = False,\n",
        "    show_progress = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "outputs": [],
      "source": [
        "print(nodes_2[0].get_content(metadata_mode=MetadataMode.LLM))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "source": [
        "**Splicing Baseline and Enriched Nodes**\n",
        "\n",
        "To fairly test the effect of metadata enrichment, we don‚Äôt want to rebuild our dataset in three completely separate ways. That would make it difficult to know if differences in answers are due to enrichment or simply because the data was reprocessed differently. Instead, we keep most of the dataset identical and **replace only a small slice of nodes with enriched versions**.\n",
        "\n",
        "This creates a controlled experiment:\n",
        "- All three indexes contain the same core content.\n",
        "- The only difference is that in \"index1\" and \"index2\", a chosen section of the document is enriched with new metadata (titles, or titles + Q&A + summaries).\n",
        "- If the enriched versions produce better answers, we can be confident the improvement comes from the metadata itself, not from unrelated differences.\n",
        "\n",
        "  \n",
        "First let's check the number of nodes in the baseline split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "outputs": [],
      "source": [
        "print(len(baseline_nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "source": [
        "When deciding which nodes to replace, we need to balance two things:\n",
        "1. Keep enough baseline nodes so the indexes are mostly identical.\n",
        "2. Pick a meaningful section of the paper (not just references, etc.).\n",
        "   \n",
        "In our case, the baseline split produced **39 nodes**. A good rule of thumb is to replace about 20‚Äì25% of the nodes. That‚Äôs large enough to see an effect, but small enough that the rest of the dataset remains constant. We chose the range 15‚Äì25, which corresponds to the middle of the paper.\n",
        "\n",
        "We will create the helper function that replaces the baseline slice [15:25] with enriched nodes from \"nodes_1\" or \"nodes_2\". The rest of the baseline stays intact:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "outputs": [],
      "source": [
        "def splice(orig, replacement, start=15, end=25):\n",
        "    # keep same length, swap slice [start:end] with enriched nodes\n",
        "    return orig[:start] + replacement[start:end] + orig[end:]\n",
        "\n",
        "# mostly baseline nodes, with titles added in positions 15‚Äì25\n",
        "nodes_for_index_1 = splice(baseline_nodes, nodes_1, 15, 25)\n",
        "\n",
        "# mostly baseline nodes, with titles added in positions 15‚Äì25\n",
        "nodes_for_index_2 = splice(baseline_nodes, nodes_2, 15, 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "source": [
        "**Creating Embeddings**\n",
        "\n",
        "Now we‚Äôll embed and index each one with the same embedding model `\"text-embedding-3-small\"`. Creating a `VectorStoreIndex` from nodes automatically computes embeddings for those nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=OPENAI_EMBED_MODEL)\n",
        "\n",
        "# baseline only\n",
        "index_0 = VectorStoreIndex(baseline_nodes, embed_model=embed_model, show_progress=True)\n",
        "\n",
        "# baseline with the slice replaced by titles\n",
        "index_1 = VectorStoreIndex(nodes_for_index_1, embed_model=embed_model, show_progress=True)\n",
        "\n",
        "# baseline with the slice replaced by titles + Q&A + summary\n",
        "index_2 = VectorStoreIndex(nodes_for_index_2, embed_model=embed_model, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "**Querying**\n",
        "\n",
        "Next, we'll create three query engines with identical parameter `similarity_top_k=1`, so each returns the single most relevant node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "outputs": [],
      "source": [
        "query = \"What metrics are commonly used to evaluate text generation quality, and what are their limitations according to the paper?\"\n",
        "\n",
        "query_engine_0 = index_0.as_query_engine(similarity_top_k=1)\n",
        "query_engine_1 = index_1.as_query_engine(similarity_top_k=1)\n",
        "query_engine_2 = index_2.as_query_engine(similarity_top_k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "source": [
        "Each index is queried with the exact same question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57",
      "metadata": {
        "id": "57"
      },
      "outputs": [],
      "source": [
        "response_0 = query_engine_0.query(query)\n",
        "response_1 = query_engine_1.query(query)\n",
        "response_2 = query_engine_2.query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58",
      "metadata": {
        "id": "58"
      },
      "source": [
        "**Important Note: Results May Vary**\n",
        "\n",
        "  The effectiveness of metadata enrichment depends on several factors:\n",
        "  - **Which nodes were enriched**: We only enriched nodes 15-25 (about\n",
        "  25% of the document)\n",
        "  - **Semantic similarity**: How well the query embedding matches chunk\n",
        "   embeddings\n",
        "  - **Data quality**: Whether reference pages and irrelevant sections\n",
        "  were included in the index\n",
        "\n",
        "  In this particular run, all three versions (baseline, titles, and\n",
        "  titles + Q&A + summary) retrieved content from **page 20, which\n",
        "  contains only references**. This demonstrates several important\n",
        "  lessons about RAG systems:\n",
        "\n",
        "  1. **RAG is non-deterministic**: Results can vary between runs due to\n",
        "   embedding variability and chunking differences\n",
        "  2. **Metadata enrichment isn't a silver bullet**: While it helps\n",
        "  improve retrieval, it doesn't guarantee perfect results every time\n",
        "  3. **Preprocessing matters critically**: We should have excluded\n",
        "  reference pages, bibliographies, and appendices before indexing to\n",
        "  prevent retrieving non-substantive content\n",
        "  4. **Retrieval can fail**: Even with enrichment, the retriever can\n",
        "  still grab the wrong chunks, especially when queries have high\n",
        "  lexical overlap with irrelevant sections\n",
        "  5. **LLMs hallucinate from poor context**: Notice how the model\n",
        "  generates plausible-sounding answers (mentioning BLEU, ROUGE,\n",
        "  perplexity) even though page 20 only contains citations, not actual\n",
        "  discussion of these metrics. The model is drawing from its training\n",
        "  knowledge rather than grounding its answer in the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59",
      "metadata": {
        "id": "59"
      },
      "outputs": [],
      "source": [
        "print(\"\\n[BASELINE]\\n\", response_0.response)\n",
        "print(\"\\n[TITLES]\\n\", response_1.response)\n",
        "print(\"\\n[TITLES + Q&A + SUMMARY]\\n\", response_2.response)\n",
        "\n",
        "def show_sources(resp, k=1):\n",
        "    for i, sn in enumerate(resp.source_nodes[:k], 1):\n",
        "        md = sn.node.metadata or {}\n",
        "        print(f\"\\nSource {i} | page={md.get('page_label')} | title={md.get('document_title')}\")\n",
        "        print(sn.node.get_content(metadata_mode=MetadataMode.NONE)[:400], \"\\n---------------\")\n",
        "\n",
        "print(\"\\n SOURCES: BASELINE\")\n",
        "show_sources(response_0)\n",
        "\n",
        "print(\"\\n SOURCES: TITLES\")\n",
        "show_sources(response_1)\n",
        "\n",
        "print(\"\\n SOURCES: TITLES + Q&A + SUMMARY\")\n",
        "show_sources(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61",
      "metadata": {
        "id": "61"
      },
      "source": [
        "# 4. Persistent Storage\n",
        "\n",
        "Once we‚Äôve decided which pages to keep and which metadata to enrich (e.g., titles only, pages with references removed), we can persist that final node set, for example, in ChromaDB. We will take the enriched nodes stored in \"nodes_1\", embed them with the same embedding model (`text-embedding-3-small`), and write those vectors into a Chroma collection we can reopen in future notebook's sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64",
      "metadata": {
        "id": "64"
      },
      "outputs": [],
      "source": [
        "from chromadb import PersistentClient\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "embed_model = OpenAIEmbedding(model = OPENAI_EMBED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65",
      "metadata": {
        "id": "65"
      },
      "source": [
        "In this code below, we connect our pipeline to ChromaDB. We start by opening (or creating) a Chroma database on disk, then define a collection called \"LLM_titles_only_v1\" where our vectors will be stored. We'll build a `VectorStoreIndex` from our enriched nodes using the embedding model and route them into Chroma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66",
      "metadata": {
        "id": "66"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext\n",
        "\n",
        "CHROMA_PATH = \"./chroma_database\"\n",
        "client = PersistentClient(path=CHROMA_PATH)\n",
        "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
        "\n",
        "# Routing vectors into Chroma via StorageContext\n",
        "vector_store = ChromaVectorStore(chroma_collection = collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
        "\n",
        "index = VectorStoreIndex(\n",
        "    nodes_1,\n",
        "    storage_context = storage_context,\n",
        "    embed_model=embed_model,\n",
        "    show_progress = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67",
      "metadata": {
        "id": "67"
      },
      "source": [
        "When we come back in a new session, we just need to wrap the existing Chroma collection and set the index as query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68",
      "metadata": {
        "id": "68"
      },
      "outputs": [],
      "source": [
        "client = PersistentClient(path=CHROMA_PATH)\n",
        "collection = client.get_or_create_collection(\"LLM_titles_only_v1\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store,\n",
        "    embed_model=embed_model,  # query-time embeddings must match!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69",
      "metadata": {
        "id": "69"
      },
      "outputs": [],
      "source": [
        "qe = index.as_query_engine(similarity_top_k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70",
      "metadata": {
        "id": "70"
      },
      "outputs": [],
      "source": [
        "query = \"What are the conclusions about hallucinations of language models?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71",
      "metadata": {
        "id": "71"
      },
      "outputs": [],
      "source": [
        "response = qe.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72",
      "metadata": {
        "id": "72"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7xu0prno0yq",
      "source": [
        "# 5. Using Metadata Filters in Queries\n",
        "\n",
        "In the previous sections, we enriched our documents with metadata like titles, summaries, and Q&A pairs. But we haven't yet shown you how to actually **USE** that metadata to filter your queries. This is one of the most powerful features of metadata enrichment.\n",
        "\n",
        "## Why Filter with Metadata?\n",
        "\n",
        "Imagine you have thousands of documents from different sources, topics, or time periods. Sometimes you don't want to search through ALL of them‚Äîyou want to search only within:\n",
        "- A specific document or set of documents\n",
        "- A particular category or topic\n",
        "- Content from a certain time period\n",
        "- Documents by a specific author\n",
        "\n",
        "**Metadata filtering lets you do exactly this.** It combines semantic search (finding similar content) with structured filtering (like SQL WHERE clauses).\n",
        "\n",
        "## How It Works\n",
        "\n",
        "LlamaIndex allows you to add filters to your queries using the `MetadataFilters` class. You can filter by:\n",
        "- **Exact match**: `key == value`\n",
        "- **In list**: `key IN [value1, value2, ...]`\n",
        "- **Greater than / Less than**: `key > value`, `key < value`\n",
        "- **Not equal**: `key != value`\n",
        "\n",
        "Let's see this in action with our enriched nodes."
      ],
      "metadata": {
        "id": "7xu0prno0yq"
      }
    },
    {
      "cell_type": "markdown",
      "id": "0oicld0izs6",
      "source": [
        "## 5.1 Example: Filter by File Name (Multi-Document Collections)\n",
        "\n",
        "In real-world applications, you often have multiple documents indexed together. Metadata filtering becomes even more powerful here‚Äîyou can search across all documents OR narrow down to specific ones.\n",
        "\n",
        "Let's demonstrate this concept. Even though we only have one PDF in this notebook, imagine you had indexed multiple research papers. You could filter by `file_name` to search within just one paper."
      ],
      "metadata": {
        "id": "0oicld0izs6"
      }
    },
    {
      "cell_type": "code",
      "id": "686v1sexmc8",
      "source": [
        "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "# Filter to search only within a specific file\n",
        "file_filter = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(\n",
        "            key=\"file_name\",\n",
        "            value=\"why-language-models-hallucinate.pdf\",\n",
        "            operator=FilterOperator.EQ  # Exact match\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create query engine with file filter\n",
        "file_filtered_qe = index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    filters=file_filter\n",
        ")\n",
        "\n",
        "response = file_filtered_qe.query(\"What are the main conclusions about hallucinations?\")\n",
        "\n",
        "print(\"ANSWER:\")\n",
        "print(response.response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SOURCES (all from the filtered file):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, node in enumerate(response.source_nodes, 1):\n",
        "    print(f\"\\nSource {i}: {node.metadata.get('file_name', 'N/A')} (Page {node.metadata.get('page_label', 'N/A')})\")"
      ],
      "metadata": {
        "id": "686v1sexmc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "n8qipecq7f",
      "source": [
        "**Real-world use case:**\n",
        "\n",
        "Imagine you're building a research assistant that has indexed 100 academic papers. A user asks:\n",
        "> \"What does the 2024 OpenAI paper say about hallucinations?\"\n",
        "\n",
        "Without filtering, the system might retrieve chunks from ANY of the 100 papers. With metadata filtering:\n",
        "```python\n",
        "filters = MetadataFilters(\n",
        "    filters=[\n",
        "        MetadataFilter(key=\"author\", value=\"OpenAI\", operator=FilterOperator.EQ),\n",
        "        MetadataFilter(key=\"year\", value=\"2024\", operator=FilterOperator.EQ)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "Now the search is constrained to ONLY the relevant paper, dramatically improving answer quality."
      ],
      "metadata": {
        "id": "n8qipecq7f"
      }
    },
    {
      "cell_type": "markdown",
      "id": "c141cl5hpo4",
      "source": [
        "# 6. Response Synthesis Modes\n",
        "\n",
        "So far, we've focused heavily on the **retrieval** side of RAG‚Äîhow to find the right chunks using semantic search, metadata enrichment, and filtering. But there's another critical component: **how the LLM synthesizes those retrieved chunks into a final answer**.\n",
        "\n",
        "This is called **response synthesis**, and LlamaIndex offers several different modes for doing this. Each mode has different trade-offs in terms of answer quality, context handling, and API cost.\n",
        "\n",
        "## The Problem: What Happens with Multiple Retrieved Chunks?\n",
        "\n",
        "When you set `similarity_top_k=5`, the retriever returns 5 chunks. But how does the LLM use them?\n",
        "\n",
        "**Three challenges:**\n",
        "1. **Context length**: If chunks are long, they might exceed the LLM's context window\n",
        "2. **Information synthesis**: Should chunks be combined, compared, or processed one-by-one?\n",
        "3. **Relevance**: Not all retrieved chunks are equally useful‚Äîsome might be noise\n",
        "\n",
        "Different response modes solve these challenges in different ways."
      ],
      "metadata": {
        "id": "c141cl5hpo4"
      }
    },
    {
      "cell_type": "markdown",
      "id": "zw8qqygzmer",
      "source": [
        "## 6.1 Response Mode: `compact` (Default)\n",
        "\n",
        "This is the default mode in LlamaIndex.\n",
        "\n",
        "**How it works:**\n",
        "1. Retrieves multiple chunks (e.g., top 5)\n",
        "2. **Concatenates them into a single context string**\n",
        "3. Sends the concatenated context + query to the LLM in ONE request\n",
        "4. LLM generates answer based on all chunks at once\n",
        "\n",
        "**Characteristics:**\n",
        "- ‚úÖ **Fast**: Only one LLM call\n",
        "- ‚úÖ **Cheap**: Minimal API cost\n",
        "- ‚úÖ **Good for short chunks**: Works well when all chunks fit in context\n",
        "- ‚ùå **Context limit risk**: If chunks are too large, might exceed LLM's context window\n",
        "- ‚ùå **No refinement**: LLM sees everything at once, can't iteratively improve\n",
        "\n",
        "**When to use:**\n",
        "- Default choice for most queries\n",
        "- Short to medium-length chunks\n",
        "- Fast prototyping"
      ],
      "metadata": {
        "id": "zw8qqygzmer"
      }
    },
    {
      "cell_type": "code",
      "id": "ngvv3vtzku",
      "source": [
        "# Example: Compact mode (default)\n",
        "query_engine_compact = index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    response_mode=\"compact\"  # This is actually the default\n",
        ")\n",
        "\n",
        "query = \"What are the main causes of hallucinations in language models?\"\n",
        "response_compact = query_engine_compact.query(query)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESPONSE MODE: COMPACT\")\n",
        "print(\"=\"*80)\n",
        "print(response_compact.response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Number of chunks used: {len(response_compact.source_nodes)}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "ngvv3vtzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bznb0ec0jk6",
      "source": [
        "## 6.2 Response Mode: `refine`\n",
        "\n",
        "This mode uses an **iterative refinement** approach.\n",
        "\n",
        "**How it works:**\n",
        "1. Retrieves multiple chunks (e.g., top 5)\n",
        "2. Sends **chunk 1** to LLM ‚Üí Get initial answer\n",
        "3. Sends **chunk 2 + previous answer** to LLM ‚Üí \"Refine the answer based on new context\"\n",
        "4. Sends **chunk 3 + refined answer** to LLM ‚Üí \"Refine again\"\n",
        "5. Continues until all chunks are processed\n",
        "6. Returns the final refined answer\n",
        "\n",
        "**Characteristics:**\n",
        "- ‚úÖ **Better quality**: Iteratively improves the answer with each chunk\n",
        "- ‚úÖ **Handles long contexts**: Processes chunks one at a time, avoids context limits\n",
        "- ‚úÖ **More comprehensive**: Can incorporate information from many chunks sequentially\n",
        "- ‚ùå **Slower**: Makes N LLM calls (where N = number of chunks)\n",
        "- ‚ùå **More expensive**: Each refinement costs API tokens\n",
        "- ‚ùå **Later chunks matter more**: Information from later chunks might overshadow earlier ones\n",
        "\n",
        "**When to use:**\n",
        "- Complex questions requiring information from multiple sources\n",
        "- Long documents where chunks are large\n",
        "- When answer quality is more important than speed/cost"
      ],
      "metadata": {
        "id": "bznb0ec0jk6"
      }
    },
    {
      "cell_type": "code",
      "id": "wanvmoob16g",
      "source": [
        "# Example: Refine mode\n",
        "query_engine_refine = index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    response_mode=\"refine\"\n",
        ")\n",
        "\n",
        "response_refine = query_engine_refine.query(query)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESPONSE MODE: REFINE\")\n",
        "print(\"=\"*80)\n",
        "print(response_refine.response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Number of chunks used: {len(response_refine.source_nodes)}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nüí° Note: This made 3 LLM calls (one per chunk) to iteratively refine the answer\")"
      ],
      "metadata": {
        "id": "wanvmoob16g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "li9dfdg7qa",
      "source": [
        "## 6.3 Response Mode: `tree_summarize`\n",
        "\n",
        "This mode uses a **hierarchical summarization** approach.\n",
        "\n",
        "**How it works:**\n",
        "1. Retrieves multiple chunks (e.g., top 8)\n",
        "2. Groups them into pairs or small batches\n",
        "3. Summarizes each batch ‚Üí Creates intermediate summaries\n",
        "4. Groups those summaries and summarizes again\n",
        "5. Repeats until one final summary remains\n",
        "\n",
        "\n",
        "```\n",
        "Chunk1, Chunk2 ‚Üí Summary A\n",
        "Chunk3, Chunk4 ‚Üí Summary B\n",
        "Chunk5, Chunk6 ‚Üí Summary C\n",
        "Chunk7, Chunk8 ‚Üí Summary D\n",
        "\n",
        "Summary A, B ‚Üí Summary AB\n",
        "Summary C, D ‚Üí Summary CD\n",
        "\n",
        "Summary AB, CD ‚Üí Final Answer\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- ‚úÖ **Handles many chunks**: Can process dozens of chunks efficiently\n",
        "- ‚úÖ **Balanced processing**: All chunks contribute equally (no recency bias)\n",
        "- ‚úÖ **Good for summarization**: Excellent for \"summarize this document\" type queries\n",
        "- ‚ùå **Multiple LLM calls**: log(N) calls where N = number of chunks\n",
        "- ‚ùå **Loss of detail**: Hierarchical summarization can lose fine-grained details\n",
        "- ‚ùå **Slower**: More calls than compact, fewer than refine\n",
        "\n",
        "**When to use:**\n",
        "- Large number of retrieved chunks (10+)\n",
        "- Summarization tasks\n",
        "- When you want balanced consideration of all chunks"
      ],
      "metadata": {
        "id": "li9dfdg7qa"
      }
    },
    {
      "cell_type": "code",
      "id": "xgrzlk20wss",
      "source": [
        "# Example: Tree Summarize mode\n",
        "query_engine_tree = index.as_query_engine(\n",
        "    similarity_top_k=4,  # Use 4 chunks to show tree structure\n",
        "    response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "response_tree = query_engine_tree.query(query)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESPONSE MODE: TREE_SUMMARIZE\")\n",
        "print(\"=\"*80)\n",
        "print(response_tree.response)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Number of chunks used: {len(response_tree.source_nodes)}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nüí° Note: This used hierarchical summarization (pairs of chunks ‚Üí intermediate summaries ‚Üí final answer)\")"
      ],
      "metadata": {
        "id": "xgrzlk20wss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "h40x3x3qk96",
      "source": [
        "## 6.4 Comparing Response Modes Side-by-Side\n",
        "\n",
        "Let's compare all three modes on the same query to see the differences:"
      ],
      "metadata": {
        "id": "h40x3x3qk96"
      }
    },
    {
      "cell_type": "code",
      "id": "zvkjc4v26o",
      "source": [
        "# Compare all three modes\n",
        "comparison_query = \"Summarize the paper's main findings about why language models hallucinate\"\n",
        "\n",
        "modes = [\"compact\", \"refine\", \"tree_summarize\"]\n",
        "responses = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"QUERY: {comparison_query}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for mode in modes:\n",
        "    qe = index.as_query_engine(similarity_top_k=3, response_mode=mode)\n",
        "    response = qe.query(comparison_query)\n",
        "    responses[mode] = response\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODE: {mode.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(response.response)\n",
        "    print(f\"\\nChunks used: {len(response.source_nodes)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zvkjc4v26o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gt4f704rgn",
      "source": [
        "## 6.5 Key Takeaways: Response Synthesis Modes\n",
        "\n",
        "### **Quick Reference Table:**\n",
        "\n",
        "| Mode | LLM Calls | Speed | Cost | Quality | Best For |\n",
        "|------|-----------|-------|------|---------|----------|\n",
        "| **compact** | 1 | Fast | Cheap | ‚≠ê‚≠ê Good | Default choice, short chunks |\n",
        "| **refine** | N (per chunk) | Slow | Expensive | ‚≠ê‚≠ê‚≠ê Best | Complex queries, detail-oriented |\n",
        "| **tree_summarize** | log(N) | Medium |  Medium | ‚≠ê‚≠ê Good | Many chunks, summarization |\n",
        "\n",
        "### **Decision Guide:**\n",
        "\n",
        "**Use `compact` when:**\n",
        "- ‚úÖ You have 2-5 short chunks\n",
        "- ‚úÖ Fast response time is important\n",
        "- ‚úÖ You're prototyping or testing\n",
        "- ‚úÖ Cost is a concern\n",
        "\n",
        "**Use `refine` when:**\n",
        "- ‚úÖ Answer quality is paramount\n",
        "- ‚úÖ You need comprehensive answers from multiple sources\n",
        "- ‚úÖ Chunks contain complementary information\n",
        "- ‚úÖ You can afford the extra API calls\n",
        "\n",
        "**Use `tree_summarize` when:**\n",
        "- ‚úÖ You have many chunks (10+)\n",
        "- ‚úÖ You're doing summarization\n",
        "- ‚úÖ You want balanced treatment of all chunks\n",
        "- ‚úÖ Moderate cost/quality trade-off is acceptable\n",
        "\n",
        "### **Pro Tips:**\n",
        "\n",
        "1. **Start with `compact`** - It's the default for a reason. Only switch if you have a specific need.\n",
        "\n",
        "2. **Monitor token usage** - In production, track how many tokens each mode uses. `refine` can get expensive quickly!\n",
        "\n",
        "3. **Test with your data** - The \"best\" mode depends on your specific documents and queries. Run experiments!\n",
        "\n",
        "4. **Consider hybrid approaches** - You can use `compact` for simple queries and `refine` for complex ones.\n",
        "\n",
        "5. **Remember: Good retrieval matters more** - A perfect synthesis mode can't fix bad retrieval. Focus on metadata, chunking, and filtering first!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gt4f704rgn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bnQZedEJ4TnT"
      },
      "id": "bnQZedEJ4TnT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}