{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q chromadb==0.4.22 openai==1.12.0 sentence-transformers==2.3.1\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBzBdNZL3ANC"
   },
   "source": [
    "# ChromaDB\n",
    "\n",
    "In this notebook, we'll learn how to use vector database called ChromaDB. It's open-source and ideal for quick prototyping and developing AI applications. As you will see, it seamlessly integrates with embedding models that convert your data into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XvgVfqfxbYB"
   },
   "source": [
    "# 1. Creating Embeddings Locally\n",
    "\n",
    "We‚Äôll start by generating embeddings locally. This means the text will be converted into numerical vectors right on your computer. Let's import `embedding_functions` module which provides various embedding functions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d9paMe4vr1t"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdHUWxhj1mod"
   },
   "source": [
    "We'll use the default embedding function with the model called `all-MiniLM-L6-v2` which outputs a **384-dimensional vector** for each piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OKAlOYevwHD"
   },
   "outputs": [],
   "source": [
    "# Instantiating embedding function\n",
    "default_emb_function = embedding_functions.DefaultEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBICbVMB2qr6"
   },
   "source": [
    "Let's create an embedding from the name \"Robert\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw_-xr_fvxZ3"
   },
   "outputs": [],
   "source": [
    "name = \"Robert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tUwkTJM24HI"
   },
   "source": [
    "The result will be a list containing a single 384-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBAUMdwbv3Kw",
    "outputId": "46b3231f-69ba-47dd-af6c-c1660eecec12"
   },
   "outputs": [],
   "source": [
    "# Creating an embedding\n",
    "local_emb = default_emb_function([name])\n",
    "\n",
    "print(\"Number of input texts processed:\", len(local_emb))\n",
    "print(\"Size of embedding vector:\", len(local_emb[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Vector:\")\n",
    "local_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 1: Create Your Own Embeddings (5 minutes)\n",
    "\n",
    "**What you'll practice:** Creating embeddings from custom text using the default embedding function.\n",
    "\n",
    "**Your task:**\n",
    "1. Create a list with 3-5 words or short phrases related to a topic you're interested in (e.g., programming languages, foods, cities, hobbies)\n",
    "2. Generate embeddings for your list using `default_emb_function()`\n",
    "3. Print the number of items processed and the size of one embedding vector\n",
    "\n",
    "**Hint:** Follow the same pattern as the \"Robert\" example above. Remember that `default_emb_function()` expects a list as input.\n",
    "\n",
    "**Expected outcome:** You should see that each item produces a 384-dimensional vector, just like the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# my_items = [\"item1\", \"item2\", \"item3\"]\n",
    "# my_embeddings = default_emb_function(my_items)\n",
    "# print(\"Number of items:\", len(my_embeddings))\n",
    "# print(\"Embedding size:\", len(my_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 1: Create Your Own Embeddings (5 minutes)\n",
    "\n",
    "**What you'll practice:** Creating embeddings from custom text using the default embedding function.\n",
    "\n",
    "**Your task:**\n",
    "1. Create a list with 3-5 words or short phrases related to a topic you're interested in (e.g., programming languages, foods, cities, hobbies)\n",
    "2. Generate embeddings for your list using `default_emb_function()`\n",
    "3. Print the number of items processed and the size of one embedding vector\n",
    "\n",
    "**Hint:** Follow the same pattern as the \"Robert\" example above. Remember that `default_emb_function()` expects a list as input.\n",
    "\n",
    "**Expected outcome:** You should see that each item produces a 384-dimensional vector, just like the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4zPKIbZ_f1K"
   },
   "source": [
    "## 1.1 Storing the data\n",
    "This was a really simple example of embedding just one word. But in the real world, we have hundreds or thousands of texts for which we want to create embeddings and then search them later. We need somewhere to keep them. Let's see how ChromaDB handle storing these data.\n",
    "\n",
    "\n",
    "**1. In-memory Chroma Database**:\n",
    "- Chroma keeps all your embeddings only while the program is running. As soon as you stop or restart your notebook, **all data is lost automatically**. This is fine for quick experiments where you don‚Äôt care about keeping the results, but it‚Äôs not practical for building something you‚Äôll revisit later.\n",
    "\n",
    "To create a temporary, in-memory Chroma database instance, we can use **an ephemeral client** right inside our notebook session. It starts a Chroma server entirely in memory and returns a Python client object that we can interact with as if it's a full database:\n",
    "\n",
    "```\n",
    "client = chromadb.EphemeralClient()\n",
    "```\n",
    "\n",
    "**2. Persistent storage**\n",
    "- The second approach is creating **a persistent client that saves everything to files on your disk**. Even if you close your notebook or shut down your computer, you can open the same database later and all your data will still be there.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dor780PJgmCH"
   },
   "source": [
    "Let's create a persistent client using the below code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVmlqgg-CURR"
   },
   "outputs": [],
   "source": [
    "# Creating a persistent storage\n",
    "client = chromadb.PersistentClient(path=\"./db/chroma_persist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4RerfafgpaT"
   },
   "source": [
    "> NOTE:\n",
    "> There is also `HttpClient` (‚Äúconnect-to-a-remote-server‚Äù) option in Chroma:  instead of running the database inside your notebook, you point a client at a Chroma server over HTTP. Use it when you want a shared, centralized vector DB for multiple notebooks/apps or users, when deploying behind Docker/Kubernetes, or when you need language-agnostic access and clearer separation between your app and the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiAWesWADuNB"
   },
   "source": [
    "## 1.2 Creating a Collection\n",
    "\n",
    "Now that we have a persistent place to store our data, the next step is organizing that data inside Chroma.\n",
    "\n",
    "We'll create **a collection that groups together documents (i.e. chunks of text), their embeddings and any associated metadata.** At the moment of creation, the collection is just like an empty database table: it defines the structure and storage space, but no vectors exist yet.\n",
    "\n",
    "To create a collection, we will use `get_or_create_collection()`:\n",
    "- It checks if a collection named, for example, \"my_documents_locally\" already exists in the Chroma database. If it does, it returns that existing collection. If it doesn‚Äôt, it creates a new one.\n",
    "\n",
    "We also have the option to **specify the distance metric that will be used for comparing embeddings**. By default, Chroma use L2 (Euclidean Distance Squared) distance metric. But if we want to work with text embeddings, we can specify **\"cosine\"**  using `meatadata` parameter. `hnsw` refers to **the Hierarchical Navigable Small World** indexing algorithm that Chroma uses for fast approximate nearest-neighbor search.\n",
    "\n",
    "By default, Chroma will assign its default embedding function (`all-MiniLM-L6-v2`) but this can be changed using `embedding_function` parameter. In this example, we'll create a collection with this default.\n",
    "\n",
    "\n",
    "> NOTE: **The embedding function and the distance metric are both set at collection creation and cannot be changed afterwards.** If you need to change them, you‚Äôll have to create a new collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRydQnOgC29E"
   },
   "outputs": [],
   "source": [
    "# Creating a collection\n",
    "collection = client.get_or_create_collection(\"my_documents_locally\",\n",
    "                                             metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYaKuNK7ndUl"
   },
   "source": [
    "You can list all created collections using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_ZTL8t9nkxp",
    "outputId": "2adf021f-74e2-43c3-986f-e478ebca4af2"
   },
   "outputs": [],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVkRKFCFnoEH"
   },
   "source": [
    "If you want to delete an entire collection, use the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUUrPDxfntuB"
   },
   "outputs": [],
   "source": [
    "# This is irreversible\n",
    "#client.delete_collection(\"my_documents_locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-khAklvHVCU"
   },
   "source": [
    "## 1.3 Adding Documents into a Collection\n",
    "\n",
    "In real projects, we embed PDFs, HTML, or other larger documents, but to clearly see how Chroma works, we‚Äôll start with a few short example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZvIkRrfrlNf"
   },
   "outputs": [],
   "source": [
    "# Documents with metadata that will be embedded and added into Chroma databasedocuments = [    {        \"id\": \"document_1\",        \"text\": \"\"\"Public-key cryptography, also known as asymmetric cryptography, represents a monumental paradigm shift from its predecessor, symmetric cryptography. The fundamental challenge it solves is that of key distribution, which was the Achilles' heel of symmetric systems requiring a shared secret key to be exchanged over a secure channel beforehand. Introduced conceptually by Whitfield Diffie and Martin Hellman in their seminal 1976 paper \"New Directions in Cryptography,\" this system utilizes pairs of mathematically linked keys: a public key and a private key. The public key can be shared openly without compromising security, while the private key must be kept secret by its owner. This ingenious design allows for two primary functions: encryption and digital signatures. For encryption, a sender uses the recipient's public key to encrypt a message, which can then only be decrypted by the recipient using their corresponding private key. This ensures confidentiality even when communicating over a hostile, monitored network like the internet. For digital signatures, a sender uses their own private key to sign a message, and anyone with their public key can verify the signature's authenticity, ensuring both integrity and non-repudiation. The security of these systems relies on the computational difficulty of 'one-way functions' with a 'trapdoor,' where it is easy to compute in one direction but infeasible to reverse without the secret information of the private key. The first practical and widely used implementation was the RSA algorithm, developed in 1977 by Ron Rivest, Adi Shamir, and Leonard Adleman, which bases its security on the difficulty of factoring large prime numbers. Over time, other algorithms like Elliptic Curve Cryptography (ECC) have gained prominence, offering equivalent security with much smaller key sizes, making them ideal for resource-constrained devices like smartphones. The practical application of public-key cryptography is managed through a Public Key Infrastructure (PKI). A PKI is a framework of policies and procedures that uses a trusted third party, known as a Certificate Authority (CA), to issue digital certificates that bind public keys to specific identities. This is the trust model that underpins the entire secure web, enabling protocols like Transport Layer Security (TLS), which secures HTTPS traffic. Its influence extends to securing email with PGP, remote shell access with SSH, and forming the transactional backbone of virtually all cryptocurrencies, including Bitcoin. However, the rise of quantum computing poses a significant future threat to current public-key algorithms, as they can theoretically solve the underlying mathematical problems efficiently. This has spurred the development of a new field called post-quantum cryptography (PQC), which aims to create a new generation of secure algorithms resistant to attacks from both classical and quantum computers.\"\"\",        \"metadata\": {\"category\": \"cryptography\", \"year\": 1976, \"difficulty\": \"advanced\", \"author\": \"Security Team\"}    },    {        \"id\": \"document_2\",        \"text\": \"\"\"Stuxnet stands as a watershed moment in the history of digital warfare, a malicious computer worm that transcended the digital realm to cause tangible, physical destruction. First discovered in 2010 by the Belarusian security firm VirusBlokAda, its origins and development are widely attributed to a top-secret joint American-Israeli intelligence operation, codenamed \"Olympic Games.\" Stuxnet's primary target was the Iranian nuclear program, specifically the Siemens industrial control systems (ICS) that managed the gas centrifuges at the Natanz uranium enrichment facility. What made Stuxnet exceptionally sophisticated was its multi-stage attack methodology and its method of propagation. It was specifically designed to cross the 'air gap,' a security measure where critical networks are physically isolated from the internet. The worm primarily spread through infected USB flash drives, exploiting a Windows Shell vulnerability related to .LNK files. Once inside a network, it would aggressively seek out machines running Siemens Step7 software, the specific platform used to program the programmable logic controllers (PLCs) that automated the centrifuges. The worm's payload was a masterpiece of deception. It would record the normal operating frequencies of the centrifuges and then replay this data back to the human operators, creating the illusion that everything was functioning correctly. Meanwhile, the malware would subtly and intermittently alter the rotational speed of the centrifuges, causing them to spin too fast and then too slow, inducing excessive vibration and stress that led to their eventual mechanical failure and destruction. This cyber-physical attack was unprecedented in its precision and impact. Stuxnet leveraged an astonishing four different zero-day vulnerabilities in Microsoft Windows, a record for a single piece of malware at the time. To further evade detection, it was signed with stolen, legitimate digital certificates from reputable hardware companies, Realtek and JMicron, allowing it to masquerade as trusted software. The discovery of Stuxnet irrevocably changed the landscape of national security, proving that code could be deployed as a precise and effective weapon to sabotage critical infrastructure. It ushered in a new era of state-sponsored cyber-physical attacks and served as a blueprint for future digital weapons, fundamentally altering the calculus of modern conflict.\"\"\",        \"metadata\": {\"category\": \"cyberwarfare\", \"year\": 2010, \"difficulty\": \"intermediate\", \"author\": \"Security Team\"}    },    {        \"id\": \"document_3\",        \"text\": \"\"\"The Zero Trust Architecture (ZTA) is a modern cybersecurity strategy built on a profound philosophical shift: 'never trust, always verify.' This model fundamentally rejects the outdated 'castle-and-moat' concept of security, where a hardened perimeter was thought to be sufficient to protect a trusted internal network. That traditional model is no longer viable in an era of cloud computing, remote workforces, and sophisticated persistent threats that often breach the perimeter. Coined by John Kindervag at Forrester Research in 2010, Zero Trust mandates a granular, identity-centric approach to security. It operates on the core principle of 'assume breach,' meaning that an attacker is already present within the network, and therefore, no user or device can be implicitly trusted based on its physical or network location. Access to resources is granted on a per-session, least-privilege basis, strictly enforced through a dynamic policy engine. This policy engine makes access decisions based on a wide array of real-time signals, not just a static password. These signals form the pillars of Zero Trust: strong identity verification, device health, and network context. Every access request must be accompanied by robust authentication, with Multi-Factor Authentication (MFA) considered the absolute baseline. The security posture of the endpoint device is scrutinized; a device that is unpatched, jailbroken, or shows signs of infection may be denied access. To prevent the lateral movement of attackers within a network, Zero Trust heavily relies on micro-segmentation. This practice involves breaking the network into small, isolated zones, often down to the individual workload level, and enforcing strict access controls between them. The National Institute of Standards and Technology (NIST) has formalized these concepts in its Special Publication 800-207, providing a comprehensive guide for organizations. Implementing Zero Trust is not about deploying a single product but is a strategic journey that involves integrating various technologies and redesigning network and security architecture. The ultimate goal is to create a resilient, adaptable security posture where access is continuously validated, and the potential 'blast radius' of any single breach is minimized. It is a transition from a location-based trust model to an identity- and context-based model fit for the complexities of modern IT environments.\"\"\",        \"metadata\": {\"category\": \"architecture\", \"year\": 2010, \"difficulty\": \"intermediate\", \"author\": \"Security Team\"}    },    {        \"id\": \"document_4\",        \"text\": \"\"\"Phishing is the most pervasive form of social engineering, an attack vector that targets the weakest link in the security chain: human psychology. Its name, a homophone of 'fishing,' originated in the mid-1990s within the hacker community, where attackers used email lures to 'phish' for passwords from unsuspecting users. This attack vector functions by having a threat actor masquerade as a trustworthy entity to deceive a victim. The ultimate objective is typically to steal sensitive data like login credentials, credit card numbers, or personally identifiable information (PII), or to deploy malware like ransomware. The attack begins with a carefully crafted lure, which most often takes the form of an email, SMS text message (known as smishing), or even a voice call (vishing). These messages are designed to exploit powerful human emotions such as fear, urgency, curiosity, or greed. For example, a message might falsely claim a user's bank account has been compromised and demand immediate action, or it might offer an unbelievable prize. The sophistication of phishing attacks varies widely. At the low end are bulk phishing campaigns sent to millions of users with generic greetings and obvious grammatical errors. At the high end is spear phishing, a highly targeted attack that uses personal information gathered about an individual or organization to create an extremely convincing and personalized lure. A sub-variant of this, known as whaling, specifically targets senior executives or other high-value individuals within a company. Another common technique is clone phishing, where an attacker copies a legitimate, previously delivered email and replaces its links or attachments with malicious ones. The attack chain relies on the victim taking a specific action, such as clicking a hyperlink that leads to a fraudulent website designed to harvest credentials or downloading an attachment laden with malware. Defending against phishing requires a multi-layered approach. User education and awareness training are critical to help people recognize the tell-tale signs of a phishing attempt, such as mismatched URLs, suspicious sender addresses, and unexpected requests for sensitive information. On the technical side, organizations deploy email security gateways that use protocols like SPF, DKIM, and DMARC to authenticate senders, as well as advanced threat protection systems that scan links and attachments for malicious content. Despite these defenses, phishing remains the primary initial access vector for a vast majority of all cyberattacks, making it an enduring and critical threat to both individuals and organizations.\"\"\",        \"metadata\": {\"category\": \"social-engineering\", \"year\": 1995, \"difficulty\": \"beginner\", \"author\": \"Security Team\"}    },    {        \"id\": \"document_5\",        \"text\": \"\"\"Multi-Factor Authentication (MFA) is a security mechanism that requires users to provide two or more verification factors to gain access to a resource, such as an application, online account, or VPN. Rather than just asking for a username and password, MFA requires additional credentials, making it harder for attackers to gain unauthorized access. The factors fall into three categories: something you know (like a password or PIN), something you have (like a smartphone or security token), and something you are (like a fingerprint or facial recognition). A common example is when you log into your bank account: you enter your password (something you know), and then the bank sends a code to your phone (something you have). Even if an attacker steals your password, they would still need access to your phone to complete the login. MFA significantly reduces the risk of account takeovers, which are common in phishing attacks where passwords are compromised. According to Microsoft, MFA can block over 99.9% of account compromise attacks. The most common MFA methods include SMS codes, authenticator apps like Google Authenticator or Microsoft Authenticator, hardware tokens like YubiKey, and biometric verification. While SMS-based MFA is better than nothing, it's vulnerable to SIM-swapping attacks where attackers convince mobile carriers to transfer a victim's phone number to a new SIM card. Authenticator apps and hardware tokens are more secure alternatives. Organizations should implement MFA for all critical systems, especially email, VPN access, and administrative accounts.\"\"\",        \"metadata\": {\"category\": \"authentication\", \"year\": 2020, \"difficulty\": \"beginner\", \"author\": \"Security Team\"}    },    {        \"id\": \"document_6\",        \"text\": \"\"\"A Virtual Private Network (VPN) creates a secure, encrypted tunnel between your device and the internet. When you connect to a VPN, all your internet traffic is routed through an encrypted connection to a server operated by the VPN provider. This serves several purposes: it hides your IP address, encrypts your data so that ISPs and other third parties cannot see what you're doing online, and allows you to appear as if you're browsing from a different location. VPNs are essential for remote workers who need to securely access corporate networks from home or public Wi-Fi networks. Without a VPN, data transmitted over public Wi-Fi can be intercepted by attackers using tools like packet sniffers. VPNs use various protocols to establish secure connections, including OpenVPN, WireGuard, and IKEv2/IPSec. Each protocol offers different balances of speed, security, and compatibility. Enterprise VPNs often integrate with corporate authentication systems and enforce security policies, such as requiring devices to have up-to-date antivirus software before allowing connections. While consumer VPNs are marketed for privacy, it's important to choose reputable providers, as a malicious VPN provider could potentially monitor your traffic. For businesses, VPNs are a critical component of remote access security, especially in the context of Zero Trust architectures where every connection is verified regardless of location.\"\"\",        \"metadata\": {\"category\": \"network-security\", \"year\": 2015, \"difficulty\": \"beginner\", \"author\": \"Network Team\"}    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0J8r326rlNf",
    "outputId": "20ec1ce4-96d2-4124-e951-dbff03a9d308"
   },
   "outputs": [],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ww0N4RAJMU7"
   },
   "source": [
    "Now we'll fill the created collection with data using `add()` function which takes a list of unique string IDs and a list of documents. These will be automatically embedded using the default embedding function. Chroma then builds vector indexes inside the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9_4Gt9gHUsE"
   },
   "outputs": [],
   "source": [
    "# Adding documents into the collection\n",
    "ids = [doc[\"id\"] for doc in documents]\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "collection.add(ids = ids, documents = texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Working with Metadata\n",
    "\n",
    "In the previous example, we added just the text. But ChromaDB also supports **metadata** - additional information about each document that can be used for filtering and organizing your data.\n",
    "\n",
    "Metadata is extremely useful for:\n",
    "- **Filtering searches** by category, date, author, difficulty level, etc.\n",
    "- **Organizing documents** by type or topic\n",
    "- **Tracking source information** without embedding it\n",
    "\n",
    "Let's delete our current collection and recreate it with metadata included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the old collection\n",
    "client.delete_collection(\"my_documents_locally\")\n",
    "\n",
    "# Recreate it\n",
    "collection = client.get_or_create_collection(\"my_documents_locally\",\n",
    "                                             metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add documents **with metadata**. Notice how we extract the metadata dictionary from each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding documents WITH metadata\n",
    "ids = [doc[\"id\"] for doc in documents]\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "metadatas = [doc[\"metadata\"] for doc in documents]\n",
    "\n",
    "collection.add(\n",
    "    ids = ids, \n",
    "    documents = texts,\n",
    "    metadatas = metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve a document and see its metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document with metadata\n",
    "result = collection.get(ids=[\"document_2\"], include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "print(\"Document ID:\", result[\"ids\"][0])\n",
    "print(\"\\nMetadata:\")\n",
    "for key, value in result[\"metadatas\"][0].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\nDocument preview:\", result[\"documents\"][0][:150], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Filtering with Where Clauses\n",
    "\n",
    "Now that we have metadata, we can use it to **filter our searches**. This is incredibly powerful - you can retrieve only documents that match specific criteria.\n",
    "\n",
    "Let's say we only want documents from the \"cryptography\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only cryptography documents\n",
    "crypto_docs = collection.get(\n",
    "    where={\"category\": \"cryptography\"},\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(crypto_docs['ids'])} cryptography document(s):\")\n",
    "for doc_id, metadata in zip(crypto_docs[\"ids\"], crypto_docs[\"metadatas\"]):\n",
    "    print(f\"  - {doc_id}: {metadata['category']} ({metadata['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by multiple criteria. Let's find all beginner-level documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get beginner documents\n",
    "beginner_docs = collection.get(\n",
    "    where={\"difficulty\": \"beginner\"},\n",
    "    include=[\"metadatas\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(beginner_docs['ids'])} beginner-level document(s):\")\n",
    "for doc_id, metadata in zip(beginner_docs[\"ids\"], beginner_docs[\"metadatas\"]):\n",
    "    print(f\"  - {doc_id}: {metadata['category']} (difficulty: {metadata['difficulty']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use operators like `$gt` (greater than), `$lt` (less than), `$gte`, `$lte`, `$ne` (not equal), and `$in` (in list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents from year 2010 or later\n",
    "recent_docs = collection.get(\n",
    "    where={\"year\": {\"$gte\": 2010}},\n",
    "    include=[\"metadatas\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(recent_docs['ids'])} document(s) from 2010 or later:\")\n",
    "for doc_id, metadata in zip(recent_docs[\"ids\"], recent_docs[\"metadatas\"]):\n",
    "    print(f\"  - {doc_id}: {metadata['category']} ({metadata['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Updating and Deleting Documents\n",
    "\n",
    "Sometimes you need to modify existing documents or remove them entirely. ChromaDB provides methods for both operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Documents with `upsert()`\n",
    "\n",
    "The `upsert()` function is smart: \n",
    "- If a document with the given ID **already exists**, it **updates** it\n",
    "- If it **doesn't exist**, it **creates** it\n",
    "\n",
    "Let's update document_5's metadata to mark it as reviewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata for document_5\n",
    "collection.upsert(\n",
    "    ids=[\"document_5\"],\n",
    "    metadatas=[{\"category\": \"authentication\", \"year\": 2020, \"difficulty\": \"beginner\", \"author\": \"Security Team\", \"reviewed\": True}]\n",
    ")\n",
    "\n",
    "# Verify the update\n",
    "result = collection.get(ids=[\"document_5\"], include=[\"metadatas\"])\n",
    "print(\"Updated metadata for document_5:\")\n",
    "print(result[\"metadatas\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `upsert()` to add a completely new document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new document using upsert\n",
    "collection.upsert(\n",
    "    ids=[\"document_7\"],\n",
    "    documents=[\"\"\"Firewalls are network security devices that monitor and control incoming and outgoing network traffic based on predetermined security rules. A firewall establishes a barrier between trusted internal networks and untrusted external networks, such as the Internet.\"\"\"],\n",
    "    metadatas=[{\"category\": \"network-security\", \"year\": 2000, \"difficulty\": \"beginner\", \"author\": \"Network Team\"}]\n",
    ")\n",
    "\n",
    "print(\"\\nTotal documents after upsert:\", collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Documents\n",
    "\n",
    "To remove specific documents, use the `delete()` method with document IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete document_7\n",
    "collection.delete(ids=[\"document_7\"])\n",
    "\n",
    "print(\"Documents after deletion:\", collection.count())\n",
    "\n",
    "# Try to get the deleted document\n",
    "try:\n",
    "    result = collection.get(ids=[\"document_7\"])\n",
    "    if not result[\"ids\"]:\n",
    "        print(\"Document document_7 has been successfully deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Document not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete documents based on metadata filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Delete all documents from before year 2000 (we don't have any, but this shows the syntax)\n",
    "# collection.delete(where={\"year\": {\"$lt\": 2000}})\n",
    "\n",
    "print(\"\\nNote: The delete with where clause is commented out to preserve our data.\")\n",
    "print(\"Syntax: collection.delete(where={'year': {'$lt': 2000}})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Collection Introspection\n",
    "\n",
    "ChromaDB provides several methods to inspect what's in your collection without retrieving all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at the first few items\n",
    "preview = collection.peek(limit=2)\n",
    "\n",
    "print(\"Peeking at first 2 documents:\")\n",
    "for doc_id, metadata in zip(preview[\"ids\"], preview[\"metadatas\"]):\n",
    "    print(f\"\\n  ID: {doc_id}\")\n",
    "    print(f\"  Category: {metadata['category']}\")\n",
    "    print(f\"  Year: {metadata['year']}\")\n",
    "    print(f\"  Difficulty: {metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total count\n",
    "total = collection.count()\n",
    "print(f\"\\nTotal documents in collection: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get all documents (be careful with large collections!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents (IDs and metadata only)\n",
    "all_docs = collection.get(include=[\"metadatas\"])\n",
    "\n",
    "print(f\"\\nAll document IDs: {all_docs['ids']}\")\n",
    "print(f\"\\nCategories present:\")\n",
    "categories = set(meta['category'] for meta in all_docs['metadatas'])\n",
    "for cat in sorted(categories):\n",
    "    print(f\"  - {cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65LSkJHDZr48"
   },
   "source": [
    "> NOTE: Chroma also provides `upsert()` function which works like `add()`, but with one difference: If an ID already exists in the collection, the record is updated (text, metadata, embedding). If it doesn‚Äôt exist, the record is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF-tOwnanBE8"
   },
   "source": [
    "You can inspect the created collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jE0qFHAYrlNg",
    "outputId": "4feefec5-01df-4951-ae2b-3184cda39cb4"
   },
   "outputs": [],
   "source": [
    "# Number of items\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUUuQuJmdxiw"
   },
   "source": [
    "Let's return both the original text and the generated embedding for ID \"document_3\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 2: Explore Your Collection (5-7 minutes)\n",
    "\n",
    "**What you'll practice:** Retrieving and comparing documents and embeddings from a ChromaDB collection.\n",
    "\n",
    "**Your task:**\n",
    "1. Retrieve the document with ID `\"document_2\"` (the Stuxnet article) from the collection\n",
    "2. Print the first 200 characters of the document text to see what it's about\n",
    "3. Print the embedding vector length\n",
    "4. Compare: Is the embedding size the same as for `document_3`? Why or why not?\n",
    "\n",
    "**Hint:** Use the same `collection.get()` method as shown above, but change the ID.\n",
    "\n",
    "**Expected outcome:** You should see the Stuxnet article text and confirm that all documents in the same collection have the same embedding size (384 dimensions for the default model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# row = collection.get(ids=[\"document_2\"], include=[\"documents\", \"embeddings\"])\n",
    "# print(\"First 200 characters:\", row[\"documents\"][0][:200])\n",
    "# print(\"Embedding length:\", len(row[\"embeddings\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJBVwyc_d_Ws",
    "outputId": "25f241e9-e4bc-4771-e7ce-7fc6348bae5f"
   },
   "outputs": [],
   "source": [
    "# Retrieving embedding\n",
    "row = collection.get(ids = [\"document_3\"], include = [\"documents\", \"embeddings\"])\n",
    "\n",
    "print(\"Original document:\\n\", row[\"documents\"][0])\n",
    "print(\"\\n Embedding length: \\n\", len(row[\"embeddings\"][0]))\n",
    "print(\"\\n Embedding vector: \\n\", row[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrHjelK_rlNg"
   },
   "source": [
    "So, the embeddings are stored permanently on disk in our Chroma database. Whenever we want to use them again (after restarting/closing kernel), we don‚Äôt need to re-embed the texts. We just point a new client at the same folder and re-open the collection using `get_collection()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOPbC6x3rlNg"
   },
   "outputs": [],
   "source": [
    "#client = chromadb.PersistentClient(path=\"./db/chroma_persist\")\n",
    "#collection = client.get_collection(\"my_documents_locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_LFsFaz3DPS"
   },
   "source": [
    "# 2. Creating Embeddings using OpenAI API\n",
    "\n",
    "Now we‚Äôll switch to provider **OpenAI**. The concept is exactly the same: text gets transformed into numerical vectors, but this time **the computation happens on OpenAI‚Äôs servers** instead of locally.\n",
    "\n",
    "**Authentication using OpenAI API key**:\n",
    "\n",
    "To access OpenAI‚Äôs API, you‚Äôll need an API key. The most convenient way is to store it as an environment variable so it loads automatically whenever your terminal starts. Open the terminal and type `nano ~/.zshrc`. At the end of the file, add: `export CHROMA_OPENAI_API_KEY=\"your_api_key\"`. Save and exit. Then reload your shell config (so the change applies immediately) using `source ~/.zshrc`.\n",
    "\n",
    "We'll use `text-embedding-3-small` model to create the embeddings. For our tiny demo texts, the number of tokens is so small that the cost of creating embeddings with OpenAI is practically negligible.\n",
    "\n",
    "Let's instantiate it using `OpenAIEmbeddingFunction()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('‚úÖ API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('‚ùå ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('‚ùå ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('‚úÖ Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'ü§ñ Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'üß† Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tl_1X5X4rA2"
   },
   "outputs": [],
   "source": [
    "# Instantiating OpenAI embedding function\n",
    "openai_embedding_function = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    model_name = OPENAI_EMBED_MODEL,\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-ayDHoS4q80"
   },
   "source": [
    "Now we need to create a new collection (in the same persistent database). Notice that we specify \"openai_embedding_function\" using`embedding_function` parameter. This ensures that whenever we add documents to this collection, they‚Äôll be embedded with OpenAI instead of the Chroma's default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SR3J-pRWfIJ"
   },
   "outputs": [],
   "source": [
    "# Creating a new collection\n",
    "openai_collection = client.get_or_create_collection(\n",
    "    \"my_documents_openai\",\n",
    "    embedding_function = openai_embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIpkgvm7XPwx"
   },
   "source": [
    "Next we'll add the documents into this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDFD_PVHXM_l"
   },
   "outputs": [],
   "source": [
    "# Adding documents\n",
    "ids = [d[\"id\"] for d in documents]\n",
    "texts = [d[\"text\"] for d in documents]\n",
    "\n",
    "openai_collection.add(ids = ids, documents = texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lvSxEb-rlNh"
   },
   "source": [
    "For the model we chose, each embedding is **1536-dimensional vector**. Because this is such a large array, Jupyter Notebook automatically truncates the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 3: Compare Local vs OpenAI Embeddings (10 minutes)\n",
    "\n",
    "**What you'll practice:** Understanding the differences between local and cloud-based embedding models.\n",
    "\n",
    "**Your task:**\n",
    "1. Retrieve `document_4` (about Phishing) from the `openai_collection`\n",
    "2. Print the embedding vector length for this OpenAI embedding\n",
    "3. Compare it to the embedding length from the local collection (384 dimensions)\n",
    "4. Think about: Why might OpenAI's embeddings be larger? What are the trade-offs?\n",
    "\n",
    "**Discussion points to consider:**\n",
    "- Local embeddings (384d): Faster, free, runs on your computer, good for prototyping\n",
    "- OpenAI embeddings (1536d): Higher dimensional, potentially captures more nuance, costs money, requires API calls\n",
    "- Both can work well depending on your use case!\n",
    "\n",
    "**Hint:** Use `openai_collection.get()` method with the appropriate ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# row = openai_collection.get(ids=[\"document_4\"], include=[\"documents\", \"embeddings\"])\n",
    "# print(\"Document topic:\", row[\"documents\"][0][:100])\n",
    "# print(\"OpenAI embedding length:\", len(row[\"embeddings\"][0]))\n",
    "# print(\"\\nComparison:\")\n",
    "# print(\"  Local model: 384 dimensions\")\n",
    "# print(\"  OpenAI model:\", len(row[\"embeddings\"][0]), \"dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN_cNHSjrlNh",
    "outputId": "c0fb913a-fd4d-4fa6-bc93-f79887d5593a"
   },
   "outputs": [],
   "source": [
    "# Retrieving embedding\n",
    "row = openai_collection.get(ids = [\"document_1\"], include = [\"documents\", \"embeddings\"])\n",
    "\n",
    "print(\"Original document:\\n\", row[\"documents\"][0])\n",
    "print(\"\\n Embedding length: \\n\", len(row[\"embeddings\"][0]))\n",
    "print(\"\\n Embedding vector: \\n\", row[\"embeddings\"][0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
