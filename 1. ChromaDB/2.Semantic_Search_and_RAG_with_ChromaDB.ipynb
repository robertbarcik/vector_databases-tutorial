{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "FWKdhJLn7c3P"
   },
   "source": [
    "# Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb-qDUVv7c3R",
    "outputId": "f4b23a9c-9895-4447-d2ac-5766aa86a6c2"
   },
   "outputs": [],
   "source": [
    "# Install required libraries - CORRECT VERSIONS for NumPy 2.0+\n",
    "!pip install -q chromadb==0.5.3 openai==0.28.1 sentence-transformers==3.0.1 transformers torch\n",
    "\n",
    "print(\"\u2705 All libraries installed successfully!\")\n",
    "print(\"\u26a0\ufe0f  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udca1 Installation Note\n",
    "\n",
    "If you see **dependency conflict warnings** during installation, you can safely **ignore them** - they won't affect this notebook.\n",
    "\n",
    "**Remember:** Always restart your runtime after installation! (Runtime \u2192 Restart runtime)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "73bBj5bt7c3S",
    "outputId": "dfbacafd-3609-4cdb-c4b1-bb41e76d358c"
   },
   "outputs": [],
   "source": [
    "# Upload and extract database (run ONLY if Google Drive didn't work)\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "USE_UPLOADED_DB = True  # Set to True if you want to use uploaded database\n",
    "\n",
    "if USE_UPLOADED_DB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"\ud83d\udce4 Please upload the chromadb_database.zip file from notebook 1...\")\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        if 'chromadb_database.zip' in uploaded:\n",
    "            # Extract the zip file\n",
    "            extract_path = \"./uploaded_db\"\n",
    "            with zipfile.ZipFile('chromadb_database.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "\n",
    "            # Override DB_PATH to use the uploaded database\n",
    "            DB_PATH = extract_path\n",
    "            print(f\"\u2705 Database extracted successfully!\")\n",
    "            print(f\"   Using uploaded database at: {DB_PATH}\")\n",
    "        else:\n",
    "            print(\"\u274c chromadb_database.zip not found in upload. Please try again.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\u2139\ufe0f  Not in Colab. To use uploaded database:\")\n",
    "        print(\"   1. Place chromadb_database.zip in the current directory\")\n",
    "        print(\"   2. Set USE_UPLOADED_DB = True and re-run this cell\")\n",
    "\n",
    "        if os.path.exists('chromadb_database.zip'):\n",
    "            extract_path = \"./uploaded_db\"\n",
    "            with zipfile.ZipFile('chromadb_database.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            DB_PATH = extract_path\n",
    "            print(f\"\u2705 Database extracted from local zip file!\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f  Using Google Drive database (default)\")\n",
    "    print(\"   To use uploaded database instead, set USE_UPLOADED_DB = True above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "C9-DmosFE0Ig"
   },
   "source": [
    "## Upload Database from Notebook 1\n",
    "\n",
    "**IMPORTANT:** This notebook requires the database you created in notebook 1.\n",
    "\n",
    "**Instructions:**\n",
    "1. Go back to notebook 1 (`1.Creating_Embeddings_using_Chroma.ipynb`)\n",
    "2. Scroll to the end and run the download cell\n",
    "3. Download the `chromadb_database.zip` file to your computer\n",
    "4. Come back here and run the cell below to upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "mDbT80nvh2j3"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrmfzxF1i2PK",
    "outputId": "1d5be57d-0d6d-4413-f71d-5199de50d2b4"
   },
   "outputs": [],
   "source": [
    "# Reconnecting to the persistent DB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./uploaded_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX8mS15biGB-",
    "outputId": "2823cd8c-a1bf-4484-feb4-2f2a37f6d9d8"
   },
   "outputs": [],
   "source": [
    "# Reopening the collection and verifying it has data\n",
    "# This cell will check if the collection exists and contains the expected documents from notebook 1\n",
    "\n",
    "try:\n",
    "    local_collection = chroma_client.get_collection(\"my_documents_locally\")\n",
    "    count = local_collection.count()\n",
    "    print(f\"\u2705 Successfully connected to collection 'my_documents_locally'\")\n",
    "    print(f\"   Documents in collection: {count}\")\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"\\n\u26a0\ufe0f  WARNING: Collection exists but contains no documents!\")\n",
    "        print(\"   Please run the first notebook (1.Creating_Embeddings_using_Chroma.ipynb) completely.\")\n",
    "    elif count < 6:\n",
    "        print(f\"\\n\u26a0\ufe0f  WARNING: Expected 6 documents but found {count}\")\n",
    "        print(\"   Please re-run the first notebook to ensure all documents are added.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"\u274c ERROR: Collection 'my_documents_locally' does not exist!\")\n",
    "    print(\"\\n\ud83d\udcdd SOLUTION:\")\n",
    "    print(\"   1. Go back to the first notebook: '1.Creating_Embeddings_using_Chroma.ipynb'\")\n",
    "    print(\"   2. Run ALL cells in that notebook to create and populate the collection\")\n",
    "    print(\"   3. Then return to this notebook\")\n",
    "    print(f\"\\nTechnical details: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "3hWLQ866je3L"
   },
   "source": [
    "Now we can run a semantic search using `query()` function. When we call it, Chroma takes 2 steps:\n",
    "1. It **embeds our query text** using the same embedding function that was used for the documents in the collection.\n",
    "2. It **compares the query embedding with all stored embeddings** and then returns the most relevant results.\n",
    "\n",
    "In this example, we tell Chroma to return the top 2 most relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qdKT1j0jfai",
    "outputId": "d7763b6f-2e77-4529-a085-6b6de1c335a8"
   },
   "outputs": [],
   "source": [
    "query_text = \"What are the foundational principles and technologies used to secure modern internet traffic?\"\n",
    "\n",
    "results = local_collection.query(\n",
    "    query_texts = [query_text],\n",
    "    n_results = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "GFQTr06YE0Ik"
   },
   "source": [
    "To make the output easier to read, let\u2019s loop through the results and print a short preview of each one. The output includes **the matching documents, their IDs and the distance scores** which is a measure of similarity (the smaller the distance, the closer the match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxG0VpC3E0Il",
    "outputId": "c1857ba1-c58b-4172-84b7-cfecffa99a91"
   },
   "outputs": [],
   "source": [
    "# Displaying results in readable format\n",
    "for rank, (document, document_id, distance_score) in enumerate(\n",
    "    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    preview = document[:600]\n",
    "    print(f\"Result {rank}\")\n",
    "    print(f\"\u2022 ID: {document_id}\")\n",
    "    print(f\"\u2022 Distance: {distance_score:.4f}\")\n",
    "    print(f\"\u2022 Preview: {preview}\u2026\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "wWhjb4Hr7c3U"
   },
   "source": [
    "## 1.1 Filtering Semantic Search with Metadata\n",
    "\n",
    "Semantic search is powerful, but sometimes you want to **combine semantic similarity with specific filters**. For example, you might want to find documents about \"encryption\" but only from the \"cryptography\" category, or only beginner-level documents.\n",
    "\n",
    "ChromaDB allows you to add a `where` clause to your queries, just like you learned in the first notebook. This combines the best of both worlds:\n",
    "- **Semantic search** finds conceptually relevant documents\n",
    "- **Metadata filters** narrow down results to specific criteria\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "HwqEd0_R7c3V"
   },
   "source": [
    "### Example 1: Search with Category Filter\n",
    "\n",
    "Let's search for documents about \"network protection\" but only from the \"architecture\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ja631fv7c3V",
    "outputId": "26d9033d-2c6b-4707-a58e-321601c4d688"
   },
   "outputs": [],
   "source": [
    "query_text = \"How to protect networks from attackers?\"\n",
    "\n",
    "# Semantic search with category filter\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=2,\n",
    "    where={\"category\": \"architecture\"},  # Only architecture documents\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: category = 'architecture'\\n\")\n",
    "\n",
    "for rank, (doc, doc_id, metadata, distance) in enumerate(\n",
    "    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    print(f\"Result {rank}:\")\n",
    "    print(f\"  ID: {doc_id}\")\n",
    "    print(f\"  Category: {metadata['category']}\")\n",
    "    print(f\"  Distance: {distance:.4f}\")\n",
    "    print(f\"  Preview: {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "f_2AybmG7c3V"
   },
   "source": [
    "### Example 2: Search with Difficulty Filter\n",
    "\n",
    "Now let's find beginner-friendly documents about authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpHCaT_P7c3V",
    "outputId": "1ceec8ab-6b6c-4291-e640-4082d5c29089"
   },
   "outputs": [],
   "source": [
    "query_text = \"How do authentication systems work?\"\n",
    "\n",
    "# Semantic search with difficulty filter\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3,\n",
    "    where={\"difficulty\": \"beginner\"},  # Only beginner-level documents\n",
    "    include=[\"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: difficulty = 'beginner'\\n\")\n",
    "\n",
    "if results[\"ids\"][0]:\n",
    "    for rank, (doc_id, metadata, distance) in enumerate(\n",
    "        zip(results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "        start=1):\n",
    "        print(f\"Result {rank}: {doc_id}\")\n",
    "        print(f\"  Category: {metadata['category']}, Difficulty: {metadata['difficulty']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\\n\")\n",
    "else:\n",
    "    print(\"No beginner-level documents found matching this query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "AgvdHyYm7c3W"
   },
   "source": [
    "### Example 3: Complex Filters with Operators\n",
    "\n",
    "You can also use operators like `$gte` (greater than or equal), `$in` (in list), etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af2H9fFA7c3W",
    "outputId": "07fdf5d2-463f-4e10-c6b5-f48424650f55"
   },
   "outputs": [],
   "source": [
    "query_text = \"Modern security approaches\"\n",
    "\n",
    "# Find documents from 2010 or later\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3,\n",
    "    where={\"year\": {\"$gte\": 2010}},  # Documents from 2010 onwards\n",
    "    include=[\"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: year >= 2010\\n\")\n",
    "\n",
    "for rank, (doc_id, metadata, distance) in enumerate(\n",
    "    zip(results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    print(f\"Result {rank}: {doc_id}\")\n",
    "    print(f\"  Category: {metadata['category']}, Year: {metadata['year']}\")\n",
    "    print(f\"  Distance: {distance:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "xEccG7DI7c3W"
   },
   "source": [
    "**Key Takeaway:** Filtered semantic search is incredibly powerful. It lets you:\n",
    "- Find semantically relevant content (\"what does it mean?\")\n",
    "- While applying specific business logic (\"show me only X type of documents\")\n",
    "\n",
    "This is essential for building production RAG systems where you need to control which documents can be retrieved based on user permissions, document types, dates, or other criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "kuJhtvFm7c3W"
   },
   "source": [
    "### \ud83d\udcdd EXERCISE 1: Try Your Own Semantic Search (5-7 minutes)\n",
    "\n",
    "**What you'll practice:** Running semantic searches and understanding how query phrasing affects results.\n",
    "\n",
    "**Your task:**\n",
    "1. Create your own query about a cybersecurity topic (e.g., \"How do hackers steal passwords?\", \"What is modern network security?\", \"How do viruses work?\")\n",
    "2. Use `local_collection.query()` to search for the top 3 most relevant documents\n",
    "3. Print the results showing the document IDs and distance scores\n",
    "4. Experiment: Try rephrasing your question differently - do you get similar results?\n",
    "\n",
    "**Hint:** Use the same structure as the example above. Remember to set `n_results=3` to get 3 documents.\n",
    "\n",
    "**Expected outcome:** You should see that semantically similar questions return similar documents, even if worded differently. Lower distance scores mean better matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "UAEbHNlJ7c3W"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# my_query = \"Your question here\"\n",
    "# results = local_collection.query(\n",
    "#     query_texts=[my_query],\n",
    "#     n_results=3\n",
    "# )\n",
    "#\n",
    "# for rank, (doc, doc_id, distance) in enumerate(\n",
    "#     zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n",
    "#     start=1):\n",
    "#     print(f\"Result {rank}: {doc_id}, Distance: {distance:.4f}\")\n",
    "#     print(f\"Preview: {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "pc2CfDbs7c3W"
   },
   "source": [
    "## 1.2 Optimizing n_results: How Many Documents Should You Retrieve?\n",
    "\n",
    "When performing semantic search for RAG, one critical parameter is `n_results` - how many documents to retrieve. This isn't just a technical detail; it's a key design decision that affects your system's quality, cost, and performance.\n",
    "\n",
    "### The Trade-offs\n",
    "\n",
    "**Too Few Documents (n_results = 1-2):**\n",
    "- \u2705 **Pros:**\n",
    "  - Faster retrieval\n",
    "  - Lower LLM cost (less context to process)\n",
    "  - More focused answers\n",
    "- \u274c **Cons:**\n",
    "  - **Low recall**: Might miss relevant information spread across multiple documents\n",
    "  - Risk of incomplete answers\n",
    "  - Single point of failure if top result isn't perfect\n",
    "\n",
    "**Too Many Documents (n_results = 10+):**\n",
    "- \u2705 **Pros:**\n",
    "  - **High recall**: More likely to capture all relevant information\n",
    "  - Better coverage of the topic\n",
    "- \u274c **Cons:**\n",
    "  - **Context size explosion**: LLMs have token limits (e.g., GPT-4: 8k-128k tokens)\n",
    "  - Higher costs (more tokens = more $$$)\n",
    "  - **Noise**: Irrelevant documents can confuse the LLM\n",
    "  - Slower processing\n",
    "  - LLM may struggle to synthesize information from too many sources\n",
    "\n",
    "### Finding the Sweet Spot\n",
    "\n",
    "Let's experiment with different values of `n_results` to see how it affects our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5Iergs97c3W",
    "outputId": "f423788a-723d-4978-8272-7aefd99e7925"
   },
   "outputs": [],
   "source": [
    "query_text = \"What are the main security threats?\"\n",
    "\n",
    "print(f\"Query: {query_text}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test different n_results values\n",
    "for n in [1, 3, 5]:\n",
    "    results = local_collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n,\n",
    "        include=[\"distances\", \"documents\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nn_results = {n}:\")\n",
    "    print(f\"  Document IDs: {results['ids'][0]}\")\n",
    "    print(f\"  Distance scores: {[f'{d:.4f}' for d in results['distances'][0]]}\")\n",
    "\n",
    "    # Calculate approximate token count (rough estimate: 1 token \u2248 4 characters)\n",
    "    total_chars = sum(len(doc) for doc in results[\"documents\"][0])\n",
    "    approx_tokens = total_chars // 4\n",
    "    print(f\"  Approx context tokens: ~{approx_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "YPUyUhxl7c3X"
   },
   "source": [
    "### Practical Guidelines\n",
    "\n",
    "**1. Start with n_results = 3-5** for most use cases:\n",
    "   - Good balance between recall and context size\n",
    "   - Works well with most LLMs' context windows\n",
    "   - Manageable cost\n",
    "\n",
    "**2. Adjust based on your documents:**\n",
    "   - **Short documents** (tweets, Q&A pairs): Can use higher n_results (10-20)\n",
    "   - **Long documents** (articles, papers): Use lower n_results (2-5)\n",
    "   - **Chunked documents**: Consider 5-10 chunks\n",
    "\n",
    "**3. Consider your LLM's context window:**\n",
    "   - GPT-3.5-turbo: 4k tokens \u2192 Keep context under 3k\n",
    "   - GPT-4: 8k-32k tokens \u2192 More flexibility\n",
    "   - Local models: Often 2k-4k \u2192 Be conservative\n",
    "\n",
    "**4. Monitor distance scores:**\n",
    "   - If the 3rd result has a distance > 0.5, the remaining results probably won't help\n",
    "   - Use a **distance threshold** instead of fixed n_results\n",
    "\n",
    "**5. Advanced technique - Reranking:**\n",
    "   - Retrieve more documents (n=10-20)\n",
    "   - Use a reranking model to select the best 3-5\n",
    "   - Send only the reranked results to the LLM\n",
    "\n",
    "### Example: Dynamic n_results with Distance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdNMSA0o7c3X",
    "outputId": "27560052-2fca-4959-dedc-dfb0116f8293"
   },
   "outputs": [],
   "source": [
    "def smart_retrieve(collection, query, max_results=5, distance_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Retrieve documents but stop if distance scores get too high (low similarity)\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=max_results,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Filter out documents with distance > threshold\n",
    "    filtered_docs = []\n",
    "    filtered_ids = []\n",
    "\n",
    "    for doc, doc_id, dist in zip(\n",
    "        results[\"documents\"][0],\n",
    "        results[\"ids\"][0],\n",
    "        results[\"distances\"][0]\n",
    "    ):\n",
    "        if dist <= distance_threshold:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_ids.append(doc_id)\n",
    "        else:\n",
    "            print(f\"  Skipping {doc_id} (distance {dist:.4f} > threshold {distance_threshold})\")\n",
    "\n",
    "    return filtered_docs, filtered_ids\n",
    "\n",
    "# Test it\n",
    "query = \"How does public key cryptography work?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "docs, ids = smart_retrieve(local_collection, query, max_results=5, distance_threshold=0.5)\n",
    "print(f\"\\nRetrieved {len(docs)} high-quality documents: {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "72G_-HW57c3X"
   },
   "source": [
    "**Key Takeaway:**\n",
    "\n",
    "Don't just blindly set `n_results=5`. Think about:\n",
    "- Your document length and structure\n",
    "- Your LLM's context window\n",
    "- Your quality vs. cost trade-offs\n",
    "- The semantic distance of retrieved results\n",
    "\n",
    "For most applications, **n_results=3** with a **distance threshold of 0.5-0.6** is a good starting point. Then tune based on your specific needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "P60ghuSFkF1F"
   },
   "source": [
    "# 2. RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "We\u2019ll keep doing the same search, but now we\u2019ll hand the retrieved text to an LLM and have it compose an answer while staying grounded in our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "M35L8wXbkaZf"
   },
   "source": [
    "## 2.1 OpenAI API\n",
    "\n",
    "We\u2019ll access LLM through the OpenAI API. To do this, we first load the API key from the environment and then create a client object that we will use to send requests to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13",
    "outputId": "ffe8e309-d2c1-4e2f-d228-9d1c61225261"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "# Try loading from Google Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print('\u2705 API key loaded from Colab secrets')\n",
    "except ImportError:\n",
    "    # Not in Colab environment\n",
    "    pass\n",
    "except Exception as e:\n",
    "    # Colab secrets not configured\n",
    "    print('\ud83d\udca1 Colab secret not found. You can add it via: \ud83d\udd11 (Secrets in left sidebar) \u2192 OPENAI_API_KEY')\n",
    "\n",
    "# Fall back to environment variable\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('\u2705 API key loaded from environment variable')\n",
    "\n",
    "# If still not found, prompt user\n",
    "if not OPENAI_API_KEY:\n",
    "    print('\\n\ud83d\udd11 OpenAI API key not found.')\n",
    "    print('   To use Colab secrets: Click \ud83d\udd11 in left sidebar \u2192 Add OPENAI_API_KEY')\n",
    "    print('   Or enter it below (will be hidden):\\n')\n",
    "    OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('\u274c ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('\\n\u2705 OpenAI API configured!')\n",
    "print('\ud83e\udd16 Using model: gpt-5-nano (cost-efficient)')\n",
    "print('\ud83e\udde0 Embedding model: text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "gqlWxwLUE0In"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "FJk-NZs7E0In"
   },
   "outputs": [],
   "source": [
    "# Loading API key from the environment\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Creating a client that we'll use to call the API\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "xsj2CHTFE0Io"
   },
   "source": [
    "We will query Chroma collection to get the most relevant passages for the question. Here we ask for the top 3 results, and we also include their IDs. These IDs help us later show where the information came from. If no results are found, we stop early and tell the user that nothing matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "cVbG6mMzE0Io"
   },
   "outputs": [],
   "source": [
    "query_text = \"What is the difference between symmetric and asymmetric encryption?\"\n",
    "\n",
    "# Step 1: Retrieving relevant passages from Chroma\n",
    "response = local_collection.query(\n",
    "    query_texts = [query_text],\n",
    "    n_results = 3,\n",
    "    include = [\"documents\"]\n",
    ")\n",
    "\n",
    "docs = response[\"documents\"][0]\n",
    "ids = response[\"ids\"][0]\n",
    "\n",
    "if not docs:\n",
    "    print(\"No relevant passages found in the vector store. Try rephrasing the question.\")\n",
    "else:\n",
    "    # Step 2: Packing retrieved results into a labeled context block to make the input clearer for the LLM\n",
    "    context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n",
    "    context = \"\\n\\n\".join(context_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "P-UNCV_qE0Io"
   },
   "source": [
    "Finally, we'll send retrieved passages along with the question to the OpenAI model `gpt-5-nano`. The `system` message is an instruction that controls how the model should behave. The `user` message then supplies both the sources and the question, so the model has all the context it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "EEcoZmst7c3Y"
   },
   "source": [
    "### \ud83d\udcdd EXERCISE 2: Build Your Own RAG Query with OpenAI (10-12 minutes)\n",
    "\n",
    "**What you'll practice:** Creating a complete RAG pipeline from retrieval to answer generation.\n",
    "\n",
    "**Your task:**\n",
    "1. Think of a new question about the cybersecurity documents (e.g., \"What is Stuxnet?\", \"How does Zero Trust work?\", \"What makes phishing attacks successful?\")\n",
    "2. Retrieve the top 2 relevant documents from `local_collection`\n",
    "3. Create a context block from the retrieved documents\n",
    "4. Send the context and your question to OpenAI's API to generate an answer\n",
    "5. Print the final answer\n",
    "\n",
    "**Hint:** Follow the same pattern as the example above. You'll need to:\n",
    "- Use `local_collection.query()` to retrieve documents\n",
    "- Format the context with document IDs\n",
    "- Use `client.chat.completions.create()` to generate the answer\n",
    "\n",
    "**Expected outcome:** You should get a clear, cited answer that references specific source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "Tc8T7s447c3Y"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "#\n",
    "# my_question = \"Your question here\"\n",
    "#\n",
    "# # Step 1: Retrieve\n",
    "# response = local_collection.query(\n",
    "#     query_texts=[my_question],\n",
    "#     n_results=2,\n",
    "#     include=[\"documents\"]\n",
    "# )\n",
    "#\n",
    "# docs = response[\"documents\"][0]\n",
    "# ids = response[\"ids\"][0]\n",
    "#\n",
    "# # Step 2: Format context\n",
    "# context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n",
    "# context = \"\\n\\n\".join(context_blocks)\n",
    "#\n",
    "# # Step 3: Generate answer\n",
    "# resp = client.chat.completions.create(\n",
    "#     model=OPENAI_MODEL,\n",
    "#     temperature=0,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a precise assistant. Answer using only the provided sources and cite them.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {my_question}\\n\\nANSWER:\"}\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "fvRstf_qE0Io",
    "outputId": "2b840796-c9cc-43fe-ed4f-115d6010bb17"
   },
   "outputs": [],
   "source": [
    "# Step 3: Generating an answer using the retrieved context\n",
    "resp = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=(\n",
    "        \"You are a precise assistant that must answer ONLY using the provided sources. \"\n",
    "        \"If the answer is not fully supported by the sources, explain that the information is not in the provided material and suggest how the user might rephrase their question. \"\n",
    "        \"At the end, cite sources by their bracket labels, e.g., [1], [2].\\n\\n\"\n",
    "        f\"SOURCES:\\n{context}\\n\\n\"\n",
    "        f\"QUESTION: {query_text}\\n\\n\"\n",
    "        f\"ANSWER:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "_QWnOZ4H7c3Y"
   },
   "source": [
    "## 2.1.1 RAG Evaluation and Source Attribution\n",
    "\n",
    "So far, we've built a RAG pipeline that retrieves documents and generates answers. But there's a critical question we haven't addressed: **How do we know if the LLM actually used the retrieved sources, or if it just made up an answer?**\n",
    "\n",
    "This is one of the biggest challenges in production RAG systems:\n",
    "\n",
    "1. **No verification that sources are actually used**: After getting an LLM answer, there's often no check whether the answer actually came from the retrieved context\n",
    "2. **Hallucinations**: LLMs can confidently generate information that wasn't in the sources at all\n",
    "3. **Source attribution**: Even when the answer is correct, it's hard to verify which specific source documents were actually used\n",
    "\n",
    "Let's explore techniques to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "AqUpp3ko7c3Y"
   },
   "source": [
    "### Technique 1: Explicit Citation Requirements\n",
    "\n",
    "The simplest approach is to **force the LLM to cite its sources** in the system prompt. Let's compare answers with and without citation requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "XgjlTJde7c3Y"
   },
   "outputs": [],
   "source": [
    "# Example WITHOUT citation requirement\n",
    "query_text = \"What is quantum computing?\"\n",
    "\n",
    "response = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=2,\n",
    "    include=[\"documents\"]\n",
    ")\n",
    "\n",
    "docs = response[\"documents\"][0]\n",
    "ids = response[\"ids\"][0]\n",
    "\n",
    "if docs:\n",
    "    context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    \n",
    "    # Weak system prompt - no citation requirement\n",
    "    resp_no_citation = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=f\"You are a helpful assistant. Answer the question using the provided sources.\\n\\nSOURCES:\\n{context}\\n\\nQUESTION: {query_text}\"\n",
    "    )\n",
    "    \n",
    "    print(\"\u274c WITHOUT Citation Requirement:\")\n",
    "    print(resp_no_citation.output_text)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "r-rsWN657c3Z"
   },
   "outputs": [],
   "source": [
    "    # Now WITH strong citation requirement\n",
    "    resp_with_citation = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=(\n",
    "            \"You are a precise assistant. Answer ONLY using the provided sources. \"\n",
    "            \"You MUST cite every claim with [1], [2], etc. \"\n",
    "            \"If the answer is not in the sources, say 'The provided sources do not contain this information.'\\n\\n\"\n",
    "            f\"SOURCES:\\n{context}\\n\\n\"\n",
    "            f\"QUESTION: {query_text}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 WITH Citation Requirement:\")\n",
    "    print(resp_with_citation.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "WZ5EuQr87c3Z"
   },
   "source": [
    "**Key Observation:** Without explicit citation requirements, the LLM may provide an answer that sounds plausible but isn't grounded in the sources. With citations, we can trace every claim back to a specific document.\n",
    "\n",
    "### Technique 2: Programmatic Source Verification\n",
    "\n",
    "A more robust approach is to **automatically verify** if the LLM's answer actually uses content from the retrieved sources. We can do this by checking for text overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "HYV7Pp6R7c3Z"
   },
   "outputs": [],
   "source": [
    "def verify_source_usage(answer, source_docs, min_overlap_words=5):\n",
    "    \"\"\"\n",
    "    Check if the LLM answer contains content from the source documents.\n",
    "    Returns a dict with verification metrics.\n",
    "    \"\"\"\n",
    "    answer_lower = answer.lower()\n",
    "    answer_words = set(answer_lower.split())\n",
    "    \n",
    "    verification = {\n",
    "        \"uses_sources\": False,\n",
    "        \"source_overlaps\": [],\n",
    "        \"total_overlap_words\": 0\n",
    "    }\n",
    "    \n",
    "    for i, doc in enumerate(source_docs):\n",
    "        doc_lower = doc.lower()\n",
    "        doc_words = set(doc_lower.split())\n",
    "        \n",
    "        # Find common words (excluding very short words)\n",
    "        overlap = answer_words & doc_words\n",
    "        meaningful_overlap = {w for w in overlap if len(w) > 3}\n",
    "        \n",
    "        if len(meaningful_overlap) >= min_overlap_words:\n",
    "            verification[\"uses_sources\"] = True\n",
    "            verification[\"source_overlaps\"].append({\n",
    "                \"source_index\": i + 1,\n",
    "                \"overlap_count\": len(meaningful_overlap),\n",
    "                \"sample_words\": list(meaningful_overlap)[:10]\n",
    "            })\n",
    "            verification[\"total_overlap_words\"] += len(meaningful_overlap)\n",
    "    \n",
    "    return verification\n",
    "\n",
    "# Test it with a real RAG query\n",
    "test_query = \"What are encryption algorithms?\"\n",
    "response = local_collection.query(\n",
    "    query_texts=[test_query],\n",
    "    n_results=2,\n",
    "    include=[\"documents\"]\n",
    ")\n",
    "\n",
    "test_docs = response[\"documents\"][0]\n",
    "test_ids = response[\"ids\"][0]\n",
    "\n",
    "if test_docs:\n",
    "    context_blocks = [f\"[{i+1} | {test_ids[i]}]\\n{test_docs[i]}\" for i in range(len(test_docs))]\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    \n",
    "    # Get LLM answer using Responses API\n",
    "    test_resp = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=f\"Answer using the provided sources.\\n\\nSOURCES:\\n{context}\\n\\nQUESTION: {test_query}\"\n",
    "    )\n",
    "    \n",
    "    answer = test_resp.output_text\n",
    "    \n",
    "    # Verify source usage\n",
    "    verification = verify_source_usage(answer, test_docs)\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n\u2713 Verification Results:\")\n",
    "    print(f\"  Uses sources: {verification['uses_sources']}\")\n",
    "    print(f\"  Total word overlap: {verification['total_overlap_words']}\")\n",
    "    for overlap_info in verification[\"source_overlaps\"]:\n",
    "        print(f\"  Source [{overlap_info['source_index']}]: {overlap_info['overlap_count']} overlapping words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "z8gjskJh7c3Z"
   },
   "source": [
    "**Key Takeaway - RAG Evaluation:**\n",
    "\n",
    "In production RAG systems, you should:\n",
    "\n",
    "1. **Enforce citations** in system prompts - make the LLM cite sources with [1], [2], etc.\n",
    "2. **Verify programmatically** - use overlap analysis or semantic similarity to check if answers use the sources\n",
    "3. **Monitor hallucinations** - if word overlap is too low, the LLM may be inventing information\n",
    "4. **Build feedback loops** - log cases where verification fails for continuous improvement\n",
    "\n",
    "These techniques help ensure your RAG system stays grounded in facts and doesn't hallucinate information that isn't in your knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "JNjIHs2IkWjY"
   },
   "source": [
    "## 2.2 Local Model from Hugging Face\n",
    "\n",
    "In the previous section, we combined local retrieval with ChromaDB (embeddings stored on disk) and then sent the retrieved passages to OpenAI\u2019s API for generation. That worked well but it also meant our data had to leave our computer and be processed by an external service.\n",
    "\n",
    "What if we don\u2019t want to send private documents outside our environment? In this section we\u2019ll keep **everything on-device by running a model locally**. We'll use a pretrained instruction-tuned model from HuggingFace called `Qwen/Qwen2.5-1.5B-Instruct`.\n",
    "\n",
    "This model is small enough to run on a laptop or a single GPU, yet powerful enough to follow instructions and generate answers. With it, both retrieval and generation happen locally:\n",
    "- our data never leave our computer\n",
    "- we avoid per-request API costs\n",
    "- we reduce reliance on external services and avoid latency from network calls\n",
    "\n",
    "You can read more about this model on [HuggingFace website](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct).\n",
    "\n",
    "We start by importing the Hugging Face `transformers` library, which gives us the tools to load and run local language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "mi41rZTSE0Io"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "egn5A1nOE0Io"
   },
   "source": [
    "We then load two key components:\n",
    "- **Tokenizer**: This converts our input text (the context and the question) into numerical tokens the model can understand and process. Later, the tokenizer also converts the model\u2019s output tokens back into human-readable text.\n",
    "- **Model**: This is the actual neural network with all its trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "BqogdfnvE0Io"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Pulling down the model weights and config\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",       # place the model on GPU if available, CPU otherwise\n",
    "    torch_dtype = \"float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "6ZNjw4LGE0Io"
   },
   "source": [
    "Now we'll create custom function tht ties everything together. First, it opens the Chroma collection and retrieves the top-k most relevant documents for the given question. Those documents are combined into a prompt which instructs the model to answer only using the retrieved context. The prompt is then tokenized and passed to the language model for generation. The model produces new tokens, which are decoded back into text to form the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "zBvIjL2sE0Ip"
   },
   "outputs": [],
   "source": [
    "def get_answer(client, collection_name: str, question: str, k: int = 2, max_new_tokens: int = 120):\n",
    "    # Opening the collection\n",
    "    col = client.get_collection(collection_name)\n",
    "\n",
    "    # Retrieving top-k documents\n",
    "    res = col.query(query_texts=[question], n_results=k, include=[\"documents\"])\n",
    "    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n",
    "    if not docs:\n",
    "        return \"Insufficient context.\"\n",
    "\n",
    "    # The prompt\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant.\\n\"\n",
    "        \"Answer ONLY using the CONTEXT below.\"\n",
    "        \"If the CONTEXT is insufficient, reply exactly: Insufficient context.\\n\"\n",
    "        \"Keep the answer short (1\u20132 sentences).\\n\\n\"\n",
    "        \"----- CONTEXT -----\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"-------------------\\n\"\n",
    "        f\"QUESTION: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Tokenizing and generating\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "    outputs = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decoding only the newly generated tokens\n",
    "    answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "hwqhgkIsE0Ip"
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between symmetric and asymmetric encryption?\"\n",
    "\n",
    "# Calling the RAG function\n",
    "answer = get_answer(\n",
    "    client = chroma_client,\n",
    "    collection_name = \"my_documents_locally\",\n",
    "    question = question,\n",
    "    k = 2\n",
    ")\n",
    "\n",
    "# Displaying the output\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}