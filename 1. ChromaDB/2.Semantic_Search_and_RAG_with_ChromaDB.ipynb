{"cells":[{"cell_type":"markdown","metadata":{"id":"C9-DmosFE0Ig"},"source":["# Introduction\n","\n","In the previous notebook, we learned how to create embeddings from text data and store them in a persistent Chroma database. That gave us a powerful foundation: a searchable knowledge base where meaning, not just keywords, is captured.\n","\n","Now, we’re going to build on that foundation. The notebook is separated into 2 sections:\n","\n","**1. Semantic Search with ChromaDB**:\n","\n","We’ll explore how to **query our database and retrieve the most relevant results based on semantic similarity**. Instead of simply matching words, we’ll be able to find passages that mean the same thing, even if they’re phrased differently. This is the first step toward building truly intelligent search systems.\n","  \n","**2. Retrieval-Augmented Generation (RAG)**:\n","\n","Once we understand how to perform semantic search, we’ll continue with building a Retrieval-Augmented Generation (RAG) pipeline. This is where **we combine information retrieval (finding the right documents) with large language models** that can generate natural, human-like answers.\n","\n","We’ll implement RAG in two different ways:\n","\n","- **Using OpenAI’s model**: We’ll connect our Chroma database to an OpenAI model, letting it use retrieved context to answer questions in a conversational way.\n","- **Using a local Hugging Face model**: We’ll replicate the same process using a local model, showing how you can achieve RAG workflows without relying on cloud APIs.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E-NbW97Qi-u3"},"source":["# 1. Semantic Search with ChromaDB\n","\n","Let's jump straight into using ChromaDB to perform searches against the database we built earlier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDbT80nvh2j3"},"outputs":[],"source":["# Importing\n","import chromadb\n","import os"]},{"cell_type":"markdown","metadata":{"id":"9F3ESZFZiict"},"source":["First, we reconnect to the same on-disk database by creating a PersistentClient with the exact path we used before. This doesn’t recreate anything. It just points the new notebook at the existing Chroma files so all our collections and records are available again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGWc2SMkh_Yy"},"outputs":[],"source":["# Reconnecting to the persistent DB\n","chroma_client = chromadb.PersistentClient(path=\"./db/chroma_persist\")"]},{"cell_type":"markdown","metadata":{"id":"jrmfzxF1i2PK"},"source":["Next, we reopen the collection we created earlier, called \"my_documents_locally\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kX8mS15biGB-"},"outputs":[],"source":["# Reopening the collection\n","local_collection = chroma_client.get_collection(\"my_documents_locally\")"]},{"cell_type":"markdown","metadata":{"id":"3hWLQ866je3L"},"source":["Now we can run a semantic search using `query()` function. When we call it, Chroma takes 2 steps:\n","1. It **embeds our query text** using the same embedding function that was used for the documents in the collection.\n","2. It **compares the query embedding with all stored embeddings** and then returns the most relevant results.\n","\n","In this example, we tell Chroma to return the top 2 most relevant documents:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qdKT1j0jfai"},"outputs":[],"source":["query_text = \"What are the foundational principles and technologies used to secure modern internet traffic?\"\n","\n","results = local_collection.query(\n","    query_texts = [query_text],\n","    n_results = 2,\n",")"]},{"cell_type":"markdown","metadata":{"id":"GFQTr06YE0Ik"},"source":["To make the output easier to read, let’s loop through the results and print a short preview of each one. The output includes **the matching documents, their IDs and the distance scores** which is a measure of similarity (the smaller the distance, the closer the match)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxG0VpC3E0Il","outputId":"a7b6dd0e-e3b5-42a2-aeb6-aeceabcce866"},"outputs":[{"name":"stdout","output_type":"stream","text":["Result 1\n","• ID: document_1\n","• Distance: 0.5760\n","• Preview: Public-key cryptography, also known as asymmetric cryptography, represents a monumental paradigm shift from its predecessor, symmetric cryptography. The fundamental challenge it solves is that of key distribution, which was the Achilles' heel of symmetric systems requiring a shared secret key to be exchanged over a secure channel beforehand. Introduced conceptually by Whitfield Diffie and Martin Hellman in their seminal 1976 paper \"New Directions in Cryptography,\" this system utilizes pairs of mathematically linked keys: a public key and a private key. The public key can be shared openly witho…\n","--------------------------------------------------------------------------------\n","Result 2\n","• ID: document_3\n","• Distance: 0.6161\n","• Preview: The Zero Trust Architecture (ZTA) is a modern cybersecurity strategy built on a profound philosophical shift: 'never trust, always verify.' This model fundamentally rejects the outdated 'castle-and-moat' concept of security, where a hardened perimeter was thought to be sufficient to protect a trusted internal network. That traditional model is no longer viable in an era of cloud computing, remote workforces, and sophisticated persistent threats that often breach the perimeter. Coined by John Kindervag at Forrester Research in 2010, Zero Trust mandates a granular, identity-centric approach to s…\n","--------------------------------------------------------------------------------\n"]}],"source":["# Displaying results in readable format\n","for rank, (document, document_id, distance_score) in enumerate(\n","    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n","    start=1):\n","    preview = document[:600]\n","    print(f\"Result {rank}\")\n","    print(f\"• ID: {document_id}\")\n","    print(f\"• Distance: {distance_score:.4f}\")\n","    print(f\"• Preview: {preview}…\")\n","    print(\"-\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"P60ghuSFkF1F"},"source":["# 2. RAG (Retrieval-Augmented-Generation)\n","\n","We’ll keep doing the same search, but now we’ll hand the retrieved text to an LLM and have it compose an answer while staying grounded in our documents."]},{"cell_type":"markdown","metadata":{"id":"M35L8wXbkaZf"},"source":["## 2.1 OpenAI API\n","\n","We’ll access LLM through the OpenAI API. To do this, we first load the API key from the environment and then create a client object that we will use to send requests to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqlWxwLUE0In"},"outputs":[],"source":["import openai\n","from openai import OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJk-NZs7E0In"},"outputs":[],"source":["# Loading API key from the environment\n","openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","\n","# Creating a client that we'll use to call the API\n","client = OpenAI()"]},{"cell_type":"markdown","metadata":{"id":"xsj2CHTFE0Io"},"source":["We will query Chroma collection to get the most relevant passages for the question. Here we ask for the top 3 results, and we also include their IDs. These IDs help us later show where the information came from. If no results are found, we stop early and tell the user that nothing matched."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVbG6mMzE0Io"},"outputs":[],"source":["query_text = \"What is the difference between symmetric and asymmetric encryption?\"\n","\n","# Step 1: Retrieving relevant passages from Chroma\n","response = local_collection.query(\n","    query_texts = [query_text],\n","    n_results = 3,\n","    include = [\"documents\"]\n",")\n","\n","docs = response[\"documents\"][0]\n","ids = response[\"ids\"][0]\n","\n","if not docs:\n","    print(\"No relevant passages found in the vector store. Try rephrasing the question.\")\n","else:\n","    # Step 2: Packing retrieved results into a labeled context block to make the input clearer for the LLM\n","    context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n","    context = \"\\n\\n\".join(context_blocks)"]},{"cell_type":"markdown","metadata":{"id":"P-UNCV_qE0Io"},"source":["Finally, we'll send retrieved passages along with the question to the OpenAI model `gpt-4o-mini`. The `system` message is an instruction that controls how the model should behave. The `user` message then supplies both the sources and the question, so the model has all the context it needs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvRstf_qE0Io","outputId":"157d5ef3-7c86-46b3-936f-fa5fdd0d052e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The provided sources do not explicitly detail the differences between symmetric and asymmetric encryption. However, it is mentioned that public-key cryptography (asymmetric cryptography) represents a shift from symmetric cryptography, primarily addressing the challenge of key distribution. Symmetric cryptography requires a shared secret key exchanged over a secure channel, while asymmetric cryptography uses a pair of keys: a public key that can be shared openly and a private key that must be kept secret. \n","\n","For a more comprehensive answer, you might consider rephrasing your question to ask specifically about the characteristics or mechanisms of symmetric encryption, or you could inquire about the advantages and disadvantages of each type of encryption. \n","\n","Sources: [1]\n"]}],"source":["# Step 3: Generating an answer using the retrieved context\n","resp = client.chat.completions.create(\n","    model = \"gpt-4o-mini\",\n","    temperature = 0,\n","    max_tokens = 500,\n","    messages=[\n","        {\n","            \"role\": \"system\",\n","            \"content\": (\n","                \"You are a precise assistant that must answer ONLY using the provided sources.\"\n","                \"If the answer is not fully supported by the sources, explain that the information is not in the provided material and suggest how the user might rephrase their question.\"\n","                \"At the end, cite sources by their bracket labels, e.g., [1], [2].\"\n","            ),\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {query_text}\\n\\nANSWER:\"\n","        },\n","    ],\n",")\n","\n","print(resp.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{"id":"JNjIHs2IkWjY"},"source":["## 2.2 Local Model from Hugging Face\n","\n","In the previous section, we combined local retrieval with ChromaDB (embeddings stored on disk) and then sent the retrieved passages to OpenAI’s API for generation. That worked well but it also meant our data had to leave our computer and be processed by an external service.\n","\n","What if we don’t want to send private documents outside our environment? In this section we’ll keep **everything on-device by running a model locally**. We'll use a pretrained instruction-tuned model from HuggingFace called `Qwen/Qwen2.5-1.5B-Instruct`.\n","\n","This model is small enough to run on a laptop or a single GPU, yet powerful enough to follow instructions and generate answers. With it, both retrieval and generation happen locally:\n","- our data never leave our computer\n","- we avoid per-request API costs\n","- we reduce reliance on external services and avoid latency from network calls\n","\n","You can read more about this model on [HuggingFace website](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct).\n","\n","We start by importing the Hugging Face `transformers` library, which gives us the tools to load and run local language models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mi41rZTSE0Io"},"outputs":[],"source":["# Importing\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"]},{"cell_type":"markdown","metadata":{"id":"egn5A1nOE0Io"},"source":["We then load two key components:\n","- **Tokenizer**: This converts our input text (the context and the question) into numerical tokens the model can understand and process. Later, the tokenizer also converts the model’s output tokens back into human-readable text.\n","- **Model**: This is the actual neural network with all its trained weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqogdfnvE0Io","outputId":"603c42b7-ab2a-48d3-f2b0-de2b55e48de3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use mps\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Pulling down the model weights and config\n","llm = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map = \"auto\",       # place the model on GPU if available, CPU otherwise\n","    torch_dtype = \"float16\")"]},{"cell_type":"markdown","metadata":{"id":"6ZNjw4LGE0Io"},"source":["Now we'll create custom function tht ties everything together. First, it opens the Chroma collection and retrieves the top-k most relevant documents for the given question. Those documents are combined into a prompt which instructs the model to answer only using the retrieved context. The prompt is then tokenized and passed to the language model for generation. The model produces new tokens, which are decoded back into text to form the final answer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBvIjL2sE0Ip"},"outputs":[],"source":["def get_answer(client, collection_name: str, question: str, k: int = 2, max_new_tokens: int = 120):\n","    # Opening the collection\n","    col = client.get_collection(collection_name)\n","\n","    # Retrieving top-k documents\n","    res = col.query(query_texts=[question], n_results=k, include=[\"documents\"])\n","    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n","    if not docs:\n","        return \"Insufficient context.\"\n","\n","    # The prompt\n","    context = \"\\n\\n\".join(docs)\n","    prompt = (\n","        \"You are a precise assistant.\\n\"\n","        \"Answer ONLY using the CONTEXT below.\"\n","        \"If the CONTEXT is insufficient, reply exactly: Insufficient context.\\n\"\n","        \"Keep the answer short (1–2 sentences).\\n\\n\"\n","        \"----- CONTEXT -----\\n\"\n","        f\"{context}\\n\"\n","        \"-------------------\\n\"\n","        f\"QUESTION: {question}\\n\"\n","        \"Answer:\"\n","    )\n","\n","    # Tokenizing and generating\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n","    outputs = llm.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        temperature=0.7,\n","        top_p=0.9,\n","        do_sample=True\n","    )\n","\n","    # Decoding only the newly generated tokens\n","    answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n","    return answer.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwqhgkIsE0Ip","outputId":"17665786-fbdd-46e2-d715-b6b80c4855df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is the difference between symmetric and asymmetric encryption?\n","Answer: Symmetric encryption uses a single key for both encryption and decryption, whereas asymmetric encryption requires two different keys - a public key for encryption and a private key for decryption. In symmetric encryption, the same key needs to be securely distributed to each participant, while in asymmetric encryption, the keys remain separate. Additionally, symmetric encryption is faster but less secure than asymmetric encryption due to the risk of key compromise. Asymmetric encryption provides better security against eavesdroppers, but it takes more time and resources to perform the same amount of data encryption compared to symmetric encryption. Both methods ensure that data remains confidential and authentic\n"]}],"source":["question = \"What is the difference between symmetric and asymmetric encryption?\"\n","\n","# Calling the RAG function\n","answer = get_answer(\n","    client = chroma_client,\n","    collection_name = \"my_documents_locally\",\n","    question = question,\n","    k = 2\n",")\n","\n","# Displaying the output\n","print(\"Question:\", question)\n","print(\"Answer:\", answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XiFGYC-BE0Ip"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}