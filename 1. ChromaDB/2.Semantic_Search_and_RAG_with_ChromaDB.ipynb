{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9-DmosFE0Ig"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In the previous notebook, we learned how to create embeddings from text data and store them in a persistent Chroma database. That gave us a powerful foundation: a searchable knowledge base where meaning, not just keywords, is captured.\n",
    "\n",
    "Now, we‚Äôre going to build on that foundation. The notebook is separated into 2 sections:\n",
    "\n",
    "**1. Semantic Search with ChromaDB**:\n",
    "\n",
    "We‚Äôll explore how to **query our database and retrieve the most relevant results based on semantic similarity**. Instead of simply matching words, we‚Äôll be able to find passages that mean the same thing, even if they‚Äôre phrased differently. This is the first step toward building truly intelligent search systems.\n",
    "  \n",
    "**2. Retrieval-Augmented Generation (RAG)**:\n",
    "\n",
    "Once we understand how to perform semantic search, we‚Äôll continue with building a Retrieval-Augmented Generation (RAG) pipeline. This is where **we combine information retrieval (finding the right documents) with large language models** that can generate natural, human-like answers.\n",
    "\n",
    "We‚Äôll implement RAG in two different ways:\n",
    "\n",
    "- **Using OpenAI‚Äôs model**: We‚Äôll connect our Chroma database to an OpenAI model, letting it use retrieved context to answer questions in a conversational way.\n",
    "- **Using a local Hugging Face model**: We‚Äôll replicate the same process using a local model, showing how you can achieve RAG workflows without relying on cloud APIs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-NbW97Qi-u3"
   },
   "source": [
    "# 1. Semantic Search with ChromaDB\n",
    "\n",
    "Let's jump straight into using ChromaDB to perform searches against the database we built earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDbT80nvh2j3"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F3ESZFZiict"
   },
   "source": [
    "First, we reconnect to the same on-disk database by creating a PersistentClient with the exact path we used before. This doesn‚Äôt recreate anything. It just points the new notebook at the existing Chroma files so all our collections and records are available again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGWc2SMkh_Yy"
   },
   "outputs": [],
   "source": [
    "# Reconnecting to the persistent DB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./db/chroma_persist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrmfzxF1i2PK"
   },
   "source": [
    "Next, we reopen the collection we created earlier, called \"my_documents_locally\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX8mS15biGB-"
   },
   "outputs": [],
   "source": [
    "# Reopening the collection\n",
    "local_collection = chroma_client.get_collection(\"my_documents_locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hWLQ866je3L"
   },
   "source": [
    "Now we can run a semantic search using `query()` function. When we call it, Chroma takes 2 steps:\n",
    "1. It **embeds our query text** using the same embedding function that was used for the documents in the collection.\n",
    "2. It **compares the query embedding with all stored embeddings** and then returns the most relevant results.\n",
    "\n",
    "In this example, we tell Chroma to return the top 2 most relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qdKT1j0jfai"
   },
   "outputs": [],
   "source": [
    "query_text = \"What are the foundational principles and technologies used to secure modern internet traffic?\"\n",
    "\n",
    "results = local_collection.query(\n",
    "    query_texts = [query_text],\n",
    "    n_results = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFQTr06YE0Ik"
   },
   "source": [
    "To make the output easier to read, let‚Äôs loop through the results and print a short preview of each one. The output includes **the matching documents, their IDs and the distance scores** which is a measure of similarity (the smaller the distance, the closer the match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxG0VpC3E0Il",
    "outputId": "a7b6dd0e-e3b5-42a2-aeb6-aeceabcce866"
   },
   "outputs": [],
   "source": [
    "# Displaying results in readable format\n",
    "for rank, (document, document_id, distance_score) in enumerate(\n",
    "    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    preview = document[:600]\n",
    "    print(f\"Result {rank}\")\n",
    "    print(f\"‚Ä¢ ID: {document_id}\")\n",
    "    print(f\"‚Ä¢ Distance: {distance_score:.4f}\")\n",
    "    print(f\"‚Ä¢ Preview: {preview}‚Ä¶\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 1: Try Your Own Semantic Search (5-7 minutes)\n",
    "\n",
    "**What you'll practice:** Running semantic searches and understanding how query phrasing affects results.\n",
    "\n",
    "**Your task:**\n",
    "1. Create your own query about a cybersecurity topic (e.g., \"How do hackers steal passwords?\", \"What is modern network security?\", \"How do viruses work?\")\n",
    "2. Use `local_collection.query()` to search for the top 3 most relevant documents\n",
    "3. Print the results showing the document IDs and distance scores\n",
    "4. Experiment: Try rephrasing your question differently - do you get similar results?\n",
    "\n",
    "**Hint:** Use the same structure as the example above. Remember to set `n_results=3` to get 3 documents.\n",
    "\n",
    "**Expected outcome:** You should see that semantically similar questions return similar documents, even if worded differently. Lower distance scores mean better matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# my_query = \"Your question here\"\n",
    "# results = local_collection.query(\n",
    "#     query_texts=[my_query],\n",
    "#     n_results=3\n",
    "# )\n",
    "# \n",
    "# for rank, (doc, doc_id, distance) in enumerate(\n",
    "#     zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n",
    "#     start=1):\n",
    "#     print(f\"Result {rank}: {doc_id}, Distance: {distance:.4f}\")\n",
    "#     print(f\"Preview: {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P60ghuSFkF1F"
   },
   "source": [
    "# 2. RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "We‚Äôll keep doing the same search, but now we‚Äôll hand the retrieved text to an LLM and have it compose an answer while staying grounded in our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M35L8wXbkaZf"
   },
   "source": [
    "## 2.1 OpenAI API\n",
    "\n",
    "We‚Äôll access LLM through the OpenAI API. To do this, we first load the API key from the environment and then create a client object that we will use to send requests to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print('‚úÖ API key loaded from Colab secrets')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from getpass import getpass\n",
    "        print('üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY')\n",
    "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
    "    except Exception as exc:\n",
    "        raise ValueError('‚ùå ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
    "    raise ValueError('‚ùå ERROR: No API key provided!')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print('‚úÖ Authentication configured!')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
    "print(f'ü§ñ Selected Model: {OPENAI_MODEL}')\n",
    "\n",
    "OPENAI_EMBED_MODEL = 'text-embedding-3-small'\n",
    "print(f'üß† Embedding Model: {OPENAI_EMBED_MODEL}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqlWxwLUE0In"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJk-NZs7E0In"
   },
   "outputs": [],
   "source": [
    "# Loading API key from the environment\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Creating a client that we'll use to call the API\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsj2CHTFE0Io"
   },
   "source": [
    "We will query Chroma collection to get the most relevant passages for the question. Here we ask for the top 3 results, and we also include their IDs. These IDs help us later show where the information came from. If no results are found, we stop early and tell the user that nothing matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVbG6mMzE0Io"
   },
   "outputs": [],
   "source": [
    "query_text = \"What is the difference between symmetric and asymmetric encryption?\"\n",
    "\n",
    "# Step 1: Retrieving relevant passages from Chroma\n",
    "response = local_collection.query(\n",
    "    query_texts = [query_text],\n",
    "    n_results = 3,\n",
    "    include = [\"documents\"]\n",
    ")\n",
    "\n",
    "docs = response[\"documents\"][0]\n",
    "ids = response[\"ids\"][0]\n",
    "\n",
    "if not docs:\n",
    "    print(\"No relevant passages found in the vector store. Try rephrasing the question.\")\n",
    "else:\n",
    "    # Step 2: Packing retrieved results into a labeled context block to make the input clearer for the LLM\n",
    "    context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n",
    "    context = \"\\n\\n\".join(context_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-UNCV_qE0Io"
   },
   "source": [
    "Finally, we'll send retrieved passages along with the question to the OpenAI model `gpt-5-nano`. The `system` message is an instruction that controls how the model should behave. The `user` message then supplies both the sources and the question, so the model has all the context it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EXERCISE 2: Build Your Own RAG Query with OpenAI (10-12 minutes)\n",
    "\n",
    "**What you'll practice:** Creating a complete RAG pipeline from retrieval to answer generation.\n",
    "\n",
    "**Your task:**\n",
    "1. Think of a new question about the cybersecurity documents (e.g., \"What is Stuxnet?\", \"How does Zero Trust work?\", \"What makes phishing attacks successful?\")\n",
    "2. Retrieve the top 2 relevant documents from `local_collection`\n",
    "3. Create a context block from the retrieved documents\n",
    "4. Send the context and your question to OpenAI's API to generate an answer\n",
    "5. Print the final answer\n",
    "\n",
    "**Hint:** Follow the same pattern as the example above. You'll need to:\n",
    "- Use `local_collection.query()` to retrieve documents\n",
    "- Format the context with document IDs\n",
    "- Use `client.chat.completions.create()` to generate the answer\n",
    "\n",
    "**Expected outcome:** You should get a clear, cited answer that references specific source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example solution structure:\n",
    "# \n",
    "# my_question = \"Your question here\"\n",
    "# \n",
    "# # Step 1: Retrieve\n",
    "# response = local_collection.query(\n",
    "#     query_texts=[my_question],\n",
    "#     n_results=2,\n",
    "#     include=[\"documents\"]\n",
    "# )\n",
    "# \n",
    "# docs = response[\"documents\"][0]\n",
    "# ids = response[\"ids\"][0]\n",
    "# \n",
    "# # Step 2: Format context\n",
    "# context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n",
    "# context = \"\\n\\n\".join(context_blocks)\n",
    "# \n",
    "# # Step 3: Generate answer\n",
    "# resp = client.chat.completions.create(\n",
    "#     model=OPENAI_MODEL,\n",
    "#     temperature=0,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a precise assistant. Answer using only the provided sources and cite them.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {my_question}\\n\\nANSWER:\"}\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvRstf_qE0Io",
    "outputId": "157d5ef3-7c86-46b3-936f-fa5fdd0d052e"
   },
   "outputs": [],
   "source": [
    "# Step 3: Generating an answer using the retrieved context\n",
    "resp = client.chat.completions.create(\n",
    "    model = OPENAI_MODEL,\n",
    "    temperature = 0,\n",
    "    max_tokens = 500,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a precise assistant that must answer ONLY using the provided sources.\"\n",
    "                \"If the answer is not fully supported by the sources, explain that the information is not in the provided material and suggest how the user might rephrase their question.\"\n",
    "                \"At the end, cite sources by their bracket labels, e.g., [1], [2].\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {query_text}\\n\\nANSWER:\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNjIHs2IkWjY"
   },
   "source": [
    "## 2.2 Local Model from Hugging Face\n",
    "\n",
    "In the previous section, we combined local retrieval with ChromaDB (embeddings stored on disk) and then sent the retrieved passages to OpenAI‚Äôs API for generation. That worked well but it also meant our data had to leave our computer and be processed by an external service.\n",
    "\n",
    "What if we don‚Äôt want to send private documents outside our environment? In this section we‚Äôll keep **everything on-device by running a model locally**. We'll use a pretrained instruction-tuned model from HuggingFace called `Qwen/Qwen2.5-1.5B-Instruct`.\n",
    "\n",
    "This model is small enough to run on a laptop or a single GPU, yet powerful enough to follow instructions and generate answers. With it, both retrieval and generation happen locally:\n",
    "- our data never leave our computer\n",
    "- we avoid per-request API costs\n",
    "- we reduce reliance on external services and avoid latency from network calls\n",
    "\n",
    "You can read more about this model on [HuggingFace website](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct).\n",
    "\n",
    "We start by importing the Hugging Face `transformers` library, which gives us the tools to load and run local language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi41rZTSE0Io"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egn5A1nOE0Io"
   },
   "source": [
    "We then load two key components:\n",
    "- **Tokenizer**: This converts our input text (the context and the question) into numerical tokens the model can understand and process. Later, the tokenizer also converts the model‚Äôs output tokens back into human-readable text.\n",
    "- **Model**: This is the actual neural network with all its trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqogdfnvE0Io",
    "outputId": "603c42b7-ab2a-48d3-f2b0-de2b55e48de3"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Pulling down the model weights and config\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",       # place the model on GPU if available, CPU otherwise\n",
    "    torch_dtype = \"float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZNjw4LGE0Io"
   },
   "source": [
    "Now we'll create custom function tht ties everything together. First, it opens the Chroma collection and retrieves the top-k most relevant documents for the given question. Those documents are combined into a prompt which instructs the model to answer only using the retrieved context. The prompt is then tokenized and passed to the language model for generation. The model produces new tokens, which are decoded back into text to form the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBvIjL2sE0Ip"
   },
   "outputs": [],
   "source": [
    "def get_answer(client, collection_name: str, question: str, k: int = 2, max_new_tokens: int = 120):\n",
    "    # Opening the collection\n",
    "    col = client.get_collection(collection_name)\n",
    "\n",
    "    # Retrieving top-k documents\n",
    "    res = col.query(query_texts=[question], n_results=k, include=[\"documents\"])\n",
    "    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n",
    "    if not docs:\n",
    "        return \"Insufficient context.\"\n",
    "\n",
    "    # The prompt\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant.\\n\"\n",
    "        \"Answer ONLY using the CONTEXT below.\"\n",
    "        \"If the CONTEXT is insufficient, reply exactly: Insufficient context.\\n\"\n",
    "        \"Keep the answer short (1‚Äì2 sentences).\\n\\n\"\n",
    "        \"----- CONTEXT -----\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"-------------------\\n\"\n",
    "        f\"QUESTION: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Tokenizing and generating\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "    outputs = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decoding only the newly generated tokens\n",
    "    answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwqhgkIsE0Ip",
    "outputId": "17665786-fbdd-46e2-d715-b6b80c4855df"
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between symmetric and asymmetric encryption?\"\n",
    "\n",
    "# Calling the RAG function\n",
    "answer = get_answer(\n",
    "    client = chroma_client,\n",
    "    collection_name = \"my_documents_locally\",\n",
    "    question = question,\n",
    "    k = 2\n",
    ")\n",
    "\n",
    "# Displaying the output\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
