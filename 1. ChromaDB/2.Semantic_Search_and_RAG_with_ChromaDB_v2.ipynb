{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "FWKdhJLn7c3P"
   },
   "source": [
    "# Setup: Installing Required Libraries\n",
    "\n",
    "Before we begin, we need to install the necessary Python libraries. Run the cell below to install all dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb-qDUVv7c3R",
    "outputId": "f4b23a9c-9895-4447-d2ac-5766aa86a6c2"
   },
   "outputs": [],
   "source": [
    "# Install required libraries - CONSISTENT VERSIONS\n",
    "# Both notebooks use: chromadb 0.5.3 + openai 0.28.1\n",
    "!pip install -q chromadb==0.5.3 openai==0.28.1 sentence-transformers==3.0.1 transformers torch\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Please restart your kernel/runtime now before running the next cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Installation Note\n",
    "\n",
    "If you see **dependency conflict warnings** during installation, you can safely **ignore them** - they won't affect this notebook.\n",
    "\n",
    "**Remember:** Always restart your runtime after installation! (Runtime ‚Üí Restart runtime)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "73bBj5bt7c3S",
    "outputId": "dfbacafd-3609-4cdb-c4b1-bb41e76d358c"
   },
   "outputs": [],
   "source": [
    "# Upload and extract the database from Notebook 1\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì§ Please upload the chromadb_database.zip file...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if 'chromadb_database.zip' in uploaded:\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile('chromadb_database.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('./uploaded_db')\n",
    "        print(\"‚úÖ Database extracted successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå chromadb_database.zip not found. Please upload it.\")\n",
    "        \n",
    "except ImportError:\n",
    "    # Not in Colab - check if zip exists locally\n",
    "    if os.path.exists('chromadb_database.zip'):\n",
    "        with zipfile.ZipFile('chromadb_database.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('./uploaded_db')\n",
    "        print(\"‚úÖ Database extracted from local zip file!\")\n",
    "    else:\n",
    "        print(\"‚ùå chromadb_database.zip not found. Please place it in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "C9-DmosFE0Ig"
   },
   "source": [
    "## Verify Database Connection\n",
    "\n",
    "Now let's verify that the collection from Notebook 1 was loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "mDbT80nvh2j3"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrmfzxF1i2PK",
    "outputId": "1d5be57d-0d6d-4413-f71d-5199de50d2b4"
   },
   "outputs": [],
   "source": [
    "# Reconnecting to the persistent DB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./uploaded_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX8mS15biGB-",
    "outputId": "2823cd8c-a1bf-4484-feb4-2f2a37f6d9d8"
   },
   "outputs": [],
   "source": [
    "# Reopening the collection and verifying it has data\n",
    "# This cell will check if the collection exists and contains the expected documents from notebook 1\n",
    "\n",
    "try:\n",
    "    local_collection = chroma_client.get_collection(\"my_documents_locally\")\n",
    "    count = local_collection.count()\n",
    "    print(f\"‚úÖ Successfully connected to collection 'my_documents_locally'\")\n",
    "    print(f\"   Documents in collection: {count}\")\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Collection exists but contains no documents!\")\n",
    "        print(\"   Please run the first notebook (1.Creating_Embeddings_using_Chroma.ipynb) completely.\")\n",
    "    elif count < 6:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Expected 6 documents but found {count}\")\n",
    "        print(\"   Please re-run the first notebook to ensure all documents are added.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"‚ùå ERROR: Collection 'my_documents_locally' does not exist!\")\n",
    "    print(\"\\nüìù SOLUTION:\")\n",
    "    print(\"   1. Go back to the first notebook: '1.Creating_Embeddings_using_Chroma.ipynb'\")\n",
    "    print(\"   2. Run ALL cells in that notebook to create and populate the collection\")\n",
    "    print(\"   3. Then return to this notebook\")\n",
    "    print(f\"\\nTechnical details: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "3hWLQ866je3L"
   },
   "source": [
    "Now we can run a semantic search using `query()` function. When we call it, Chroma takes 2 steps:\n",
    "1. It **embeds our query text** using the same embedding function that was used for the documents in the collection.\n",
    "2. It **compares the query embedding with all stored embeddings** and then returns the most relevant results.\n",
    "\n",
    "In this example, we tell Chroma to return the top 2 most relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qdKT1j0jfai",
    "outputId": "d7763b6f-2e77-4529-a085-6b6de1c335a8"
   },
   "outputs": [],
   "source": [
    "query_text = \"What are the foundational principles and technologies used to secure modern internet traffic?\"\n",
    "\n",
    "results = local_collection.query(\n",
    "    query_texts = [query_text],\n",
    "    n_results = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "GFQTr06YE0Ik"
   },
   "source": [
    "To make the output easier to read, let‚Äôs loop through the results and print a short preview of each one. The output includes **the matching documents, their IDs and the distance scores** which is a measure of similarity (the smaller the distance, the closer the match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxG0VpC3E0Il",
    "outputId": "c1857ba1-c58b-4172-84b7-cfecffa99a91"
   },
   "outputs": [],
   "source": [
    "# Displaying results in readable format\n",
    "for rank, (document, document_id, distance_score) in enumerate(\n",
    "    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    preview = document[:600]\n",
    "    print(f\"Result {rank}\")\n",
    "    print(f\"‚Ä¢ ID: {document_id}\")\n",
    "    print(f\"‚Ä¢ Distance: {distance_score:.4f}\")\n",
    "    print(f\"‚Ä¢ Preview: {preview}‚Ä¶\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "wWhjb4Hr7c3U"
   },
   "source": [
    "## 1.1 Filtering Semantic Search with Metadata\n",
    "\n",
    "Semantic search is powerful, but sometimes you want to **combine semantic similarity with specific filters**. For example, you might want to find documents about \"encryption\" but only from the \"cryptography\" category, or only beginner-level documents.\n",
    "\n",
    "ChromaDB allows you to add a `where` clause to your queries, just like you learned in the first notebook. This combines the best of both worlds:\n",
    "- **Semantic search** finds conceptually relevant documents\n",
    "- **Metadata filters** narrow down results to specific criteria\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "HwqEd0_R7c3V"
   },
   "source": [
    "### Example 1: Search with Category Filter\n",
    "\n",
    "Let's search for documents about \"network protection\" but only from the \"architecture\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ja631fv7c3V",
    "outputId": "26d9033d-2c6b-4707-a58e-321601c4d688"
   },
   "outputs": [],
   "source": [
    "query_text = \"How to protect networks from attackers?\"\n",
    "\n",
    "# Semantic search with category filter\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=2,\n",
    "    where={\"category\": \"architecture\"},  # Only architecture documents\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: category = 'architecture'\\n\")\n",
    "\n",
    "for rank, (doc, doc_id, metadata, distance) in enumerate(\n",
    "    zip(results[\"documents\"][0], results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    print(f\"Result {rank}:\")\n",
    "    print(f\"  ID: {doc_id}\")\n",
    "    print(f\"  Category: {metadata['category']}\")\n",
    "    print(f\"  Distance: {distance:.4f}\")\n",
    "    print(f\"  Preview: {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "f_2AybmG7c3V"
   },
   "source": [
    "### Example 2: Search with Difficulty Filter\n",
    "\n",
    "Now let's find beginner-friendly documents about authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpHCaT_P7c3V",
    "outputId": "1ceec8ab-6b6c-4291-e640-4082d5c29089"
   },
   "outputs": [],
   "source": [
    "query_text = \"How do authentication systems work?\"\n",
    "\n",
    "# Semantic search with difficulty filter\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3,\n",
    "    where={\"difficulty\": \"beginner\"},  # Only beginner-level documents\n",
    "    include=[\"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: difficulty = 'beginner'\\n\")\n",
    "\n",
    "if results[\"ids\"][0]:\n",
    "    for rank, (doc_id, metadata, distance) in enumerate(\n",
    "        zip(results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "        start=1):\n",
    "        print(f\"Result {rank}: {doc_id}\")\n",
    "        print(f\"  Category: {metadata['category']}, Difficulty: {metadata['difficulty']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\\n\")\n",
    "else:\n",
    "    print(\"No beginner-level documents found matching this query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "AgvdHyYm7c3W"
   },
   "source": [
    "### Example 3: Complex Filters with Operators\n",
    "\n",
    "You can also use operators like `$gte` (greater than or equal), `$in` (in list), etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af2H9fFA7c3W",
    "outputId": "07fdf5d2-463f-4e10-c6b5-f48424650f55"
   },
   "outputs": [],
   "source": [
    "query_text = \"Modern security approaches\"\n",
    "\n",
    "# Find documents from 2010 or later\n",
    "results = local_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3,\n",
    "    where={\"year\": {\"$gte\": 2010}},  # Documents from 2010 onwards\n",
    "    include=[\"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(f\"Filter: year >= 2010\\n\")\n",
    "\n",
    "for rank, (doc_id, metadata, distance) in enumerate(\n",
    "    zip(results[\"ids\"][0], results[\"metadatas\"][0], results[\"distances\"][0]),\n",
    "    start=1):\n",
    "    print(f\"Result {rank}: {doc_id}\")\n",
    "    print(f\"  Category: {metadata['category']}, Year: {metadata['year']}\")\n",
    "    print(f\"  Distance: {distance:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "xEccG7DI7c3W"
   },
   "source": [
    "**Key Takeaway:** Filtered semantic search is incredibly powerful. It lets you:\n",
    "- Find semantically relevant content (\"what does it mean?\")\n",
    "- While applying specific business logic (\"show me only X type of documents\")\n",
    "\n",
    "This is essential for building production RAG systems where you need to control which documents can be retrieved based on user permissions, document types, dates, or other criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "kuJhtvFm7c3W"
   },
   "source": [
    "### üìù EXERCISE 1: Try Your Own Semantic Search (5-7 minutes)\n",
    "\n",
    "**What you'll practice:** Running semantic searches and understanding how query phrasing affects results.\n",
    "\n",
    "**Your task:**\n",
    "1. Create your own query about a cybersecurity topic (e.g., \"How do hackers steal passwords?\", \"What is modern network security?\", \"How do viruses work?\")\n",
    "2. Use `local_collection.query()` to search for the top 3 most relevant documents\n",
    "3. Print the results showing the document IDs and distance scores\n",
    "4. Experiment: Try rephrasing your question differently - do you get similar results?\n",
    "\n",
    "**Hint:** Use the same structure as the example above. Remember to set `n_results=3` to get 3 documents.\n",
    "\n",
    "**Expected outcome:** You should see that semantically similar questions return similar documents, even if worded differently. Lower distance scores mean better matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "UAEbHNlJ7c3W"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "pc2CfDbs7c3W"
   },
   "source": [
    "## 1.2 Optimizing n_results: How Many Documents Should You Retrieve?\n",
    "\n",
    "When performing semantic search for RAG, one critical parameter is `n_results` - how many documents to retrieve. This isn't just a technical detail; it's a key design decision that affects your system's quality, cost, and performance.\n",
    "\n",
    "### The Trade-offs\n",
    "\n",
    "**Too Few Documents (n_results = 1-2):**\n",
    "- ‚úÖ **Pros:**\n",
    "  - Faster retrieval\n",
    "  - Lower LLM cost (less context to process)\n",
    "  - More focused answers\n",
    "- ‚ùå **Cons:**\n",
    "  - **Low recall**: Might miss relevant information spread across multiple documents\n",
    "  - Risk of incomplete answers\n",
    "  - Single point of failure if top result isn't perfect\n",
    "\n",
    "**Too Many Documents (n_results = 10+):**\n",
    "- ‚úÖ **Pros:**\n",
    "  - **High recall**: More likely to capture all relevant information\n",
    "  - Better coverage of the topic\n",
    "- ‚ùå **Cons:**\n",
    "  - **Context size explosion**: LLMs have token limits (e.g., GPT-4: 8k-128k tokens)\n",
    "  - Higher costs (more tokens = more $$$)\n",
    "  - **Noise**: Irrelevant documents can confuse the LLM\n",
    "  - Slower processing\n",
    "  - LLM may struggle to synthesize information from too many sources\n",
    "\n",
    "### Finding the Sweet Spot\n",
    "\n",
    "Let's experiment with different values of `n_results` to see how it affects our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5Iergs97c3W",
    "outputId": "f423788a-723d-4978-8272-7aefd99e7925"
   },
   "outputs": [],
   "source": [
    "query_text = \"What are the main security threats?\"\n",
    "\n",
    "print(f\"Query: {query_text}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test different n_results values\n",
    "for n in [1, 3, 5]:\n",
    "    results = local_collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n,\n",
    "        include=[\"distances\", \"documents\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nn_results = {n}:\")\n",
    "    print(f\"  Document IDs: {results['ids'][0]}\")\n",
    "    print(f\"  Distance scores: {[f'{d:.4f}' for d in results['distances'][0]]}\")\n",
    "\n",
    "    # Calculate approximate token count (rough estimate: 1 token ‚âà 4 characters)\n",
    "    total_chars = sum(len(doc) for doc in results[\"documents\"][0])\n",
    "    approx_tokens = total_chars // 4\n",
    "    print(f\"  Approx context tokens: ~{approx_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "YPUyUhxl7c3X"
   },
   "source": [
    "### Practical Guidelines\n",
    "\n",
    "**1. Start with n_results = 3-5** for most use cases:\n",
    "   - Good balance between recall and context size\n",
    "   - Works well with most LLMs' context windows\n",
    "   - Manageable cost\n",
    "\n",
    "**2. Adjust based on your documents:**\n",
    "   - **Short documents** (tweets, Q&A pairs): Can use higher n_results (10-20)\n",
    "   - **Long documents** (articles, papers): Use lower n_results (2-5)\n",
    "   - **Chunked documents**: Consider 5-10 chunks\n",
    "\n",
    "**3. Consider your LLM's context window:**\n",
    "   - GPT-3.5-turbo: 4k tokens ‚Üí Keep context under 3k\n",
    "   - GPT-4: 8k-32k tokens ‚Üí More flexibility\n",
    "   - Local models: Often 2k-4k ‚Üí Be conservative\n",
    "\n",
    "**4. Monitor distance scores:**\n",
    "   - If the 3rd result has a distance > 0.5, the remaining results probably won't help\n",
    "   - Use a **distance threshold** instead of fixed n_results\n",
    "\n",
    "**5. Advanced technique - Reranking:**\n",
    "   - Retrieve more documents (n=10-20)\n",
    "   - Use a reranking model to select the best 3-5\n",
    "   - Send only the reranked results to the LLM\n",
    "\n",
    "### Example: Dynamic n_results with Distance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdNMSA0o7c3X",
    "outputId": "27560052-2fca-4959-dedc-dfb0116f8293"
   },
   "outputs": [],
   "source": [
    "def smart_retrieve(collection, query, max_results=5, distance_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Retrieve documents but stop if distance scores get too high (low similarity)\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=max_results,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Filter out documents with distance > threshold\n",
    "    filtered_docs = []\n",
    "    filtered_ids = []\n",
    "\n",
    "    for doc, doc_id, dist in zip(\n",
    "        results[\"documents\"][0],\n",
    "        results[\"ids\"][0],\n",
    "        results[\"distances\"][0]\n",
    "    ):\n",
    "        if dist <= distance_threshold:\n",
    "            filtered_docs.append(doc)\n",
    "            filtered_ids.append(doc_id)\n",
    "        else:\n",
    "            print(f\"  Skipping {doc_id} (distance {dist:.4f} > threshold {distance_threshold})\")\n",
    "\n",
    "    return filtered_docs, filtered_ids\n",
    "\n",
    "# Test it\n",
    "query = \"How does public key cryptography work?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "docs, ids = smart_retrieve(local_collection, query, max_results=5, distance_threshold=0.5)\n",
    "print(f\"\\nRetrieved {len(docs)} high-quality documents: {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "72G_-HW57c3X"
   },
   "source": [
    "**Key Takeaway:**\n",
    "\n",
    "Don't just blindly set `n_results=5`. Think about:\n",
    "- Your document length and structure\n",
    "- Your LLM's context window\n",
    "- Your quality vs. cost trade-offs\n",
    "- The semantic distance of retrieved results\n",
    "\n",
    "For most applications, **n_results=3** with a **distance threshold of 0.5-0.6** is a good starting point. Then tune based on your specific needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "P60ghuSFkF1F"
   },
   "source": [
    "# 2. RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "We‚Äôll keep doing the same search, but now we‚Äôll hand the retrieved text to an LLM and have it compose an answer while staying grounded in our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "M35L8wXbkaZf"
   },
   "source": [
    "## 2.1 OpenAI API\n",
    "\n",
    "We‚Äôll access LLM through the OpenAI API. To do this, we first load the API key from the environment and then create a client object that we will use to send requests to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13",
    "outputId": "ffe8e309-d2c1-4e2f-d228-9d1c61225261"
   },
   "outputs": [],
   "source": [
    "import os\nfrom getpass import getpass\n\n# Configure OpenAI API key\ntry:\n    from google.colab import userdata\n    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n    print(\"‚úÖ API key loaded from Colab secrets\")\nexcept:\n    from getpass import getpass\n    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\nif not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n\nprint(\"\\n‚úÖ OpenAI API configured!\")\n\n# Set the model to use\nOPENAI_MODEL = \"gpt-3.5-turbo\"\nprint(f\"ü§ñ Using model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "gqlWxwLUE0In"
   },
   "outputs": [],
   "source": [
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "FJk-NZs7E0In"
   },
   "outputs": [],
   "source": [
    "# Configure OpenAI for old API (0.28.1)\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "xsj2CHTFE0Io"
   },
   "source": [
    "We will query Chroma collection to get the most relevant passages for the question. Here we ask for the top 3 results, and we also include their IDs. These IDs help us later show where the information came from. If no results are found, we stop early and tell the user that nothing matched."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "cVbG6mMzE0Io"
   },
   "outputs": [],
   "source": "Finally, we'll send retrieved passages along with the question to the OpenAI model `gpt-3.5-turbo`. The `system` message is an instruction that controls how the model should behave. The `user` message then supplies both the sources and the question, so the model has all the context it needs."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "Tc8T7s447c3Y"
   },
   "outputs": [],
   "source": "### üìù EXERCISE 2: Build Your Own RAG Query with OpenAI (10-12 minutes)\n\n**What you'll practice:** Creating a complete RAG pipeline from retrieval to answer generation.\n\n**Your task:**\n1. Think of a new question about the cybersecurity documents (e.g., \"What is Stuxnet?\", \"How does Zero Trust work?\", \"What makes phishing attacks successful?\")\n2. Retrieve the top 2 relevant documents from `local_collection`\n3. Create a context block from the retrieved documents\n4. Send the context and your question to OpenAI's API to generate an answer\n5. Print the final answer\n\n**Hint:** Follow the same pattern as the example above. You'll need to:\n- Use `local_collection.query()` to retrieve documents\n- Format the context with document IDs\n- Use `openai.ChatCompletion.create()` with messages in the format: `[{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}]`\n\n**Expected outcome:** You should get a clear, cited answer that references specific source documents."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "fvRstf_qE0Io",
    "outputId": "2b840796-c9cc-43fe-ed4f-115d6010bb17"
   },
   "outputs": [],
   "source": "# Step 3: Generating an answer using the retrieved context\nresponse = openai.ChatCompletion.create(\n    model=OPENAI_MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are a precise assistant that must answer ONLY using the provided sources. \"\n                \"If the answer is not fully supported by the sources, explain that the information is not in the provided material and suggest how the user might rephrase their question. \"\n                \"At the end, cite sources by their bracket labels, e.g., [1], [2].\"\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {query_text}\\n\\nANSWER:\"\n        }\n    ],\n    temperature=0\n)\n\nprint(response['choices'][0]['message']['content'])"
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "P-UNCV_qE0Io"
   },
   "source": [
    "Finally, we'll send retrieved passages along with the question to the OpenAI model `gpt-5-nano`. The `system` message is an instruction that controls how the model should behave. The `user` message then supplies both the sources and the question, so the model has all the context it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "EEcoZmst7c3Y"
   },
   "source": [
    "### üìù EXERCISE 2: Build Your Own RAG Query with OpenAI (10-12 minutes)\n",
    "\n",
    "**What you'll practice:** Creating a complete RAG pipeline from retrieval to answer generation.\n",
    "\n",
    "**Your task:**\n",
    "1. Think of a new question about the cybersecurity documents (e.g., \"What is Stuxnet?\", \"How does Zero Trust work?\", \"What makes phishing attacks successful?\")\n",
    "2. Retrieve the top 2 relevant documents from `local_collection`\n",
    "3. Create a context block from the retrieved documents\n",
    "4. Send the context and your question to OpenAI's API to generate an answer\n",
    "5. Print the final answer\n",
    "\n",
    "**Hint:** Follow the same pattern as the example above. You'll need to:\n",
    "- Use `local_collection.query()` to retrieve documents\n",
    "- Format the context with document IDs\n",
    "- Use `client.chat.completions.create()` to generate the answer\n",
    "\n",
    "**Expected outcome:** You should get a clear, cited answer that references specific source documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "_QWnOZ4H7c3Y"
   },
   "source": [
    "## 2.1.1 RAG Evaluation and Source Attribution\n",
    "\n",
    "So far, we've built a RAG pipeline that retrieves documents and generates answers. But there's a critical question we haven't addressed: **How do we know if the LLM actually used the retrieved sources, or if it just made up an answer?**\n",
    "\n",
    "This is one of the biggest challenges in production RAG systems:\n",
    "\n",
    "1. **No verification that sources are actually used**: After getting an LLM answer, there's often no check whether the answer actually came from the retrieved context\n",
    "2. **Hallucinations**: LLMs can confidently generate information that wasn't in the sources at all\n",
    "3. **Source attribution**: Even when the answer is correct, it's hard to verify which specific source documents were actually used\n",
    "\n",
    "Let's explore techniques to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "AqUpp3ko7c3Y"
   },
   "source": [
    "### Technique 1: Explicit Citation Requirements\n",
    "\n",
    "The simplest approach is to **force the LLM to cite its sources** in the system prompt. Let's compare answers with and without citation requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "XgjlTJde7c3Y"
   },
   "outputs": [],
   "source": "# Example WITHOUT citation requirement\nquery_text = \"What is quantum computing?\"\n\nresponse = local_collection.query(\n    query_texts=[query_text],\n    n_results=2,\n    include=[\"documents\"]\n)\n\ndocs = response[\"documents\"][0]\nids = response[\"ids\"][0]\n\nif docs:\n    context_blocks = [f\"[{i+1} | {ids[i]}]\\n{docs[i]}\" for i in range(len(docs))]\n    context = \"\\n\\n\".join(context_blocks)\n    \n    # Weak system prompt - no citation requirement\n    resp_no_citation = openai.ChatCompletion.create(\n        model=OPENAI_MODEL,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Answer the question using the provided sources.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {query_text}\"\n            }\n        ]\n    )\n    \n    print(\"‚ùå WITHOUT Citation Requirement:\")\n    print(resp_no_citation['choices'][0]['message']['content'])\n    print(\"\\n\" + \"=\"*80 + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "r-rsWN657c3Z"
   },
   "outputs": [],
   "source": "    # Now WITH strong citation requirement\n    resp_with_citation = openai.ChatCompletion.create(\n        model=OPENAI_MODEL,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a precise assistant. Answer ONLY using the provided sources. \"\n                    \"You MUST cite every claim with [1], [2], etc. \"\n                    \"If the answer is not in the sources, say 'The provided sources do not contain this information.'\"\n                )\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {query_text}\"\n            }\n        ]\n    )\n    \n    print(\"‚úÖ WITH Citation Requirement:\")\n    print(resp_with_citation['choices'][0]['message']['content'])"
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "WZ5EuQr87c3Z"
   },
   "source": [
    "**Key Observation:** Without explicit citation requirements, the LLM may provide an answer that sounds plausible but isn't grounded in the sources. With citations, we can trace every claim back to a specific document.\n",
    "\n",
    "### Technique 2: Programmatic Source Verification\n",
    "\n",
    "A more robust approach is to **automatically verify** if the LLM's answer actually uses content from the retrieved sources. We can do this by checking for text overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "HYV7Pp6R7c3Z"
   },
   "outputs": [],
   "source": "def verify_source_usage(answer, source_docs, min_overlap_words=5):\n    \"\"\"\n    Check if the LLM answer contains content from the source documents.\n    Returns a dict with verification metrics.\n    \"\"\"\n    answer_lower = answer.lower()\n    answer_words = set(answer_lower.split())\n    \n    verification = {\n        \"uses_sources\": False,\n        \"source_overlaps\": [],\n        \"total_overlap_words\": 0\n    }\n    \n    for i, doc in enumerate(source_docs):\n        doc_lower = doc.lower()\n        doc_words = set(doc_lower.split())\n        \n        # Find common words (excluding very short words)\n        overlap = answer_words & doc_words\n        meaningful_overlap = {w for w in overlap if len(w) > 3}\n        \n        if len(meaningful_overlap) >= min_overlap_words:\n            verification[\"uses_sources\"] = True\n            verification[\"source_overlaps\"].append({\n                \"source_index\": i + 1,\n                \"overlap_count\": len(meaningful_overlap),\n                \"sample_words\": list(meaningful_overlap)[:10]\n            })\n            verification[\"total_overlap_words\"] += len(meaningful_overlap)\n    \n    return verification\n\n# Test it with a real RAG query\ntest_query = \"What are encryption algorithms?\"\nresponse = local_collection.query(\n    query_texts=[test_query],\n    n_results=2,\n    include=[\"documents\"]\n)\n\ntest_docs = response[\"documents\"][0]\ntest_ids = response[\"ids\"][0]\n\nif test_docs:\n    context_blocks = [f\"[{i+1} | {test_ids[i]}]\\n{test_docs[i]}\" for i in range(len(test_docs))]\n    context = \"\\n\\n\".join(context_blocks)\n    \n    # Get LLM answer using ChatCompletion API\n    test_resp = openai.ChatCompletion.create(\n        model=OPENAI_MODEL,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Answer using the provided sources.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"SOURCES:\\n{context}\\n\\nQUESTION: {test_query}\"\n            }\n        ]\n    )\n    \n    answer = test_resp['choices'][0]['message']['content']\n    \n    # Verify source usage\n    verification = verify_source_usage(answer, test_docs)\n    \n    print(f\"Query: {test_query}\\n\")\n    print(f\"Answer: {answer}\\n\")\n    print(\"=\"*80)\n    print(f\"\\n‚úì Verification Results:\")\n    print(f\"  Uses sources: {verification['uses_sources']}\")\n    print(f\"  Total word overlap: {verification['total_overlap_words']}\")\n    for overlap_info in verification[\"source_overlaps\"]:\n        print(f\"  Source [{overlap_info['source_index']}]: {overlap_info['overlap_count']} overlapping words\")"
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "z8gjskJh7c3Z"
   },
   "source": [
    "**Key Takeaway - RAG Evaluation:**\n",
    "\n",
    "In production RAG systems, you should:\n",
    "\n",
    "1. **Enforce citations** in system prompts - make the LLM cite sources with [1], [2], etc.\n",
    "2. **Verify programmatically** - use overlap analysis or semantic similarity to check if answers use the sources\n",
    "3. **Monitor hallucinations** - if word overlap is too low, the LLM may be inventing information\n",
    "4. **Build feedback loops** - log cases where verification fails for continuous improvement\n",
    "\n",
    "These techniques help ensure your RAG system stays grounded in facts and doesn't hallucinate information that isn't in your knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "JNjIHs2IkWjY"
   },
   "source": [
    "## 2.2 Local Model from Hugging Face\n",
    "\n",
    "In the previous section, we combined local retrieval with ChromaDB (embeddings stored on disk) and then sent the retrieved passages to OpenAI‚Äôs API for generation. That worked well but it also meant our data had to leave our computer and be processed by an external service.\n",
    "\n",
    "What if we don‚Äôt want to send private documents outside our environment? In this section we‚Äôll keep **everything on-device by running a model locally**. We'll use a pretrained instruction-tuned model from HuggingFace called `Qwen/Qwen2.5-1.5B-Instruct`.\n",
    "\n",
    "This model is small enough to run on a laptop or a single GPU, yet powerful enough to follow instructions and generate answers. With it, both retrieval and generation happen locally:\n",
    "- our data never leave our computer\n",
    "- we avoid per-request API costs\n",
    "- we reduce reliance on external services and avoid latency from network calls\n",
    "\n",
    "You can read more about this model on [HuggingFace website](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct).\n",
    "\n",
    "We start by importing the Hugging Face `transformers` library, which gives us the tools to load and run local language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "mi41rZTSE0Io"
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "egn5A1nOE0Io"
   },
   "source": [
    "We then load two key components:\n",
    "- **Tokenizer**: This converts our input text (the context and the question) into numerical tokens the model can understand and process. Later, the tokenizer also converts the model‚Äôs output tokens back into human-readable text.\n",
    "- **Model**: This is the actual neural network with all its trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "id": "BqogdfnvE0Io"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Pulling down the model weights and config\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",       # place the model on GPU if available, CPU otherwise\n",
    "    torch_dtype = \"float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "6ZNjw4LGE0Io"
   },
   "source": [
    "Now we'll create custom function tht ties everything together. First, it opens the Chroma collection and retrieves the top-k most relevant documents for the given question. Those documents are combined into a prompt which instructs the model to answer only using the retrieved context. The prompt is then tokenized and passed to the language model for generation. The model produces new tokens, which are decoded back into text to form the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "zBvIjL2sE0Ip"
   },
   "outputs": [],
   "source": [
    "def get_answer(client, collection_name: str, question: str, k: int = 2, max_new_tokens: int = 120):\n",
    "    # Opening the collection\n",
    "    col = client.get_collection(collection_name)\n",
    "\n",
    "    # Retrieving top-k documents\n",
    "    res = col.query(query_texts=[question], n_results=k, include=[\"documents\"])\n",
    "    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n",
    "    if not docs:\n",
    "        return \"Insufficient context.\"\n",
    "\n",
    "    # The prompt\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant.\\n\"\n",
    "        \"Answer ONLY using the CONTEXT below.\"\n",
    "        \"If the CONTEXT is insufficient, reply exactly: Insufficient context.\\n\"\n",
    "        \"Keep the answer short (1‚Äì2 sentences).\\n\\n\"\n",
    "        \"----- CONTEXT -----\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"-------------------\\n\"\n",
    "        f\"QUESTION: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Tokenizing and generating\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "    outputs = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decoding only the newly generated tokens\n",
    "    answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "hwqhgkIsE0Ip"
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between symmetric and asymmetric encryption?\"\n",
    "\n",
    "# Calling the RAG function\n",
    "answer = get_answer(\n",
    "    client = chroma_client,\n",
    "    collection_name = \"my_documents_locally\",\n",
    "    question = question,\n",
    "    k = 2\n",
    ")\n",
    "\n",
    "# Displaying the output\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}