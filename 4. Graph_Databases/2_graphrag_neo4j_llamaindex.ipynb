{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 🔗 GraphRAG: Combining Graph Databases with Vector Search\n",
    "\n",
    "## Welcome to GraphRAG!\n",
    "\n",
    "In this notebook, you'll learn how to build **GraphRAG** (Graph Retrieval-Augmented Generation) systems that combine the power of:\n",
    "- **Graph databases** (Neo4j) for understanding relationships\n",
    "- **Vector embeddings** for semantic similarity search\n",
    "- **LLMs** (GPT-5-nano) for intelligent entity extraction and query generation\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "- How to automatically extract entities and relationships from unstructured text using LLMs\n",
    "- Building knowledge graphs programmatically with LlamaIndex\n",
    "- Creating vector embeddings for semantic search\n",
    "- Combining graph traversal with vector similarity for superior retrieval\n",
    "- When GraphRAG outperforms traditional RAG systems\n",
    "\n",
    "## 💼 Why GraphRAG Matters\n",
    "\n",
    "Traditional RAG (Retrieval-Augmented Generation) retrieves documents based on **semantic similarity alone**. This works well for simple questions, but fails when:\n",
    "\n",
    "- **Multi-hop reasoning** is needed (\"Who worked with people who know experts in NLP?\")\n",
    "- **Relationships matter** (\"Show me all projects connected to the Data Science team\")\n",
    "- **Entity disambiguation** is required (distinguishing between multiple people named \"John\")\n",
    "- **Structured knowledge** improves context (organizational hierarchies, citation networks)\n",
    "\n",
    "**GraphRAG solves these problems** by:\n",
    "1. Extracting entities and relationships using LLMs\n",
    "2. Building a knowledge graph that captures structure\n",
    "3. Using vector search to find relevant entities\n",
    "4. Traversing the graph to gather rich, connected context\n",
    "5. Providing LLMs with both semantically similar AND structurally related information\n",
    "\n",
    "## 🔗 Building on Notebook 1\n",
    "\n",
    "You've already learned:\n",
    "- Graph database concepts (nodes, relationships, properties)\n",
    "- Cypher query language for pattern matching\n",
    "- Manual knowledge graph construction\n",
    "\n",
    "Now you'll automate everything using:\n",
    "- **LLM-powered entity extraction** (GPT-5-nano)\n",
    "- **LlamaIndex** for document processing and retrieval\n",
    "- **Hybrid search** combining vectors and graphs\n",
    "\n",
    "Let's begin! 🚀\n",
    "\n",
    "::: note\n",
    "**Setup expectations**\n",
    "- The installation cell downloads several Python packages (~1–2 minutes in Colab).\n",
    "- Skip it if you already have the dependencies; just ensure `neo4j`, `py2neo`, `llama-index`, and `chromadb` are installed.\n",
    "- Graph extraction uses OpenAI; set `OPENAI_API_KEY` before running those cells.\n",
    "- Neo4j install commands run only on Colab/Linux; on other environments follow the linked Neo4j docs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "## 📚 Part 1: Theory - Understanding GraphRAG\n",
    "\n",
    "### What is GraphRAG?\n",
    "\n",
    "**GraphRAG** (Graph Retrieval-Augmented Generation) is an advanced RAG technique that enhances traditional vector-based retrieval with graph database capabilities. Instead of only finding semantically similar documents, GraphRAG understands **how entities are connected**.\n",
    "\n",
    "### Traditional RAG vs. GraphRAG\n",
    "\n",
    "**Traditional RAG:**\n",
    "1. Chunk documents into passages\n",
    "2. Create vector embeddings for each chunk\n",
    "3. User asks a question\n",
    "4. Find top-K most similar chunks\n",
    "5. Pass chunks to LLM for answer generation\n",
    "\n",
    "**Limitations:**\n",
    "- No understanding of relationships between entities\n",
    "- Can't answer \"who knows whom\" or \"what's connected to what\"\n",
    "- Misses context from related but not semantically similar documents\n",
    "- Poor performance on multi-hop questions\n",
    "\n",
    "**GraphRAG:**\n",
    "1. Extract entities and relationships from documents using LLMs\n",
    "2. Build a knowledge graph (nodes = entities, edges = relationships)\n",
    "3. Create vector embeddings for both documents AND entities\n",
    "4. User asks a question\n",
    "5. Find relevant entities via vector search\n",
    "6. Traverse the graph to find connected entities and documents\n",
    "7. Combine semantically similar and structurally related context\n",
    "8. Pass enriched context to LLM for answer generation\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Understands relationships (\"Sarah manages Marcus who works on Project X\")\n",
    "- ✅ Multi-hop reasoning (\"Find experts connected to AI researchers in our network\")\n",
    "- ✅ Entity disambiguation (distinguishing between different \"John Smiths\")\n",
    "- ✅ Richer context from connected entities\n",
    "- ✅ Explainable retrieval paths (\"I found this via Sarah → Project X → Document Y\")\n",
    "\n",
    "### 🔑 Key Components of GraphRAG\n",
    "\n",
    "1. **Entity Extraction**: Use LLMs to identify entities (people, organizations, concepts) in text\n",
    "2. **Relationship Extraction**: Identify how entities are connected\n",
    "3. **Knowledge Graph Construction**: Store entities and relationships in a graph database\n",
    "4. **Vector Embeddings**: Create semantic representations of entities and documents\n",
    "5. **Hybrid Retrieval**: Combine vector similarity with graph traversal\n",
    "6. **Context Assembly**: Gather relevant information from multiple sources\n",
    "7. **LLM Generation**: Generate answers using enriched context\n",
    "\n",
    "### 🌟 Real-World Use Cases\n",
    "\n",
    "1. **Enterprise Knowledge Management**: \"Find all documents related to projects that Sarah's team collaborated on with Engineering\"\n",
    "2. **Research Paper Discovery**: \"Show me papers cited by NLP experts who also published on transformers\"\n",
    "3. **Customer Support**: \"Find solutions used by customers in similar industries with related issues\"\n",
    "4. **Legal Document Analysis**: \"Identify all cases related to this statute through precedent citations\"\n",
    "5. **Medical Knowledge Graphs**: \"Find treatment protocols for conditions related to this patient's diagnosis\"\n",
    "\n",
    "### 💡 Key Point: When to Use GraphRAG\n",
    "\n",
    "Use **GraphRAG** when:\n",
    "- Your domain has rich entity relationships (organizational, citation, social networks)\n",
    "- Questions require multi-hop reasoning\n",
    "- Entity disambiguation is important\n",
    "- Relationships are as important as content similarity\n",
    "\n",
    "Use **Traditional RAG** when:\n",
    "- Documents are mostly independent\n",
    "- Simple semantic similarity is sufficient\n",
    "- Lower complexity and faster implementation are priorities\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "- GraphRAG combines vector search with graph traversal for superior retrieval\n",
    "- LLMs extract entities and relationships automatically from text\n",
    "- Knowledge graphs capture structure that vectors alone cannot\n",
    "- Hybrid search provides both semantic and structural relevance\n",
    "- GraphRAG excels at multi-hop reasoning and entity-centric questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "## ⚙️ Part 2: Setup - Installing Dependencies\n",
    "\n",
    "We'll install everything needed for GraphRAG:\n",
    "\n",
    "- **Neo4j Community Edition**: Graph database (same as Notebook 1)\n",
    "- **LlamaIndex**: Document processing and RAG orchestration framework\n",
    "- **OpenAI Python SDK**: For GPT-5-nano API access\n",
    "- **ChromaDB**: Vector database for embeddings\n",
    "- **Additional libraries**: pandas, networkx, matplotlib for analysis\n",
    "\n",
    "This setup takes about 60-90 seconds. Let's begin! ⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Install Neo4j Community Edition in Colab\n",
    "# (Same process as Notebook 1)\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"🔄 Step 1: Installing Java (required for Neo4j)...\")\n",
    "!apt-get update -qq > /dev/null 2>&1\n",
    "!apt-get install -y default-jre wget > /dev/null 2>&1\n",
    "print(\"✅ Java installed!\\n\")\n",
    "\n",
    "print(\"📥 Step 2: Downloading Neo4j Community Edition 4.4.36...\")\n",
    "!wget -q https://dist.neo4j.org/neo4j-community-4.4.36-unix.tar.gz\n",
    "print(\"✅ Downloaded!\\n\")\n",
    "\n",
    "print(\"📦 Step 3: Extracting Neo4j...\")\n",
    "!tar -xf neo4j-community-4.4.36-unix.tar.gz\n",
    "!mv neo4j-community-4.4.36 neo4j\n",
    "print(\"✅ Extracted to 'neo4j' folder!\\n\")\n",
    "\n",
    "print(\"🔐 Step 4: Setting initial password...\")\n",
    "!neo4j/bin/neo4j-admin set-initial-password password123\n",
    "print(\"✅ Password set to: password123\\n\")\n",
    "\n",
    "print(\"🚀 Step 5: Starting Neo4j server...\")\n",
    "!neo4j/bin/neo4j start\n",
    "print(\"✅ Neo4j starting...\\n\")\n",
    "\n",
    "print(\"⏳ Step 6: Waiting for Neo4j to be fully ready...\")\n",
    "import socket\n",
    "max_attempts = 30\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)\n",
    "        result = sock.connect_ex(('127.0.0.1', 7687))\n",
    "        sock.close()\n",
    "        if result == 0:\n",
    "            print(f\"✅ Neo4j is ready! (took {attempt * 2} seconds)\\n\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    if attempt % 5 == 0 and attempt > 0:\n",
    "        print(f\"   Still waiting... ({attempt}/{max_attempts})\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"📊 NEO4J CONNECTION DETAILS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"URI:      bolt://localhost:7687\")\n",
    "print(\"Username: neo4j\")\n",
    "print(\"Password: password123\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n🎉 Neo4j setup complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Install Python dependencies for GraphRAG\n",
    "\n",
    "print(\"📥 Installing GraphRAG dependencies...\")\n",
    "print(\"This may take 30-60 seconds...\\n\")\n",
    "\n",
    "# Fix dependency issue first\n",
    "!pip install -q jedi\n",
    "\n",
    "# Core dependencies\n",
    "!pip install -q neo4j py2neo\n",
    "!pip install -q openai\n",
    "!pip install -q llama-index\n",
    "!pip install -q llama-index-graph-stores-neo4j\n",
    "!pip install -q llama-index-vector-stores-chroma\n",
    "!pip install -q llama-index-embeddings-openai\n",
    "!pip install -q chromadb\n",
    "!pip install -q pandas networkx matplotlib\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ All dependencies installed!\")\n",
    "print(\"\\n📦 Installed packages:\")\n",
    "print(\"  ✓ neo4j, py2neo - Graph database drivers\")\n",
    "print(\"  ✓ openai - GPT-5-nano API access\")\n",
    "print(\"  ✓ llama-index - RAG orchestration framework\")\n",
    "print(\"  ✓ chromadb - Vector database\")\n",
    "print(\"  ✓ pandas, networkx, matplotlib - Data analysis and visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 Import all necessary libraries\n",
    "\n",
    "# Neo4j for graph database\n",
    "from neo4j import GraphDatabase\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "# LlamaIndex core\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "import chromadb\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "# Try loading from Colab secrets first (if running in Colab)\n",
    "try:\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    if OPENAI_API_KEY:\n",
    "        print(\"✅ API key loaded from Colab secrets\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Fallback to environment variable\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\n",
    "        \"❌ ERROR: No OpenAI API key found. Set OPENAI_API_KEY as an environment variable or Colab secret.\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Cost-efficient model for function calling\n",
    "print(f\"🤖 Selected Model: {OPENAI_MODEL}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Configure LlamaIndex to use OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = None  # We'll use OpenAI client directly for more control\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"✅ OpenAI client configured for LlamaIndex!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Configure Neo4j connection\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password123\"\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# Test connection\n",
    "def test_connection():\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"MATCH (n) RETURN count(n) as count\")\n",
    "            count = result.single()[\"count\"]\n",
    "            print(\"✅ Successfully connected to Neo4j!\")\n",
    "            print(f\"📊 Current node count: {count}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_connection()\n",
    "\n",
    "# Initialize LlamaIndex Neo4j graph store\n",
    "# Note: refresh_schema=False because APOC procedures are not installed in Neo4j Community Edition\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    "    database=\"neo4j\",\n",
    "    refresh_schema=False,  # Disable APOC-dependent schema refresh\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Neo4j graph store initialized for LlamaIndex!\")\n",
    "print(\"   (Schema refresh disabled - APOC not required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗄️ Initialize ChromaDB vector store\n",
    "\n",
    "# Create ChromaDB client (in-memory for Colab)\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "# Create a collection for our documents\n",
    "collection_name = \"graphrag_documents\"\n",
    "chroma_collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "# Initialize LlamaIndex ChromaDB vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "print(\"✅ ChromaDB vector store initialized!\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Mode: Ephemeral (in-memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "## 📄 Part 3: Loading Sample Documents\n",
    "\n",
    "### Creating Unstructured Text Data\n",
    "\n",
    "In this section, we'll create sample documents that simulate real-world scenarios:\n",
    "- **Project updates** describing team collaborations\n",
    "- **Research summaries** mentioning authors and citations\n",
    "- **Meeting notes** with action items and relationships\n",
    "\n",
    "These documents contain **implicit entities** (people, projects, organizations) and **relationships** (works on, collaborates with, cites) that we'll extract using GPT-5-nano.\n",
    "\n",
    "### Why Unstructured Text?\n",
    "\n",
    "Real-world knowledge exists in:\n",
    "- Email threads and Slack messages\n",
    "- Research papers and technical reports\n",
    "- Meeting notes and project documentation\n",
    "- Customer support tickets\n",
    "- Legal contracts and medical records\n",
    "\n",
    "**GraphRAG automatically extracts structure** from this unstructured text, building a knowledge graph without manual data entry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📝 Create sample documents with rich entities and relationships\n",
    "\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"title\": \"Q4 2024 AI Research Team Update\",\n",
    "        \"content\": \"\"\"\n",
    "The AI Research team, led by Dr. Sarah Chen, has made significant progress on the \n",
    "Customer Churn Prediction project. The team includes Marcus Johnson (Senior ML Engineer) \n",
    "and Priya Patel (Data Scientist), who have been collaborating closely with the \n",
    "Data Engineering team headed by Tom Wilson.\n",
    "\n",
    "The project leverages transformer-based models and achieved 89% accuracy in predicting \n",
    "customer churn. Sarah presented these results at the NeurIPS 2024 conference, where \n",
    "she connected with Prof. Andrew Ng from Stanford University, who expressed interest \n",
    "in collaborating on future research.\n",
    "\n",
    "The project is scheduled to move to production in Q1 2025, with support from the \n",
    "Engineering department led by David Kim. Marcus will be leading the deployment effort.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Recommendation Engine v2 - Technical Design\",\n",
    "        \"content\": \"\"\"\n",
    "The Recommendation Engine v2 project aims to improve our content recommendation system \n",
    "using graph neural networks. The technical lead is Marcus Johnson, working with James Liu \n",
    "(ML Engineer) and Robert Brown (Senior Data Scientist).\n",
    "\n",
    "This project builds upon research from \"Attention Is All You Need\" (Vaswani et al., 2017) \n",
    "and incorporates recent advances in graph representation learning. The team cited work by \n",
    "Prof. Jure Leskovec from Stanford on knowledge graph embeddings as a key inspiration.\n",
    "\n",
    "James Liu previously worked on the NLP Chatbot project with Priya Patel, bringing valuable \n",
    "experience in transformer architectures. Robert Brown contributed his expertise in \n",
    "feature engineering, drawing from his background at Google Research where he collaborated \n",
    "with Dr. Geoffrey Hinton's team.\n",
    "\n",
    "The project integrates with our existing Neo4j knowledge graph, which contains user behavior \n",
    "data collected by the Analytics team under Sophie Martin (Product Manager).\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Fraud Detection System - Project Kickoff Notes\",\n",
    "        \"content\": \"\"\"\n",
    "Meeting Date: October 15, 2024\n",
    "Attendees: Marcus Johnson, Robert Brown, James Liu, Amy Zhang (Senior Backend Engineer), \n",
    "Lisa Anderson (DevOps Engineer)\n",
    "\n",
    "The Fraud Detection System project was initiated following a request from the Finance \n",
    "department. This critical project will use anomaly detection algorithms to identify \n",
    "suspicious transaction patterns in real-time.\n",
    "\n",
    "Marcus Johnson will serve as technical lead, with Robert Brown focusing on model development. \n",
    "The system will be deployed on AWS infrastructure managed by Lisa Anderson. Amy Zhang will \n",
    "build the API layer for integration with our existing payment processing system.\n",
    "\n",
    "The project references research on graph-based fraud detection by Prof. Danai Koutra from \n",
    "University of Michigan. Robert Brown attended her keynote at KDD 2024 and proposed adapting \n",
    "her techniques for our use case.\n",
    "\n",
    "Timeline: 6-month project with go-live targeted for April 2025. The project will be reviewed \n",
    "monthly by Alex Turner (CEO) and Jennifer Lee (CTO).\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Research Paper: Deep Learning for Time Series Forecasting\",\n",
    "        \"content\": \"\"\"\n",
    "Authors: Priya Patel, Elena Rodriguez, Tom Wilson\n",
    "Affiliation: TechCorp AI Research Lab\n",
    "\n",
    "Abstract: This paper presents a novel approach to time series forecasting using deep learning \n",
    "techniques. We build upon recent work by Prof. Yoshua Bengio on attention mechanisms and \n",
    "propose a hybrid model combining LSTMs with transformer architectures.\n",
    "\n",
    "Our approach was validated on the Customer Churn Prediction dataset, demonstrating significant \n",
    "improvements over baseline methods. We acknowledge contributions from Dr. Sarah Chen, who \n",
    "provided guidance on model architecture, and Marcus Johnson, who assisted with hyperparameter \n",
    "tuning.\n",
    "\n",
    "This work was presented at the Real-time Analytics Dashboard internal symposium and received \n",
    "positive feedback from Jennifer Lee (CTO) and David Kim (VP of Engineering). We plan to submit \n",
    "this work to ICML 2025.\n",
    "\n",
    "Related Work: Our approach builds on \"BERT: Pre-training of Deep Bidirectional Transformers\" \n",
    "(Devlin et al., 2018) and \"Temporal Fusion Transformers\" (Lim et al., 2021). We also \n",
    "incorporate ideas from graph neural networks research by Prof. Jure Leskovec at Stanford.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Engineering Team Quarterly Review - Q4 2024\",\n",
    "        \"content\": \"\"\"\n",
    "The Engineering team, under VP David Kim, delivered exceptional results this quarter. \n",
    "Key accomplishments include:\n",
    "\n",
    "1. Infrastructure Migration: Led by Lisa Anderson and Mohammed Ali (Backend Engineer), \n",
    "the team successfully migrated 80% of our services to Kubernetes. This project involved \n",
    "close collaboration with the DevOps team and was completed ahead of schedule.\n",
    "\n",
    "2. Mobile App Redesign: Carlos Santos (Frontend Engineer) and Raj Sharma (Product Designer) \n",
    "shipped the new mobile interface, which increased user engagement by 35%. Nina Williams \n",
    "(UX Researcher) conducted extensive user testing that informed the final design.\n",
    "\n",
    "3. A/B Testing Platform: Sophie Martin (Product Manager) spearheaded the development of \n",
    "our new A/B testing infrastructure. Amy Zhang built the backend services, while Carlos \n",
    "Santos implemented the frontend dashboard.\n",
    "\n",
    "The team collaborated extensively with the Data Science department (Dr. Sarah Chen) on the \n",
    "Real-time Analytics Dashboard project. Tom Wilson (Data Engineer) built the data pipelines, \n",
    "while Amy Zhang and Elena Rodriguez (Data Analyst) created the visualization layer.\n",
    "\n",
    "Next quarter priorities were set in consultation with Alex Turner (CEO) and Jennifer Lee (CTO). \n",
    "The focus will be on AI/ML model deployment automation and expanding our cloud infrastructure.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Partnership Announcement: TechCorp and Stanford AI Lab\",\n",
    "        \"content\": \"\"\"\n",
    "TechCorp is excited to announce a research partnership with Stanford University's AI Lab, \n",
    "led by Prof. Andrew Ng and Prof. Jure Leskovec. This collaboration emerged from discussions \n",
    "at NeurIPS 2024 where Dr. Sarah Chen presented our work on customer churn prediction.\n",
    "\n",
    "The partnership will focus on three areas:\n",
    "\n",
    "1. Graph Neural Networks: Prof. Jure Leskovec will advise our team (Marcus Johnson, \n",
    "Robert Brown, James Liu) on applying graph neural networks to recommendation systems.\n",
    "\n",
    "2. Transfer Learning: Prof. Andrew Ng will collaborate with Priya Patel and Maria Garcia \n",
    "(Junior ML Engineer) on transfer learning techniques for computer vision applications \n",
    "related to our Computer Vision API project.\n",
    "\n",
    "3. Knowledge Graphs: The Stanford team will work with our Data Engineering group (Tom Wilson, \n",
    "Elena Rodriguez) to enhance our Neo4j-based knowledge graph infrastructure.\n",
    "\n",
    "This partnership was championed by Jennifer Lee (CTO) and Alex Turner (CEO), with support \n",
    "from David Kim (VP of Engineering) and Dr. Sarah Chen (VP of Data Science). The first joint \n",
    "research workshop is scheduled for January 2025 at Stanford's campus.\n",
    "\n",
    "Prof. Andrew Ng commented: \"TechCorp's work on combining graph databases with machine learning \n",
    "aligns perfectly with our research vision. We're particularly impressed by the team's implementation \n",
    "of attention mechanisms in their recommendation engine.\"\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to LlamaIndex Document objects\n",
    "documents = [\n",
    "    Document(\n",
    "        text=doc[\"content\"],\n",
    "        metadata={\"title\": doc[\"title\"]}\n",
    "    )\n",
    "    for doc in sample_documents\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(documents)} sample documents\")\n",
    "print(\"\\n📄 Document titles:\")\n",
    "for i, doc in enumerate(sample_documents, 1):\n",
    "    print(f\"  {i}. {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "## 🤖 Part 4: LLM-Powered Entity Extraction\n",
    "\n",
    "### Automating Knowledge Graph Construction\n",
    "\n",
    "Instead of manually identifying entities and relationships (as in Notebook 1), we'll use **GPT-5-nano** to automatically extract:\n",
    "\n",
    "- **Entities**: People, organizations, projects, technologies, locations\n",
    "- **Relationships**: works_on, collaborates_with, leads, cites, presents_at\n",
    "- **Properties**: Roles, affiliations, dates, locations\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Prompt Engineering**: We'll craft prompts that instruct the LLM to extract structured information\n",
    "2. **Entity Recognition**: GPT-5-nano identifies named entities in the text\n",
    "3. **Relationship Extraction**: The LLM infers connections between entities\n",
    "4. **Structured Output**: We'll parse the LLM response into graph-ready format\n",
    "\n",
    "This approach scales to thousands of documents without manual annotation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Create entity extraction function using GPT-5-nano\n",
    "\n",
    "def extract_entities_and_relationships(text: str, doc_title: str = \"\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use GPT-5-nano to extract entities and relationships from text.\n",
    "    \n",
    "    Args:\n",
    "        text: The document text to analyze\n",
    "        doc_title: Optional document title for context\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing extracted entities and relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    extraction_prompt = f\"\"\"\n",
    "Extract entities and relationships from the following text. Return the results as a JSON object.\n",
    "\n",
    "TEXT:\n",
    "{text[:2000]}  # Limit to first 2000 chars for efficiency\n",
    "\n",
    "Extract:\n",
    "1. **Entities** with types: Person, Organization, Project, Technology, Location, Event, Concept\n",
    "2. **Relationships** between entities (who works with whom, who leads what, etc.)\n",
    "\n",
    "Return JSON format:\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\"name\": \"Entity Name\", \"type\": \"Person|Organization|Project|Technology|Location|Event|Concept\", \"properties\": {{\"role\": \"...\", \"affiliation\": \"...\"}}}},\n",
    "    ...\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {{\"source\": \"Entity 1\", \"target\": \"Entity 2\", \"type\": \"works_on|collaborates_with|leads|cites|presents_at|employed_by\", \"properties\": {{}}}},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Focus on the most important entities and clear relationships. Be concise.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call GPT-5-nano using the Responses API\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            input=extraction_prompt,\n",
    "            reasoning={\"effort\": \"minimal\"}  # Optimize for speed\n",
    "        )\n",
    "        \n",
    "        # Extract the response text\n",
    "        result_text = response.output_text if hasattr(response, 'output_text') else str(response)\n",
    "        \n",
    "        # Parse JSON (handle cases where LLM might add markdown formatting)\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        extracted = json.loads(result_text.strip())\n",
    "        extracted[\"document_title\"] = doc_title\n",
    "        \n",
    "        return extracted\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Extraction error: {e}\")\n",
    "        return {\"entities\": [], \"relationships\": [], \"document_title\": doc_title}\n",
    "\n",
    "print(\"✅ Entity extraction function created!\")\n",
    "print(\"   Model: gpt-5-nano\")\n",
    "print(\"   Format: JSON output with entities and relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Extract entities from all documents\n",
    "\n",
    "print(\"🤖 Extracting entities and relationships from documents...\")\n",
    "print(\"This will take 30-60 seconds (using GPT-5-nano)...\\n\")\n",
    "\n",
    "extracted_data = []\n",
    "\n",
    "for i, doc in enumerate(sample_documents, 1):\n",
    "    print(f\"📄 Processing document {i}/{len(sample_documents)}: {doc['title'][:50]}...\")\n",
    "    \n",
    "    result = extract_entities_and_relationships(doc[\"content\"], doc[\"title\"])\n",
    "    extracted_data.append(result)\n",
    "    \n",
    "    # Show sample of extracted entities\n",
    "    if result[\"entities\"]:\n",
    "        print(f\"   ✓ Found {len(result['entities'])} entities, {len(result['relationships'])} relationships\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ No entities extracted\")\n",
    "    \n",
    "print(f\"\\n✅ Extraction complete!\")\n",
    "print(f\"📊 Total entities extracted: {sum(len(d['entities']) for d in extracted_data)}\")\n",
    "print(f\"🔗 Total relationships extracted: {sum(len(d['relationships']) for d in extracted_data)}\")\n",
    "\n",
    "# Show a sample of extracted data\n",
    "print(\"\\n📋 Sample extraction from first document:\")\n",
    "if extracted_data[0][\"entities\"]:\n",
    "    print(f\"\\\\nEntities (first 3):\")\n",
    "    for entity in extracted_data[0][\"entities\"][:3]:\n",
    "        print(f\"  - {entity['name']} ({entity['type']})\")\n",
    "    \n",
    "    print(f\"\\\\nRelationships (first 3):\")\n",
    "    for rel in extracted_data[0][\"relationships\"][:3]:\n",
    "        print(f\"  - {rel['source']} --[{rel['type']}]--> {rel['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "---\n",
    "## 🏗️ Part 5: Building the Knowledge Graph Automatically\n",
    "\n",
    "### From Extracted Data to Neo4j\n",
    "\n",
    "Now that we've extracted entities and relationships using GPT-5-nano, we'll automatically populate our Neo4j knowledge graph. This process:\n",
    "\n",
    "1. **Deduplicates entities** (merge multiple mentions of \"Sarah Chen\" into one node)\n",
    "2. **Creates nodes** for each unique entity\n",
    "3. **Establishes relationships** between entities\n",
    "4. **Adds properties** (roles, affiliations, etc.)\n",
    "5. **Links documents** to entities they mention\n",
    "\n",
    "This is where **manual work** (Notebook 1) becomes **fully automated** (Notebook 2)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ Build the knowledge graph from extracted data\n",
    "\n",
    "# Suppress Neo4j cartesian product warnings (they're informational, not errors)\n",
    "import logging\n",
    "logging.getLogger(\"neo4j.notifications\").setLevel(logging.ERROR)\n",
    "\n",
    "# Clear existing data\n",
    "with driver.session() as session:\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"🧹 Cleared existing graph data\\n\")\n",
    "\n",
    "# Helper function to insert entities and relationships\n",
    "def build_graph_from_extractions(extracted_data_list):\n",
    "    \"\"\"\n",
    "    Build Neo4j knowledge graph from LLM-extracted data.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        total_entities = 0\n",
    "        total_relationships = 0\n",
    "        \n",
    "        for extraction in extracted_data_list:\n",
    "            doc_title = extraction.get(\"document_title\", \"Unknown\")\n",
    "            \n",
    "            # Create Document node\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (d:Document {title: $title})\n",
    "                \"\"\",\n",
    "                title=doc_title\n",
    "            )\n",
    "            \n",
    "            # Create entity nodes\n",
    "            for entity in extraction.get(\"entities\", []):\n",
    "                name = entity.get(\"name\", \"\").strip()\n",
    "                entity_type = entity.get(\"type\", \"Entity\")\n",
    "                properties = entity.get(\"properties\", {})\n",
    "                \n",
    "                if not name:\n",
    "                    continue\n",
    "                \n",
    "                # Create or merge entity node\n",
    "                query = f\"\"\"\n",
    "                MERGE (e:{entity_type} {{name: $name}})\n",
    "                SET e += $properties\n",
    "                \"\"\"\n",
    "                session.run(query, name=name, properties=properties)\n",
    "                \n",
    "                # Link entity to document (fixed: removed double braces)\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (e {name: $name}), (d:Document {title: $doc_title})\n",
    "                    MERGE (e)-[:MENTIONED_IN]->(d)\n",
    "                    \"\"\",\n",
    "                    name=name,\n",
    "                    doc_title=doc_title\n",
    "                )\n",
    "                \n",
    "                total_entities += 1\n",
    "            \n",
    "            # Create relationships\n",
    "            for rel in extraction.get(\"relationships\", []):\n",
    "                source = rel.get(\"source\", \"\").strip()\n",
    "                target = rel.get(\"target\", \"\").strip()\n",
    "                rel_type = rel.get(\"type\", \"RELATED_TO\").upper().replace(\" \", \"_\")\n",
    "                rel_props = rel.get(\"properties\", {})\n",
    "                \n",
    "                if not source or not target:\n",
    "                    continue\n",
    "                \n",
    "                session.run(\n",
    "                    f\"\"\"\n",
    "                    MATCH (s {{name: $source}}), (t {{name: $target}})\n",
    "                    MERGE (s)-[r:{rel_type}]->(t)\n",
    "                    SET r += $properties\n",
    "                    \"\"\",\n",
    "                    source=source,\n",
    "                    target=target,\n",
    "                    properties=rel_props\n",
    "                )\n",
    "                \n",
    "                total_relationships += 1\n",
    "        \n",
    "        return total_entities, total_relationships\n",
    "\n",
    "# Build the graph\n",
    "print(\"🏗️ Building knowledge graph in Neo4j...\")\n",
    "entities_created, rels_created = build_graph_from_extractions(extracted_data)\n",
    "\n",
    "print(f\"\\n✅ Knowledge graph built successfully!\")\n",
    "print(f\"   📊 Entities created: {entities_created}\")\n",
    "print(f\"   🔗 Relationships created: {rels_created}\")\n",
    "\n",
    "# Verify the graph\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN labels(n)[0] as type, count(*) as count\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    print(f\"\\n📋 Node types in graph:\")\n",
    "    for record in result:\n",
    "        print(f\"   - {record['type']}: {record['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Part 6: Creating Vector Embeddings\n",
    "\n",
    "### Why Vector Embeddings?\n",
    "\n",
    "While our knowledge graph captures **structure** (who knows whom, who works on what), vector embeddings capture **semantic meaning**. By combining both, we get:\n",
    "\n",
    "- **Semantic search**: Find conceptually similar content\n",
    "- **Entity disambiguation**: Distinguish between entities with similar names using context\n",
    "- **Hybrid retrieval**: Use vector similarity to find entry points, then traverse the graph\n",
    "\n",
    "### What We'll Create\n",
    "\n",
    "- Document embeddings for each text\n",
    "- Entity embeddings (using entity names + context)\n",
    "- Store everything in ChromaDB for fast similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Create vector embeddings for documents\n",
    "\n",
    "# Split documents into chunks for better retrieval\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"📄 Split {len(documents)} documents into {len(nodes)} chunks\")\n",
    "\n",
    "# Create embeddings and store in ChromaDB\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print(\"\\n🔄 Creating embeddings (this may take 30-60 seconds)...\")\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Vector embeddings created and stored in ChromaDB!\")\n",
    "print(f\"   📊 Total document chunks embedded: {len(nodes)}\")\n",
    "print(f\"   🎯 Embedding model: text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔍 Part 7-8: Vector Search vs Graph Search\n",
    "\n",
    "### Traditional RAG (Vector-Only)\n",
    "\n",
    "Traditional RAG uses only vector similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Traditional Vector-Only Search (Baseline)\n",
    "\n",
    "def vector_search(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Pure vector similarity search (traditional RAG baseline).\n",
    "    \"\"\"\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=top_k)\n",
    "    response = query_engine.query(query)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test vector search\n",
    "query = \"Who is working on the Recommendation Engine project?\"\n",
    "print(f\"❓ Query: {query}\\n\")\n",
    "print(\"📊 Vector Search Results:\")\n",
    "result = vector_search(query)\n",
    "print(f\"\\nAnswer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Graph-Based Retrieval\n",
    "\n",
    "Graph search uses Cypher to traverse relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Graph-Based Search\n",
    "\n",
    "def graph_search(entity_name: str):\n",
    "    \"\"\"\n",
    "    Retrieve entity and its connections from the knowledge graph.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (e {name: $name})-[r]-(connected)\n",
    "            RETURN e.name as entity, type(r) as relationship, \n",
    "                   connected.name as connected_entity, labels(connected)[0] as type\n",
    "            LIMIT 20\n",
    "            \"\"\",\n",
    "            name=entity_name\n",
    "        )\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "# Test graph search\n",
    "print(\"🔗 Graph Search Results for 'Marcus Johnson':\\n\")\n",
    "connections = graph_search(\"Marcus Johnson\")\n",
    "for conn in connections[:5]:\n",
    "    print(f\"  {conn['entity']} --[{conn['relationship']}]--> {conn['connected_entity']} ({conn['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "## 🚀 Part 9-10: Hybrid Search - GraphRAG in Action\n",
    "\n",
    "### Combining Vector + Graph\n",
    "\n",
    "GraphRAG combines both approaches:\n",
    "1. Use vector search to find relevant entities\n",
    "2. Traverse the graph to find connected context\n",
    "3. Assemble rich, relationship-aware context for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 GraphRAG: Hybrid Vector + Graph Search\n",
    "\n",
    "def graphrag_search(query: str, top_k: int = 3, max_hops: int = 2):\n",
    "    \"\"\"\n",
    "    Hybrid search combining vector similarity and graph traversal.\n",
    "    \n",
    "    Steps:\n",
    "    1. Vector search to find relevant documents\n",
    "    2. Extract mentioned entities from those documents\n",
    "    3. Graph traversal to find connected entities and documents\n",
    "    4. Assemble comprehensive context\n",
    "    \"\"\"\n",
    "    print(f\"🔍 GraphRAG Search for: '{query}'\\n\")\n",
    "    \n",
    "    # Step 1: Vector search for relevant documents\n",
    "    print(\"📊 Step 1: Vector search for relevant documents...\")\n",
    "    vector_results = vector_index.as_query_engine(similarity_top_k=top_k).query(query)\n",
    "    \n",
    "    # Step 2: Extract entities mentioned in top documents\n",
    "    print(\"📋 Step 2: Identifying entities from results...\")\n",
    "    entities_found = set()\n",
    "    \n",
    "    # Find entities connected to retrieved documents\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (e)-[:MENTIONED_IN]->(d:Document)\n",
    "            RETURN DISTINCT e.name as entity, labels(e)[0] as type\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        )\n",
    "        for record in result:\n",
    "            entities_found.add(record[\"entity\"])\n",
    "    \n",
    "    print(f\"   Found {len(entities_found)} entities\")\n",
    "    \n",
    "    # Step 3: Graph traversal from these entities\n",
    "    print(\"🔗 Step 3: Traversing knowledge graph...\")\n",
    "    graph_context = []\n",
    "    \n",
    "    for entity in list(entities_found)[:5]:  # Limit for demo\n",
    "        connections = graph_search(entity)\n",
    "        graph_context.extend(connections[:3])\n",
    "    \n",
    "    print(f\"   Retrieved {len(graph_context)} relationship triples\")\n",
    "    \n",
    "    # Step 4: Assemble final context\n",
    "    print(\"\\n✅ GraphRAG Results:\\n\")\n",
    "    print(f\"📊 Vector-based answer:\\n{vector_results}\\n\")\n",
    "    print(f\"🔗 Graph-enriched context:\")\n",
    "    for ctx in graph_context[:5]:\n",
    "        print(f\"   - {ctx['entity']} --[{ctx['relationship']}]--> {ctx['connected_entity']}\")\n",
    "    \n",
    "    return {\"vector_answer\": str(vector_results), \"graph_context\": graph_context}\n",
    "\n",
    "# Test GraphRAG\n",
    "result = graphrag_search(\"Who collaborates with Marcus Johnson on AI projects?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎓 Part 11: Advanced GraphRAG Patterns\n",
    "\n",
    "### Multi-Hop Reasoning\n",
    "\n",
    "GraphRAG excels at multi-hop questions that require traversing multiple relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎓 Advanced Pattern: Multi-Hop Reasoning\n",
    "\n",
    "def multi_hop_query(start_entity: str, relationship_pattern: str, hops: int = 2):\n",
    "    \"\"\"\n",
    "    Traverse multiple relationship hops in the knowledge graph.\n",
    "    Example: Find people who work with people who collaborated with start_entity\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        query = f\"\"\"\n",
    "        MATCH path = (start {{name: $start}})-[*1..{hops}]-(connected)\n",
    "        WHERE connected:Person OR connected:Organization\n",
    "        RETURN DISTINCT connected.name as name, \n",
    "               labels(connected)[0] as type,\n",
    "               length(path) as distance\n",
    "        ORDER BY distance\n",
    "        LIMIT 15\n",
    "        \"\"\"\n",
    "        result = session.run(query, start=start_entity)\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "# Example: Find people within 2 degrees of Sarah Chen\n",
    "print(\"🔗 Multi-hop query: People within 2 connections of Sarah Chen\\n\")\n",
    "results = multi_hop_query(\"Sarah Chen\", \"COLLABORATES_WITH|WORKS_ON\", hops=2)\n",
    "\n",
    "for r in results[:10]:\n",
    "    print(f\"  {r['name']} ({r['type']}) - {r['distance']} hop(s) away\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Entity Disambiguation\n",
    "\n",
    "GraphRAG uses graph context to disambiguate entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Entity Disambiguation using Graph Context\n",
    "\n",
    "def disambiguate_entity(entity_name: str):\n",
    "    \"\"\"\n",
    "    Use graph relationships to provide context for entity disambiguation.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (e {name: $name})\n",
    "            OPTIONAL MATCH (e)-[r1]-(connected1)\n",
    "            OPTIONAL MATCH (e)-[:MENTIONED_IN]->(d:Document)\n",
    "            RETURN e.name as name, \n",
    "                   labels(e)[0] as type,\n",
    "                   collect(DISTINCT type(r1)) as relationship_types,\n",
    "                   collect(DISTINCT connected1.name)[0..5] as connected_to,\n",
    "                   collect(DISTINCT d.title)[0..3] as mentioned_in_docs\n",
    "            \"\"\"\n",
    "            ,\n",
    "            name=entity_name\n",
    "        )\n",
    "        \n",
    "        for record in result:\n",
    "            print(f\"Entity: {record['name']} ({record['type']})\")\n",
    "            print(f\"  Connected via: {', '.join(record['relationship_types'][:5])}\")\n",
    "            print(f\"  Connected to: {', '.join([c for c in record['connected_to'] if c][:5])}\")\n",
    "            print(f\"  Mentioned in: {', '.join(record['mentioned_in_docs'])}\")\n",
    "\n",
    "# Example\n",
    "print(\"🔍 Disambiguating 'Marcus Johnson':\\n\")\n",
    "disambiguate_entity(\"Marcus Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "---\n",
    "## 🚀 Part 12: Next Steps & Production Considerations\n",
    "\n",
    "### Congratulations! 🎉\n",
    "\n",
    "You've learned how to build complete GraphRAG systems that combine:\n",
    "- ✅ LLM-powered entity extraction (GPT-5-nano)\n",
    "- ✅ Automated knowledge graph construction (Neo4j)\n",
    "- ✅ Vector embeddings for semantic search (ChromaDB)\n",
    "- ✅ Hybrid retrieval combining graphs and vectors\n",
    "- ✅ Multi-hop reasoning and entity disambiguation\n",
    "\n",
    "### 🏭 Production Considerations\n",
    "\n",
    "**Scaling Entity Extraction:**\n",
    "- Batch processing for large document corpora\n",
    "- Caching LLM extractions to reduce API costs\n",
    "- Fine-tuning models for domain-specific entity types\n",
    "- Using cheaper models (gpt-5-nano) for extraction, premium models for generation\n",
    "\n",
    "**Knowledge Graph Optimization:**\n",
    "- Entity resolution and deduplication\n",
    "- Confidence scores for extracted relationships\n",
    "- Temporal relationships (when did X work with Y?)\n",
    "- Graph embeddings for entity similarity\n",
    "\n",
    "**Vector Store Scaling:**\n",
    "- Persistent ChromaDB or Pinecone for production\n",
    "- Hybrid search with BM25 + dense embeddings\n",
    "- Metadata filtering for faster retrieval\n",
    "- Query result caching\n",
    "\n",
    "**GraphRAG Query Optimization:**\n",
    "- Limit graph traversal depth to prevent slowdowns\n",
    "- Use Cypher query optimization (indexes, profiling)\n",
    "- Parallel retrieval (vector + graph searches simultaneously)\n",
    "- LLM result caching for common queries\n",
    "\n",
    "### 📚 Further Learning\n",
    "\n",
    "1. **LlamaIndex Graph RAG Documentation**: Advanced GraphRAG patterns\n",
    "2. **Neo4j Graph Data Science**: Centrality, community detection, graph algorithms\n",
    "3. **Hybrid Search Research**: Papers on combining dense + sparse + graph retrieval\n",
    "4. **Entity Linking**: Linking extracted entities to knowledge bases (Wikidata, DBpedia)\n",
    "\n",
    "### 🛠️ Practice Exercises\n",
    "\n",
    "1. **Add New Documents**: Extract entities from your own documents\n",
    "2. **Custom Entity Types**: Modify extraction prompts for your domain\n",
    "3. **Complex Queries**: Write multi-hop Cypher queries for your use case\n",
    "4. **Evaluation**: Compare GraphRAG vs traditional RAG on complex questions\n",
    "5. **Visualization**: Use NetworkX to visualize the extracted knowledge graph\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "- **GraphRAG > Traditional RAG** for entity-centric and relationship-heavy domains\n",
    "- **LLMs automate** what used to require manual knowledge engineering\n",
    "- **Hybrid search** provides both semantic and structural relevance\n",
    "- **Knowledge graphs** enable explainable, multi-hop reasoning\n",
    "- **Production systems** require careful optimization of extraction, storage, and retrieval\n",
    "\n",
    "You now have the skills to build production GraphRAG systems! 🚀\n",
    "\n",
    "### 📊 System Architecture Summary\n",
    "\n",
    "```\n",
    "Documents (Unstructured Text)\n",
    "      |\n",
    "      v\n",
    "GPT-5-nano (Entity Extraction)\n",
    "      |\n",
    "      v\n",
    "Entities + Relationships (Structured)\n",
    "      |\n",
    "      ├─────> Neo4j (Knowledge Graph)\n",
    "      |\n",
    "      └─────> ChromaDB (Vector Embeddings)\n",
    "             |\n",
    "             v\n",
    "        GraphRAG Engine\n",
    "             |\n",
    "             ├─> Vector Search (Semantic)\n",
    "             ├─> Graph Traversal (Structural)\n",
    "             └─> Hybrid Results (Best of Both)\n",
    "```\n",
    "\n",
    "Thank you for completing this notebook! See you in your next AI project! 🎓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 Cleanup: Close connections\n",
    "\n",
    "print(\"🧹 Closing database connections...\")\n",
    "driver.close()\n",
    "print(\"✅ Connections closed!\")\n",
    "print(\"\\n👋 Thanks for learning GraphRAG!\")\n",
    "print(\"🚀 Now go build amazing knowledge-graph-powered AI applications!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
