{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "\n",
        "# Memorization vs RAG, and 4 Layers of Protection\n",
        "\n",
        "In this notebook, you will:\n",
        "  - Understand model memorization and how Differential Privacy (DP)\n",
        "  mitigates it\n",
        "  - Implement four security layers that work together to protect\n",
        "  sensitive information while maintaining system utility.\n",
        "\n",
        "  By combining these layers, we create a defense-in-depth approach\n",
        "  where even if one layer fails, others provide backup protection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "\n",
        "# 1. Model Memorization & Differential Privacy\n",
        "\n",
        "  Large language models can sometimes memorize rare or sensitive data\n",
        "  seen during training, a phenomenon known as **model memorization**. This\n",
        "  creates **a risk that private information could unintentionally\n",
        "  resurface in model outputs**. For example, an LLM trained on customer\n",
        "  support tickets might inadvertently reproduce specific email\n",
        "  addresses, account numbers, or confidential details when prompted in\n",
        "  certain ways.\n",
        "\n",
        "\n",
        "  In the real world, there is a technique called **differential privacy**\n",
        "  (DP) which is designed to **prevent such leakage by adding controlled\n",
        "  noise during training.** This ensures the model learns general patterns\n",
        "   rather than exact records.\n",
        "\n",
        "\n",
        "Imagine a group of employees wants to compute their average salary\n",
        "  without revealing individual salaries:\n",
        "  1. Each person adds a small random offset (e.g., +50€ or -120€)\n",
        "  to their actual salary before reporting it\n",
        "  2. Individually, the reported salaries are \"noisy\", so no one can\n",
        "  determine a person's true salary\n",
        "  3. However, when averaged across many people, the random offsets\n",
        "  cancel out statistically\n",
        "  4. The final average remains accurate while each individual's privacy\n",
        "   is protected\n",
        "\n",
        "  This is how differential privacy works: by\n",
        "  injecting carefully calibrated noise during training, we ensure that\n",
        "  no single data point (like one person's salary or one customer's\n",
        "  email) can be reliably extracted from the model, while the overall\n",
        "  learned patterns remain useful.\n",
        "\n",
        "\n",
        "However, applying differential privacy it requires retraining the\n",
        "  model from scratch with specialized algorithms. This is:\n",
        "  - **Computationally expensive** - Requires massive GPU resources\n",
        "  - **Time-intensive** - Training large models takes weeks or months\n",
        "  - **Access-limited** - Most organizations don't control model training\n",
        "\n",
        "**Our Solution: Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "Instead of retraining models with DP, we take a more practical\n",
        "  approach using Retrieval-Augmented Generation (RAG), where sensitive\n",
        "  data remains outside the model weights. The model never sees private\n",
        "  information during training - instead, it retrieves relevant context\n",
        "  at query time from a controlled, secure database.\n",
        "\n",
        "  Through complementary measures like:\n",
        "  - **Data anonymization** - Removing PII before indexing\n",
        "  - **Access control** - Filtering retrieval based on user permissions\n",
        "  - **Prompt Constraint** - Instructing the model how to use retrieved\n",
        "  information safely\n",
        "  - **Output filtering** - Redacting any sensitive details that slip\n",
        "  through\n",
        "\n",
        "  ...we keep private information secure while still enabling the model\n",
        "  to generate useful, context-aware responses.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "# Installing libraries\n",
        "!pip install openai pinecone tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "# Importing\n",
        "import json, re, math, uuid, ipaddress\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "# 2. Securing RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "\n",
        "## 2.1 Loading the Data\n",
        "\n",
        "We're loading the raw tickets dataset, which contains unprocessed support ticket data as it would appear in a production system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import json\n",
        "\n",
        "  # Load raw tickets\n",
        "  raw_tickets = []\n",
        "  with open('support_tickets_raw.jsonl', 'r') as f:\n",
        "      for line in f:\n",
        "          raw_tickets.append(json.loads(line))\n",
        "\n",
        "  # Load sanitized tickets\n",
        "  sanitized_tickets = []\n",
        "  with open('support_tickets_sanitized.jsonl', 'r') as f:\n",
        "      for line in f:\n",
        "          sanitized_tickets.append(json.loads(line))\n",
        "\n",
        "  print(f\"Loaded {len(raw_tickets)} raw tickets\")\n",
        "  print(f\"Loaded {len(sanitized_tickets)} sanitized tickets\")"
      ],
      "metadata": {
        "id": "Ku8x8DQ8YbfN"
      },
      "id": "Ku8x8DQ8YbfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine what this data looks like.\n",
        "\n",
        "Each ticket contains standard support information:\n",
        "  - Organizational metadata: org, department, severity, issue_type\n",
        "  - Ticket identifiers: ticket_id, customer_id, asset\n",
        "  - Contact information: contact_name, contact_email\n",
        "  - Technical details: account_number, source_ip, tags\n",
        "  - Content: body (initial report) and conversation (back-and-forth\n",
        "  messages)\n",
        "\n",
        "\n",
        "Notice the sensitive information scattered throughout this ticket:\n",
        "  - Personal names: \"Grace Williams\"\n",
        "  - Email addresses: grace.williams@ironpeakinsurance.com\n",
        "  - Account numbers: 74353833 (8-digit identifier)\n",
        "  - IP addresses: 233.168.89.205\n",
        "  - Asset identifiers: WS-1053 (potentially links to a specific\n",
        "  person's workstation)"
      ],
      "metadata": {
        "id": "Gf6TjCq9DZKf"
      },
      "id": "Gf6TjCq9DZKf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "raw_tickets[14]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were to index this data directly into a vector database and expose it through a RAG system, we'd face several risks such as privacy violations, compliance issues, data leakage, etc.\n",
        "\n",
        "In the next section, we'll tackle the first layer: Data anonymization."
      ],
      "metadata": {
        "id": "ExgrUYt5DtbU"
      },
      "id": "ExgrUYt5DtbU"
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "\n",
        "## 2.2 Data Anonymization\n",
        "\n",
        "The first step is protecting sensitive data by removing PII (Personally Identifiable Information) so that they never enter the system. PII includes details like names, emails, phone numbers, account numbers and IP addresses - anything that could identify a specific person. We will **replaces each sensitive element with a consistent placeholder**. This way, the data still retains enough structure to be useful for analysis and retrieval.\n",
        "\n",
        "This approach has two key benefits:\n",
        "1. **Risk reduction**: even if a query slips through access control or a model tries to reveal more than it should, the sensitive data simply isn’t there to give away.\n",
        "2. **Utility preservation**: placeholders remain consistent across documents, so the system can still link related tickets or detect patterns without exposing private details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "# Define the token formats we’ll use for each PII type\n",
        "PLACEHOLDER_FMT = {\n",
        "    \"email\":  \"EMAIL_{:04d}\",\n",
        " #   \"phone\":  \"PHONE_{:04d}\",\n",
        "    \"account\":\"ACCT_{:04d}\",\n",
        "    \"ip\":     \"IP_{:04d}\",\n",
        "    \"person\": \"PERSON_{:04d}\",\n",
        "}\n",
        "\n",
        "# Global token vault:\n",
        "# Keeps a STABLE, GLOBAL mapping: original value -> placeholder (per type)\n",
        "# Ensures the same email, etc. always maps to the same token across tickets\n",
        "class TokenVault:\n",
        "    \"\"\"\n",
        "    Maps original values -> stable placeholders, per type.\n",
        "    Example: vault.get_token(\"email\", \"alice@example.com\") -> \"{{EMAIL_0001}}\"\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self._maps: Dict[str, Dict[str, str]] = {t:{} for t in PLACEHOLDER_FMT}\n",
        "        self._counters: Dict[str, int] = {t:0 for t in PLACEHOLDER_FMT}\n",
        "\n",
        "    def _normalize_key(self, typ: str, value: str) -> str:\n",
        "        # Normalize by type (e.g., emails to lowercase)\n",
        "        v = value.strip()\n",
        "        if typ == \"email\":\n",
        "            v = v.lower()\n",
        "        elif typ == \"ip\":\n",
        "            v = v.strip()\n",
        "        elif typ == \"account\":\n",
        "            v = re.sub(r\"\\D\", \"\", v)\n",
        "        elif typ == \"person\":\n",
        "            v = re.sub(r\"\\s+\", \" \", v)\n",
        "        return v\n",
        "\n",
        "    def get_token(self, typ: str, value: str) -> str:\n",
        "        # Return the existing token for (typ,value) OR create a new one\n",
        "        # Final tokens are wrapped like \"{{EMAIL_0001}}\"\n",
        "        assert typ in PLACEHOLDER_FMT, f\"Unknown token type: {typ}\"\n",
        "        key = self._normalize_key(typ, value)\n",
        "        if not key:\n",
        "            return value\n",
        "        if key not in self._maps[typ]:\n",
        "            self._counters[typ] += 1\n",
        "            token = \"{{\" + PLACEHOLDER_FMT[typ].format(self._counters[typ]) + \"}}\"\n",
        "            self._maps[typ][key] = token\n",
        "        return self._maps[typ][key]\n",
        "\n",
        "    def stats(self) -> Dict[str, int]:\n",
        "        return {k: len(v) for k, v in self._maps.items()}\n",
        "\n",
        "\n",
        "# Regexes (free-text pass)\n",
        "# -------------------------\n",
        "# Patterns to find PII inside unstructured text fields (body, messages)\n",
        "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
        "# Long numbers that might be account IDs\n",
        "ACCOUNT_RE = re.compile(r\"\\b\\d{8,12}\\b\")  # generic long number (use with caution in free text)\n",
        "# IPv4 candidates in text. We’ll validate them with ipaddress before replacing.\n",
        "IP_CANDIDATE_RE = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
        "\n",
        "\n",
        "# Helpers\n",
        "# -------------------------\n",
        "# Find/replace emails in free text\n",
        "def replace_emails(text: str, vault: TokenVault) -> str:\n",
        "    return EMAIL_RE.sub(lambda m: vault.get_token(\"email\", m.group(0)), text)\n",
        "\n",
        "def replace_accounts_text(text: str, vault: TokenVault) -> str:\n",
        "    # Find/replace long numeric sequences as account IDs in free text.\n",
        "    def repl(m):\n",
        "        num = m.group(0)\n",
        "        return vault.get_token(\"account\", num)\n",
        "    return ACCOUNT_RE.sub(repl, text)\n",
        "\n",
        "def replace_ips_text(text: str, vault: TokenVault) -> str:\n",
        "    # Find/validate/replace IPv4 addresses in free text.\n",
        "    def repl(m):\n",
        "        cand = m.group(0)\n",
        "        try:\n",
        "            ipaddress.IPv4Address(cand)\n",
        "            return vault.get_token(\"ip\", cand)\n",
        "        except Exception:\n",
        "            return cand\n",
        "    return IP_CANDIDATE_RE.sub(repl, text)\n",
        "\n",
        "def anonymize_free_text(text: str, vault: TokenVault) -> str:\n",
        "    # Apply free-text anonymization\n",
        "    t = text\n",
        "    t = replace_emails(t, vault)\n",
        " #   t = replace_phones_text(t, vault)\n",
        "    t = replace_ips_text(t, vault)\n",
        "    t = replace_accounts_text(t, vault)\n",
        "    return t\n",
        "\n",
        "\n",
        "# Field-level anonymization\n",
        "# -------------------------\n",
        "STRUCTURED_FIELD_RULES = [\n",
        "    # Define which structured fields to anonymize, with an optional validator\n",
        "    ((\"contact_email\",), \"email\",  None),\n",
        "    ((\"account_number\",),\"account\", None),\n",
        "    ((\"source_ip\",),     \"ip\",     lambda s: _is_ipv4(s:=str(s))),\n",
        "    ((\"contact_name\",),  \"person\", None),\n",
        "]\n",
        "\n",
        "def _is_ipv4(s: str) -> bool:\n",
        "    # True if s is a valid IPv4 address\n",
        "    try:\n",
        "        ipaddress.IPv4Address(s)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def set_in(obj: Dict[str, Any], path: tuple, value: Any) -> None:\n",
        "    # Write a value at a dotted path in a dict (shallow)\n",
        "    d = obj\n",
        "    for p in path[:-1]:\n",
        "        d = d.get(p, {})\n",
        "    d[path[-1]] = value\n",
        "\n",
        "def get_in(obj: Dict[str, Any], path: tuple) -> Any:\n",
        "    # Read a value at a dotted path in a dict; return None if missing\n",
        "    d = obj\n",
        "    for p in path:\n",
        "        if not isinstance(d, dict) or p not in d:\n",
        "            return None\n",
        "        d = d[p]\n",
        "    return d\n",
        "\n",
        "def anonymize_ticket(ticket: Dict[str, Any], vault: TokenVault) -> Dict[str, Any]:\n",
        "    # Produce an anonymized copy of a single ticket:\n",
        "    # 1) Structured fields: precise tokenization using rules above.\n",
        "    # 2) Free-text fields: regex-based replacement (body + conversation messages)\n",
        "    t = json.loads(json.dumps(ticket))\n",
        "\n",
        "    # 1) Structured fields (email, account, IP, person)\n",
        "    for path, typ, validator in STRUCTURED_FIELD_RULES:\n",
        "        val = get_in(t, path)\n",
        "        if not isinstance(val, str):\n",
        "            continue\n",
        "        if validator is None or validator(val):\n",
        "            token = vault.get_token(typ, val)\n",
        "            set_in(t, path, token)\n",
        "\n",
        "    # 2) Free text fields (body)\n",
        "    if isinstance(t.get(\"body\"), str):\n",
        "        t[\"body\"] = anonymize_free_text(t[\"body\"], vault)\n",
        "\n",
        "    # 2b) Free text in conversation messages\n",
        "    conv = t.get(\"conversation\", [])\n",
        "    if isinstance(conv, list):\n",
        "        new_conv = []\n",
        "        for turn in conv:\n",
        "            if not isinstance(turn, dict):\n",
        "                new_conv.append(turn); continue\n",
        "            msg = turn.get(\"message\")\n",
        "            if isinstance(msg, str):\n",
        "                turn = dict(turn)\n",
        "                turn[\"message\"] = anonymize_free_text(msg, vault)\n",
        "            new_conv.append(turn)\n",
        "        t[\"conversation\"] = new_conv\n",
        "\n",
        "    return t\n",
        "\n",
        "def anonymize_corpus(tickets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    # Anonymize a list of tickets\n",
        "    vault = TokenVault()\n",
        "    out = [anonymize_ticket(t, vault) for t in tickets]\n",
        "    # You can inspect vault.stats() if you want counts per type\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "source": [
        "Now, we'll call `anonymize_corpus` function on \"raw_tickets\". This will return a new list of tickets where PII fields have been replaced with placeholders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "sanitized = anonymize_corpus(raw_tickets)\n",
        "out_path = Path(\"support_tickets_sanitized.jsonl\")\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for doc in sanitized:\n",
        "        f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote:\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "source": [
        "To persist the anonymized data, write it out in JSON Lines:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  sanitized_path = \"support_tickets_sanitized.jsonl\"\n",
        "\n",
        "  def load_jsonl(file_path):\n",
        "      \"\"\"Load JSONL file\"\"\"\n",
        "      with open(file_path, 'r', encoding='utf-8') as f:\n",
        "          return [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "  # Load the sanitized tickets\n",
        "  sanitized_tickets = load_jsonl(sanitized_path)\n",
        "\n",
        "  print(f\"✅ Loaded {len(sanitized_tickets)} sanitized tickets\")"
      ],
      "metadata": {
        "id": "WSYobXDPZreg"
      },
      "id": "WSYobXDPZreg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "source": [
        "And now let's load them back take a look at one support ticket (raw and sanitized) to confirm placeholders look right:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "# Raw ticket\n",
        "raw_tickets[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice what was sanitized:\n",
        "  - 'contact_name': '{{PERSON_0011}}' - Real name replaced\n",
        "  - 'contact_email': '{{EMAIL_0011}}' - Email anonymized\n",
        "  - 'account_number': '{{ACCT_0011}}' - Account number tokenized\n",
        "  - 'source_ip': '{{IP_0011}}' - IP address replaced"
      ],
      "metadata": {
        "id": "OtOulox2GVxz"
      },
      "id": "OtOulox2GVxz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "# Sanitized ticket\n",
        "sanitized_tickets[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "\n",
        "## 2.3 Access Control - Preparing Metadata\n",
        "\n",
        "The second layer is about **ensuring that users can only retrieve information they're authorized to see**.\n",
        "\n",
        "  Access control in RAG works differently than in traditional\n",
        "  databases. Instead of checking permissions after retrieval, **we embed\n",
        "  access rules directly into the metadata that travels with each\n",
        "  document chunk**. When a user queries the system, their permissions are\n",
        "   translated into filters that the vector database applies during the\n",
        "  search itself - so unauthorized data never even reaches the retrieval\n",
        "   stage.\n",
        "\n",
        "  Think of metadata as a set of security labels attached to each\n",
        "  ticket. These labels answer questions like:\n",
        "  - Who owns this data? (Which organization?)\n",
        "  - What department handles this? (HR, IT, Security?)\n",
        "  - Can customers see this? (Internal-only vs. customer-facing?)\n",
        "\n",
        "  At query time, we translate the user's permissions into database\n",
        "  filters. For example:\n",
        "\n",
        "\n",
        "1. **Tenant isolation** (multi-customer separation):\n",
        "  - Business rule: Users from Zephyr Telecom should never see tickets\n",
        "  from IronPeak Insurance\n",
        "  - How it works: Each ticket has an **org** field in metadata\n",
        "  - Filter logic: ticket.org == user.org\n",
        "  - Example: When a Zephyr Telecom employee queries the system, we add\n",
        "  a filter `{\"org\": \"Zephyr Telecom\"}` - Pinecone will only search within\n",
        "   Zephyr's tickets\n",
        "\n",
        "2. **Department scoping**\n",
        "  - Business rule: HR staff should only see HR-related tickets, not\n",
        "  Security or IT tickets\n",
        "  - How it works: Each ticket has a **department** field in metadata\n",
        "  - Filter logic: ticket.department IN user.allowed_departments\n",
        "  - Example: An HR employee's query includes filter `{\"department\":\n",
        "  {\"$in\": [\"HR\"]}}` - they can't accidentally retrieve Security incident\n",
        "   reports\n",
        "  \n",
        "3. **Visibility flags** (internal vs. customer-facing)\n",
        "  \n",
        "  - Business rule: External customers shouldn't see internal\n",
        "  troubleshooting notes\n",
        "  - How it works: Each ticket has a **customer_visible** boolean flag\n",
        "  - Filter logic: If user is a customer, filter ticket.customer_visible\n",
        "   == true\n",
        "  - Example: Internal analysts see everything. cĆustomers only see\n",
        "  tickets marked as customer-facing\n",
        "\n",
        " Let's attach these access-control labels to our sanitized tickets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "def add_metadata(sanitized_tickets):\n",
        "    enriched = []\n",
        "    for t in sanitized_tickets:\n",
        "        t2 = dict(t)  # making a copy\n",
        "        t2[\"metadata\"] = {\n",
        "            \"org\": t.get(\"org\"),\n",
        "            \"department\": t.get(\"department\"),\n",
        "            \"customer_visible\": True,\n",
        "            \"tags\": t.get(\"tags\", []),\n",
        "        }\n",
        "        enriched.append(t2)\n",
        "    return enriched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "# Adding metadata to sanitized tickets\n",
        "enriched_tickets = add_metadata(sanitized_tickets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Let's inspect a ticket after enrichment. Notice the new metadata section at the bottom of the ticket."
      ],
      "metadata": {
        "id": "lnGEpwjFGHLb"
      },
      "id": "lnGEpwjFGHLb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "outputs": [],
      "source": [
        "# Inspecting\n",
        "enriched_tickets[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "## 2.4 Preparing for Embeddings\n",
        "\n",
        "Now that our tickets are sanitized and enriched with metadata, the next step is to prepare them for embeddings. Instead of embedding an entire ticket as one big block of text, we’ll **break it into smaller, more meaningful pieces**.\n",
        "\n",
        "Each ticket will become multiple rows:\n",
        "- 1 row for the ticket body\n",
        "- 1 row for each message in the conversation\n",
        "\n",
        "Treating these smaller chunks as separate rows has two big benefits:\n",
        "1. it **improves retrieval accuracy**: we can return the exact part of a ticket that matches a query\n",
        "2. it **respects model limitations on input length**\n",
        "\n",
        "Along with the text, each row will carry its own metadata, such as organization, department and visibility, which will later let us enforce access control when querying the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "outputs": [],
      "source": [
        "## Break it into smaller, more meaningful pieces\n",
        "\n",
        "def ticket_to_rows(t):\n",
        "    rows = []\n",
        "    tid = t[\"ticket_id\"]\n",
        "\n",
        "    # this is the per-chunk \"catalog metadata\"\n",
        "    base_md = {\n",
        "        **t[\"metadata\"],            # org, department, tags, customer_visible\n",
        "        \"ticket_id\": tid,           # for provenance & catalog writes\n",
        "        \"created_at\": t.get(\"created_at\"),\n",
        "    }\n",
        "\n",
        "    body = (t.get(\"body\") or \"\").strip()\n",
        "    if body:\n",
        "        rows.append({\n",
        "            \"id\": f\"{tid}:body\",\n",
        "            \"text\": body,\n",
        "            \"metadata\": {**base_md, \"part\": \"body\"}\n",
        "        })\n",
        "\n",
        "    for i, turn in enumerate(t.get(\"conversation\", [])):\n",
        "        msg = (turn or {}).get(\"message\", \"\").strip()\n",
        "        if not msg:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"id\": f\"{tid}:conv:{i}\",\n",
        "            \"text\": msg,\n",
        "            \"metadata\": {**base_md, \"part\": \"conversation\", \"role\": turn.get(\"role\"), \"idx\": i}\n",
        "        })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "# Apply ticket_to_rows() function to every ticket in the dataset\n",
        "rows = [r for t in enriched_tickets for r in ticket_to_rows(t)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "# Peek at the first few rows\n",
        "for r in rows[:5]:\n",
        "    print(json.dumps(r, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's an important security benefit to this chunking strategy:\n",
        "  **fine-grained access control enforcement**. By splitting tickets into\n",
        "  individual message-level chunks, each with its own metadata, we\n",
        "  ensure that access control filters apply to every single piece of\n",
        "  content independently.\n",
        "\n",
        "  For example, if a support conversation contains both customer\n",
        "  messages and internal notes, we could theoretically mark them with\n",
        "  different customer_visible flags. When a customer queries the system,\n",
        "   they'd only retrieve their own messages, not the internal\n",
        "  discussion. While our current dataset marks everything as\n",
        "  customer-visible for simplicity, this architecture supports much more\n",
        "   granular policies in production systems.\n",
        "\n",
        "  Additionally, smaller chunks mean **less context leakage** - if a query\n",
        "  retrieves 3 conversation turns instead of an entire 20-message\n",
        "  ticket, there's less chance of accidentally exposing unrelated\n",
        "  sensitive information through the LLM's response."
      ],
      "metadata": {
        "id": "NOhDq2KpHgv5"
      },
      "id": "NOhDq2KpHgv5"
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "## 2.5 Creating Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "source": [
        "Now that we have our chunks prepared with their metadata, we need to convert the text into embeddings. We will use embedding model `all-MiniLM-L6-v2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device = \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We batch-encode all our chunks at once for efficiency. The model processes 64 chunks at a time."
      ],
      "metadata": {
        "id": "fNtMHKueII7Q"
      },
      "id": "fNtMHKueII7Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "texts = [r[\"text\"] for r in rows]\n",
        "\n",
        "# Embedding in batches\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size = 64,\n",
        "    show_progress_bar = True,\n",
        "    convert_to_numpy = True,\n",
        "    normalize_embeddings = True\n",
        ")\n",
        "\n",
        "# Attaching embeddings back to rows as Python lists\n",
        "for r, vec in zip(rows, embeddings):\n",
        "    r[\"values\"] = vec.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "vectors = [\n",
        "    {\"id\": r[\"id\"], \"values\": r[\"values\"], \"metadata\": r[\"metadata\"]}\n",
        "    for r in rows\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row contains:\n",
        "  - \"id\": Unique identifier\n",
        "  - \"metadata\": Access control labels (org, department, etc.)\n",
        "  - \"values\": 384-dimensional embedding vector"
      ],
      "metadata": {
        "id": "HG2kUtPzIhmL"
      },
      "id": "HG2kUtPzIhmL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "vectors[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "## 2.6 Upserting Data into Pinecone\n",
        "\n",
        "Now we'll upload the embeddings into Pinecone:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Load API key from Google Colab secrets\n",
        "pinecone_client = Pinecone(api_key=userdata.get('PINECONE_API_KEY'))"
      ],
      "metadata": {
        "id": "fTuXMSVr4daN"
      },
      "id": "fTuXMSVr4daN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "# Creating an Index\n",
        "pinecone_client.create_index(name = \"support-tickets-demo\",\n",
        "                             dimension = 384,\n",
        "                             metric = \"cosine\",\n",
        "                             spec = ServerlessSpec(\n",
        "                                 cloud = \"aws\",\n",
        "                                 region = \"us-east-1\"\n",
        "                             ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "outputs": [],
      "source": [
        "index = pinecone_client.Index(\"support-tickets-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We upload our vectors in batches of 200 to avoid overwhelming the API:"
      ],
      "metadata": {
        "id": "0nTLaNnMJaGg"
      },
      "id": "0nTLaNnMJaGg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": [],
      "source": [
        "BATCH = 200\n",
        "total = len(vectors)\n",
        "\n",
        "for i in range(0, total, BATCH):\n",
        "    batch = vectors[i:i+BATCH]\n",
        "    index.upsert(vectors=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "### 2.6.1 Storing Raw Data with SQLite\n",
        "\n",
        "In production, the copy of our documents would be in a primary data store, usually a relational database like Postgres/MySQL or object storage (S3/GCS) referenced from a small catalog table. Pinecone holds only the embeddings plus minimal filterable metadata and a pointer back to the source.\n",
        "\n",
        "In this demo we use `sqlite3` to create a simple local data store.\n",
        "\n",
        "Let's understand how LLM will actually use it:\n",
        "1. We'll query Pinecone with ACL filters and get back chunk IDs/pointers.\n",
        "2. We'll fetch the raw text for those IDs from sqlite3.\n",
        "3. We'll pass only those snippets into the LLM to compose the answer.\n",
        "\n",
        "So this sets us up for the next section of this notebook - constraining the model - where we instruct the LLM to answer strictly from those snippets.\n",
        "\n",
        "Let's create a content catalog which stores the raw text for each chunk (plus basic provenance like ticket ID, org, department and timestamps):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "CATALOG_PATH = Path(\"chunk_catalog.sqlite\")\n",
        "\n",
        "def init_catalog(db_path=CATALOG_PATH):\n",
        "    # Open (or create) the SQLite database file\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.executescript(\"\"\"\n",
        "    PRAGMA journal_mode = WAL;\n",
        "    PRAGMA synchronous = NORMAL;\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS chunks (\n",
        "      id         TEXT PRIMARY KEY,   -- e.g. \"87428682:conv:2\"\n",
        "      ticket_id  TEXT,\n",
        "      part       TEXT,               -- 'body' | 'conversation'\n",
        "      idx        INTEGER,            -- turn index for conversation\n",
        "      org        TEXT,\n",
        "      department TEXT,\n",
        "      created_at TEXT,\n",
        "      text       TEXT                -- raw chunk text lives here (not in Pinecone)\n",
        "    );\n",
        "\n",
        "    CREATE INDEX IF NOT EXISTS idx_chunks_ticket ON chunks(ticket_id);\n",
        "    CREATE INDEX IF NOT EXISTS idx_chunks_org_dept ON chunks(org, department);\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def write_chunks_to_catalog(rows, db_path=CATALOG_PATH):\n",
        "    # Insert/overwrite chunk rows produced by ticket_to_rows()\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    data = []\n",
        "    for r in rows:\n",
        "        md = r[\"metadata\"]\n",
        "        data.append((\n",
        "            r[\"id\"],\n",
        "            md.get(\"ticket_id\"),\n",
        "            md.get(\"part\"),\n",
        "            md.get(\"idx\"),\n",
        "            md.get(\"org\"),\n",
        "            md.get(\"department\"),\n",
        "            md.get(\"created_at\"),\n",
        "            r[\"text\"],\n",
        "        ))\n",
        "    cur.executemany(\n",
        "        \"INSERT OR REPLACE INTO chunks (id, ticket_id, part, idx, org, department, created_at, text) VALUES (?,?,?,?,?,?,?,?)\",\n",
        "        data\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Initialization + load the current batch of chunks\n",
        "init_catalog()\n",
        "write_chunks_to_catalog(rows)\n",
        "print(\"Catalog ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "Before we run real queries, we sanity-check the vector index: confirm the index is ready and try fetching a single known vector by ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "# Index readiness\n",
        "desc = pinecone_client.describe_index(\"support-tickets-demo\")\n",
        "print(\"Index ready:\", desc.status.get(\"ready\"))\n",
        "\n",
        "probe_id = rows[0][\"id\"]\n",
        "fetched = index.fetch(ids=[probe_id])\n",
        "\n",
        "exists = probe_id in (fetched.vectors or {})\n",
        "print(\"Fetch exists in Pinecone:\", exists)\n",
        "\n",
        "if exists:\n",
        "    vec_obj = fetched.vectors[probe_id]\n",
        "    # Values\n",
        "    vals = vec_obj.values\n",
        "    print(\"Dimensions:\", len(vals))\n",
        "\n",
        "    # Metadata\n",
        "    md = vec_obj.metadata\n",
        "    print(\"Metadata:\", md)\n",
        "\n",
        "    # ACL filters\n",
        "    if md:\n",
        "        print(\"organization:\", md.get(\"org\"), \"| department:\", md.get(\"department\"),\n",
        "              \"| part:\", md.get(\"part\"), \"| idx:\", md.get(\"idx\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "# Check some organization/department we expect to exist\n",
        "conn = sqlite3.connect(CATALOG_PATH)\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT COUNT(*) FROM chunks WHERE org=? AND department=?\", (\"Zephyr Telecom\", \"Security\"))\n",
        "print(\"Zephyr/Security chunks:\", cursor.fetchone()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "## 2.7 Querying the Database - Testing ACL Filtering\n",
        "\n",
        "Now it's time to test that our access control layer is working correctly before we add the LLM.\n",
        "\n",
        " In this section we're not generating answers yet - we need to verify that:\n",
        "  1. ✅ Different users see different data based on their organization\n",
        "  2. ✅ Filters are applied at the database level (not after retrieval)\n",
        "  3. ✅ The metadata we added in Section 2.3 successfully controls what\n",
        "   gets retrieved\n",
        "\n",
        "We'll ask the same question twice, but pretend to be users from two different organizations:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": [],
      "source": [
        "# TEST 1: User from Zephyr Telecom\n",
        "user_org = \"Zephyr Telecom\"\n",
        "allowed_depts = {\"Security\"}\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "We'll encode the question into an embedding with the same model used for our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "# Encoding the question\n",
        "question = \"Okta group membership was wrong; what was the fix?\"\n",
        "q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "Finally, we query the Pinecone with `acl_filter`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "# Querying\n",
        "res = index.query(\n",
        "    vector=q_vec,\n",
        "    top_k=8,\n",
        "    include_metadata=True,\n",
        "    filter=acl_filter\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "source": [
        "Expected result is that only Zephyr Telecom's Security tickets are returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "outputs": [],
      "source": [
        "# Returned matches\n",
        "matches = res.get(\"matches\", [])\n",
        "print(f\"Got {len(matches)} match(es).\")\n",
        "for m in matches:\n",
        "    mid = m[\"id\"]\n",
        "    score = m[\"score\"]\n",
        "    md = m.get(\"metadata\", {})\n",
        "    print(f\"{mid:25s}  score={score:.3f}  org={md.get('org')}  dept={md.get('department')}  part={md.get('part')}  role={md.get('role')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "source": [
        "We can try the same query with a different organization in the filter, such as \"BlueShield Bank\". Pinecone returned different chunks from BlueShield Bank tickets. That means our ACL filter is working: we’re not seeing Zephyr data anymore. We’re seeing BlueShield’s own relevant tickets about Okta-like issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "outputs": [],
      "source": [
        "# TEST 2: User from BlueShield Bank\n",
        "user_org = \"BlueShield Bank\"\n",
        "allowed_depts = {\"Security\"}  # tweak to test\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Encoding the question\n",
        "question = \"Okta group membership was wrong; what was the fix?\"\n",
        "q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()\n",
        "\n",
        "# Querying\n",
        "res = index.query(\n",
        "    vector=q_vec,\n",
        "    top_k=8,\n",
        "    include_metadata=True,\n",
        "    filter=acl_filter\n",
        ")\n",
        "\n",
        "# Returned matches\n",
        "matches = res.get(\"matches\", [])\n",
        "print(f\"Got {len(matches)} match(es).\")\n",
        "for m in matches:\n",
        "    mid = m[\"id\"]\n",
        "    score = m[\"score\"]\n",
        "    md = m.get(\"metadata\", {})\n",
        "    print(f\"{mid:25s}  score={score:.3f}  org={md.get('org')}  dept={md.get('department')}  part={md.get('part')}  role={md.get('role')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "\n",
        "## 2.8 Prompt Constraints\n",
        "\n",
        "We've now confirmed that our access control filters work - users only retrieve data they're authorized to see. But there's still a risk: what if the LLM invents information, quotes sensitive details verbatim, or generates content beyond what's in our retrieved chunks?\n",
        "\n",
        "This is where Layer 3: Prompt Constraints (the \"Chaperone\") comes in. **We'll tell the LLM exactly how it's allowed to behave through strict instructions in the system prompt.**\n",
        "\n",
        "Why Constraint Matters:\n",
        "\n",
        "  Without constraints, an LLM might:\n",
        "  - ❌ Hallucinate plausible-sounding but false information\n",
        "  - ❌ Quote anonymized placeholders like {{EMAIL_0011}} verbatim\n",
        "  (looks odd to users)\n",
        "  - ❌ Mix retrieved facts with its training knowledge (breaking trust)\n",
        "  - ❌ Fail to cite sources (making answers unverifiable)\n",
        "\n",
        "  With constraints, we guide the model to:\n",
        "  - ✅ Answer only from retrieved snippets\n",
        "  - ✅ Summarize naturally (don't quote placeholder tokens)\n",
        "  - ✅ Avoid mentioning any PII or specific identifiers\n",
        "  - ✅ Cite sources by number (e.g., [1], [2])\n",
        "  - ✅ Admit when evidence is insufficient\n",
        "\n",
        "\n",
        "**Building the Context Pipeline**\n",
        "\n",
        "Before we can constrain the LLM, we need to prepare the retrieved chunks into a clean, numbered format that the model can reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "source": [
        "**Step 1: Fetch the raw text**\n",
        "\n",
        "We have chunk IDs from Pinecone - now we fetch their actual text from SQLite:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57",
      "metadata": {
        "id": "57"
      },
      "outputs": [],
      "source": [
        "CATALOG_PATH = Path(\"chunk_catalog.sqlite\")\n",
        "\n",
        "def fetch_texts_by_ids(ids, db_path=CATALOG_PATH):\n",
        "    # Fetch raw chunk text for a list of chunk IDs, preserving input order.\"\"\"\n",
        "    if not ids:\n",
        "        return {}\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    qmarks = \",\".join(\"?\" * len(ids))\n",
        "    cur.execute(f\"SELECT id, text FROM chunks WHERE id IN ({qmarks})\", ids)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    # Map found ids -> text\n",
        "    found = dict(rows)\n",
        "    # Preserve Pinecone order; missing ids map to \"\"\n",
        "    return {i: found.get(i, \"\") for i in ids}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58",
      "metadata": {
        "id": "58"
      },
      "source": [
        "**Step 2: Format chunks with labels**\n",
        "\n",
        "Next, we turn each chunk into a numbered snippet that the LLM can cite.\n",
        "\n",
        "We'll create a function `build_context_from_matches()` that turns each Pinecone match into a small, numbered snippet such as `[1 | f5bfddc9:body]\\n<text>`. The numbers preserve relevance order and give the model (and us) stable citation handles like `[1], [2]` to reference in the answer. We trim long snippets to keep the prompt compact so the LLM focuses on the most useful details, and we skip any IDs that didn’t resolve to text to avoid noise.\n",
        "\n",
        "The result is a clean context: answers can cite exactly which chunk supported each claim, making the system more explainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59",
      "metadata": {
        "id": "59"
      },
      "outputs": [],
      "source": [
        "def build_context_from_matches(matches, id_to_text, max_chars=4000):\n",
        "    \"\"\"\n",
        "    Create labeled context blocks like:\n",
        "    [1 | f5bfddc9:body]\n",
        "    <snippet>\n",
        "\n",
        "    Returns (context_str, used_ids)\n",
        "    \"\"\"\n",
        "    blocks = []\n",
        "    used_ids = []\n",
        "    total = 0\n",
        "    for i, m in enumerate(matches, start=1):\n",
        "        cid = m[\"id\"]\n",
        "        txt = (id_to_text.get(cid) or \"\").strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        # Light truncation to avoid oversized prompts\n",
        "        snippet = txt if len(txt) <= 800 else (txt[:800] + \" …\")\n",
        "        block = f\"[{i} | {cid}]\\n{snippet}\"\n",
        "        if total + len(block) + 2 > max_chars:\n",
        "            break\n",
        "        blocks.append(block)\n",
        "        used_ids.append(cid)\n",
        "        total += len(block) + 2\n",
        "    return \"\\n\\n\".join(blocks), used_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60",
      "metadata": {
        "id": "60"
      },
      "source": [
        "**The Constrained Answer Function**\n",
        "\n",
        "Now we create the function that actually calls the LLM with strict rules.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configure OpenAI API key\n",
        "OPENAI_API_KEY = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY:\n",
        "        print('✅ API key loaded from Colab secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        print('💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY')\n",
        "        OPENAI_API_KEY = getpass('Enter your OpenAI API Key: ')\n",
        "    except Exception as exc:\n",
        "        raise ValueError('❌ ERROR: No API key provided! Set OPENAI_API_KEY as an environment variable or Colab secret.') from exc\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == '':\n",
        "    raise ValueError('❌ ERROR: No API key provided!')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print('✅ Authentication configured!')\n",
        "\n",
        "OPENAI_MODEL = 'gpt-5-nano'  # Using gpt-5-nano for cost efficiency\n",
        "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n"
      ],
      "metadata": {
        "id": "M1Zz2lBz5-tk"
      },
      "id": "M1Zz2lBz5-tk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rules for a model are simple:\n",
        "  - 🔒 Scope limitation: \"Answer ONLY using the provided context\n",
        "  snippets\"\n",
        "  - 🔒 Style guidance: \"Summarize; do not quote verbatim\" (avoids\n",
        "  echoing {{EMAIL_0011}})\n",
        "  - 🔒 PII protection: Explicit list of what NOT to include\n",
        "  - 🔒 Citation requirement: Forces transparency with numbered\n",
        "  references\n",
        "  - 🔒 Fallback behavior: If context is insufficient, say so (no\n",
        "  hallucination)\n"
      ],
      "metadata": {
        "id": "NBBi7Qk_OgpB"
      },
      "id": "NBBi7Qk_OgpB"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def answer_with_constrained_model(question, context):\n",
        "    if not context.strip():\n",
        "        return (\n",
        "            \"No eligible context was found for this user. \"\n",
        "            \"Please refine the query or check permissions.\"\n",
        "        )\n",
        "\n",
        "    resp = client.responses.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        input=(\n",
        "            \"You are a helpful security support assistant. \"\n",
        "            \"Answer ONLY using the provided context snippets. \"\n",
        "            \"Summarize; do not quote verbatim. \"\n",
        "            \"Do NOT include PII or specifics (no names, emails, phone numbers, \"\n",
        "            \"IPs, account numbers, dates, project codes, or dollar figures). \"\n",
        "            \"If specifics are essential, use generic placeholders like [REDACTED]. \"\n",
        "            \"If the context is insufficient, say so and suggest safe next steps.\\n\\n\"\n",
        "            f\"SOURCES:\\n{context}\\n\\n\"\n",
        "            f\"QUESTION: {question}\\n\\n\"\n",
        "            \"Answer in 3-5 sentences and cite sources by their bracket labels, e.g., [1], [2].\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return resp.output_text\n"
      ],
      "metadata": {
        "id": "3Ljdhjis6r0u"
      },
      "id": "3Ljdhjis6r0u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "62",
      "metadata": {
        "id": "62"
      },
      "source": [
        "**Testing the Constrained Pipeline**\n",
        "\n",
        "Let's see the full workflow in action. We'll simulate a Zephyr Telecom Security user asking about a phishing incident:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63",
      "metadata": {
        "id": "63"
      },
      "outputs": [],
      "source": [
        "# Step 1: Set up access control for the user\n",
        "user_org = \"Zephyr Telecom\"\n",
        "allowed_depts = {\"Security\"}\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64",
      "metadata": {
        "id": "64"
      },
      "source": [
        "Then we turn the question into an embedding and run a similarity search, asking Pinecone to return metadata and applying the ACL filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65",
      "metadata": {
        "id": "65"
      },
      "outputs": [],
      "source": [
        "# Step 2: Encode the question and search Pinecone (with ACL filters applied)\n",
        "\n",
        "# User question\n",
        "question = \"I clicked a phishing link and my endpoint is acting weird\"\n",
        "\n",
        "# Encoding\n",
        "q_vec = model.encode([question], convert_to_numpy = True, normalize_embeddings = False)[0].tolist()\n",
        "\n",
        "# Querying\n",
        "res = index.query(\n",
        "    vector = q_vec,\n",
        "    top_k = 8,\n",
        "    include_metadata = True,\n",
        "    filter = acl_filter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Let's display adn verify what organizations/departments were actually retrieved\n",
        "print(\"=\" * 80)\n",
        "print(\"RETRIEVAL RESULTS - Access Control Verification\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "matches = res.get(\"matches\", [])\n",
        "print(f\"Total matches found: {len(matches)}\\n\")\n",
        "\n",
        "if matches:\n",
        "    # Show organization and department for each match\n",
        "    print(\"Retrieved chunks (verifying ACL filtering):\")\n",
        "    for i, m in enumerate(matches, 1):\n",
        "        md = m.get(\"metadata\", {})\n",
        "        print(\n",
        "            f\"  [{i}] {m['id']:25s} | \"\n",
        "            f\"Org: {md.get('org', 'N/A'):20s} | \"\n",
        "            f\"Dept: {md.get('department', 'N/A'):10s} | \"\n",
        "            f\"Score: {m['score']:.3f}\"\n",
        "        )\n",
        "\n",
        "    # Verify all are from the expected org/dept\n",
        "    orgs = set(m.get(\"metadata\", {}).get(\"org\") for m in matches)\n",
        "    depts = set(m.get(\"metadata\", {}).get(\"department\") for m in matches)\n",
        "\n",
        "    print(f\"\\n✅ All retrieved chunks are from:\")\n",
        "    print(f\"   Organizations: {orgs}\")\n",
        "    print(f\"   Departments: {depts}\")\n",
        "    print(f\"   (Expected: {user_org} / {allowed_depts})\")\n",
        "else:\n",
        "    print(\"⚠️ No matches found with the given ACL filter\")\n",
        "\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Step 3: Build the numbered context from matches\n",
        "ids = [m[\"id\"] for m in matches]\n"
      ],
      "metadata": {
        "id": "XYpXGrCtQKTP"
      },
      "id": "XYpXGrCtQKTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "66",
      "metadata": {
        "id": "66"
      },
      "source": [
        "Finally, let's put that together: matches → context → answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67",
      "metadata": {
        "id": "67"
      },
      "outputs": [],
      "source": [
        "# Step 3: Build the numbered context from matches\n",
        "\n",
        "# We grab the returned chunk IDs in ranked order. These are just pointers, they don’t contain the raw text.\n",
        "matches = res.get(\"matches\", [])\n",
        "ids = [m[\"id\"] for m in matches]\n",
        "\n",
        "# Fetch raw text for those IDs\n",
        "id_to_text = fetch_texts_by_ids(ids)\n",
        "\n",
        "# Build labeled, trimmed context blocks\n",
        "context, used_ids = build_context_from_matches(matches, id_to_text, max_chars=4000)\n",
        "print(\"Context used:\\n\", context[:600], \"...\\n\",\"\\n-----------------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate a constrained answer\n",
        "\n",
        "# Ask the model under strict rules\n",
        "final_answer = answer_with_constrained_model(question, context)\n",
        "print(\"Model's answer: \",\"\\n\", final_answer)"
      ],
      "metadata": {
        "id": "C0FdYqMpPAYW"
      },
      "id": "C0FdYqMpPAYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens here:\n",
        "\n",
        "  1. ACL filtering ensures only Zephyr Security tickets are searched\n",
        "  2. Semantic search finds the 8 most relevant chunks about\n",
        "  phishing/endpoint issues\n",
        "  3. Context building formats them with citation numbers\n",
        "  4. Constrained LLM generates an answer that:\n",
        "    - Cites specific chunks (e.g., \"According to [1] and [3]...\")\n",
        "    - Summarizes the fix steps without exposing PII\n",
        "    - Stays within the boundaries of retrieved content\n",
        "    - Provides actionable guidance\n",
        "\n",
        "Even with these constraints, there's still a small risk that something slips through. In Section 2.9, we'll add Layer 4 (Output Filtering) as a final safety net."
      ],
      "metadata": {
        "id": "jx_y7JVuPF9K"
      },
      "id": "jx_y7JVuPF9K"
    },
    {
      "cell_type": "markdown",
      "id": "68",
      "metadata": {
        "id": "68"
      },
      "source": [
        "## 2.9 Output Filtering and Guardrails\n",
        "\n",
        "\n",
        "The last layer scans the model's output just before showing it to the user and redacts any suspicious patterns that match PII formats.\n",
        "\n",
        "We use regex patterns to catch:\n",
        "  - Emails: user@example.com → [REDACTED_EMAIL]\n",
        "  - Account numbers: 8-12 digit sequences → [REDACTED_ACCOUNT]\n",
        "  - IP addresses: IPv4/IPv6 → [REDACTED_IP]\n",
        "  - Customer IDs: CUST-2407 → [REDACTED_CUSTOMER_ID]\n",
        "  - Asset IDs: WS-1054 → [REDACTED_ASSET_ID]\n",
        "  - Names: Two capitalized words (heuristic) → [REDACTED_NAME]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69",
      "metadata": {
        "id": "69"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Emails\n",
        "EMAIL_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "\n",
        "# 8–12 digit account-like numbers\n",
        "ACCOUNT_RE = re.compile(r'(?<!\\d)\\d{8,12}(?!\\d)')\n",
        "\n",
        "# Customer IDs like CUST-2407\n",
        "CUSTOMER_ID_RE = re.compile(r'\\bCUST-\\d{3,6}\\b', re.IGNORECASE)\n",
        "\n",
        "# Asset IDs like WS-1054 (two letters + dash + 3–6 digits)\n",
        "ASSET_ID_RE = re.compile(r'\\b[A-Z]{2,4}-\\d{3,6}\\b')\n",
        "\n",
        "# IPv4 0–255 per octet\n",
        "IPV4_RE = re.compile(\n",
        "    r'\\b(?:(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\.){3}'\n",
        "    r'(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\b'\n",
        ")\n",
        "\n",
        "# Basic IPv6 (not exhaustive but useful)\n",
        "IPV6_RE = re.compile(r'\\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\\b')\n",
        "\n",
        "# Very light \"name-like\" pattern (two capitalized words).\n",
        "NAME_RE = re.compile(r'\\b([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\b')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70",
      "metadata": {
        "id": "70"
      },
      "source": [
        "  ⚠️ Note on NAME_RE: This pattern catches two consecutive capitalized\n",
        "  words, which is a simple heuristic for person names. However, it can\n",
        "  also match non-person phrases like \"Active Directory\" or \"Conditional\n",
        "   Access\". To reduce false positives, you could:\n",
        "  - Add a whitelist of common technical terms to exclude\n",
        "  - Only apply name detection when \"person cues\" appear nearby (e.g.,\n",
        "  \"contact\", \"reported by\")\n",
        "  - Use a lightweight NER model like spaCy for more accurate person\n",
        "  detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71",
      "metadata": {
        "id": "71"
      },
      "outputs": [],
      "source": [
        " def redact_output(output: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace PII-ish patterns in the model's output with placeholders.\n",
        "    Designed around our dataset: emails, 8–12 digit accounts, IPs, customer & asset IDs,\n",
        "    and (optionally) name-like strings.\n",
        "    \"\"\"\n",
        "    s = output\n",
        "\n",
        "    # Order can reduce false positives: more specific first, then broader\n",
        "    s = EMAIL_RE.sub('[REDACTED_EMAIL]', s)\n",
        "    s = IPV4_RE.sub('[REDACTED_IP]', s)\n",
        "    s = IPV6_RE.sub('[REDACTED_IP]', s)\n",
        "    s = CUSTOMER_ID_RE.sub('[REDACTED_CUSTOMER_ID]', s)\n",
        "    s = ASSET_ID_RE.sub('[REDACTED_ASSET_ID]', s)\n",
        "    s = ACCOUNT_RE.sub('[REDACTED_ACCOUNT]', s)\n",
        "    s = NAME_RE.sub('[REDACTED_NAME]', s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why the order matters: We apply specific patterns (like CUSTOMER_ID_RE) before generic ones (like ACCOUNT_RE) to avoid over-redaction. For example, \"CUST-2407\" should become [REDACTED_CUSTOMER_ID], not get partially matched by the account number pattern."
      ],
      "metadata": {
        "id": "wttPWiIXR4xU"
      },
      "id": "wttPWiIXR4xU"
    },
    {
      "cell_type": "markdown",
      "id": "72",
      "metadata": {
        "id": "72"
      },
      "source": [
        "\n",
        "\n",
        "Now we tie all four layers together into a single function.\n",
        "\n",
        "  What happens behind the scenes:\n",
        "\n",
        "  1. Layer 1 (Anonymization): The indexed data already has PII replaced\n",
        "   with tokens like {{EMAIL_0011}}\n",
        "  2. Layer 2 (ACL): Only Zephyr Telecom Security tickets are searched\n",
        "  3. Layer 3 (Constraints): The LLM summarizes without quoting exact\n",
        "  tokens, cites sources by number\n",
        "  4. Layer 4 (Redaction): If any PII-like patterns appear in the answer\n",
        "   (e.g., the model says \"Contact john.doe@example.com\"), they're\n",
        "  caught and replaced with [REDACTED_EMAIL].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73",
      "metadata": {
        "id": "73"
      },
      "outputs": [],
      "source": [
        "def secure_rag_answer(\n",
        "    *,\n",
        "    question: str,\n",
        "    index,\n",
        "    model,\n",
        "    acl_filter: dict,\n",
        "    top_k: int = 8,\n",
        "    namespace: str | None = None,\n",
        "    max_context_chars: int = 4000,\n",
        "):\n",
        "    \"\"\"\n",
        "    End-to-end solution: ACL-filtered search -> fetch texts -> labeled context -> constrained LLM -> redaction\n",
        "    Returns (context, safe_answer)\n",
        "    \"\"\"\n",
        "    # 1) Encode and query Pinecone with ACLs\n",
        "    q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()\n",
        "    res = index.query(\n",
        "        vector = q_vec,\n",
        "        top_k = top_k,\n",
        "        include_metadata = True,\n",
        "        filter = acl_filter,\n",
        "        **({\"namespace\": namespace} if namespace is not None else {})\n",
        "    )\n",
        "    matches = res.get(\"matches\", [])\n",
        "    ids = [m[\"id\"] for m in matches]\n",
        "\n",
        "    # 2) Fetch raw text from SQLite and build labeled context\n",
        "    id_to_text = fetch_texts_by_ids(ids)\n",
        "    context, _used_ids = build_context_from_matches(matches, id_to_text, max_chars=max_context_chars)\n",
        "\n",
        "    # 3) Constrained answer, then redact\n",
        "    draft = answer_with_constrained_model(question, context)\n",
        "    safe_answer = redact_output(draft)\n",
        "\n",
        "    return context, safe_answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Complete Pipeline**\n",
        "\n",
        "Let's see all four layers working together."
      ],
      "metadata": {
        "id": "5MwV22iwSgGw"
      },
      "id": "5MwV22iwSgGw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74",
      "metadata": {
        "id": "74"
      },
      "outputs": [],
      "source": [
        "# User question\n",
        "question = \"I clicked a phishing link and my endpoint is acting weird\"\n",
        "\n",
        "context, safe_answer = secure_rag_answer(\n",
        "    question = question,\n",
        "    index = index,\n",
        "    model = model,\n",
        "    acl_filter = acl_filter,\n",
        "    top_k = 8,\n",
        "    namespace = None,\n",
        "    max_context_chars = 4000\n",
        ")\n",
        "\n",
        "print(\"Context used:\\n\", context, \"...\\n\",\"\\n-----------------\\n\")\n",
        "print(\"Model's answer:\\n\", safe_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75",
      "metadata": {
        "id": "75"
      },
      "outputs": [],
      "source": [
        "# User question\n",
        "question = \"Hi, my workstation popped up a malware alert from the endpoint agent.\"\n",
        "\n",
        "context, safe_answer = secure_rag_answer(\n",
        "    question = question,\n",
        "    index = index,\n",
        "    model = model,\n",
        "    acl_filter = acl_filter,\n",
        "    top_k = 8,\n",
        "    namespace = None,\n",
        "    max_context_chars = 4000\n",
        ")\n",
        "\n",
        "print(\"Context used:\\n\", context, \"...\\n\",\"\\n-----------------\\n\")\n",
        "print(\"Model's answer:\\n\", safe_answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l84KwN9T76j3"
      },
      "id": "l84KwN9T76j3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}