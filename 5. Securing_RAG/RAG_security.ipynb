{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "\n",
        "# RAG Security Lab: Memorization vs RAG, and 4 Layers of Protection\n",
        "\n",
        "**You’ll learn to:**\n",
        "- Explain LLM **memorization/regurgitation** at a high level, and why **Differential Privacy** (DP) is used to mitigate it.\n",
        "- Build a small **RAG** system over a mock support-ticket dataset.\n",
        "- Add **four security layers** to RAG:\n",
        "  1) Access Control (the *Bouncer*)\n",
        "  2) Data Anonymization (the *Sanitizer*)\n",
        "  3) Prompt Constraints (the *Chaperone*)\n",
        "  4) Output Filtering (the *Redactor*)\n",
        "\n",
        "> This notebook includes a pure-Python in-memory vector index so it runs offline for class. Swap in Pinecone in the exercises to connect to a real vector DB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "\n",
        "# 1. Model Memorization & Differential Privacy\n",
        "\n",
        "- Training compresses huge corpora into weights. That *can* lead to **memorization** of rare/private strings that later leak.\n",
        "- **Differential Privacy (DP)** adds calibrated randomness (noise) during training so models learn **patterns**, not exact **records**.\n",
        "\n",
        "**Salary analogy:** everyone adds a private random offset to their salary before sharing. The average remains accurate, but no individual's value is exposed.\n",
        "\n",
        "In this lab, we won’t retrain an LLM. Instead, we’ll focus on **RAG**, which keeps private data **outside** the model weights and behind a **policy wall**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "If running locally, create a fresh virtual environment and install packages as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "#pip install -U pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "# If you want to use real embeddings/LLM later, uncomment and install:\n",
        "!pip install openai pinecone tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "# 2. Securing RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import json, re, math, uuid, ipaddress\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "\n",
        "## 2.1 Loading the Data\n",
        "\n",
        "We provide **raw** tickets (with PII) and an **anonymized** copy to demonstrate the Sanitizer pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import json\n",
        "\n",
        "  # Load raw tickets\n",
        "  raw_tickets = []\n",
        "  with open('support_tickets_raw.jsonl', 'r') as f:\n",
        "      for line in f:\n",
        "          raw_tickets.append(json.loads(line))\n",
        "\n",
        "  # Load sanitized tickets\n",
        "  sanitized_tickets = []\n",
        "  with open('support_tickets_sanitized.jsonl', 'r') as f:\n",
        "      for line in f:\n",
        "          sanitized_tickets.append(json.loads(line))\n",
        "\n",
        "  print(f\"Loaded {len(raw_tickets)} raw tickets\")\n",
        "  print(f\"Loaded {len(sanitized_tickets)} sanitized tickets\")"
      ],
      "metadata": {
        "id": "Ku8x8DQ8YbfN"
      },
      "id": "Ku8x8DQ8YbfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#DATA_DIR = Path(\"data\")\n",
        "#raw_path = DATA_DIR / \"support_tickets_raw.jsonl\"\n",
        "\n",
        "\n",
        "#def load_jsonl(p):\n",
        "#    return [json.loads(x) for x in p.read_text(encoding=\"utf-8\").splitlines()]\n",
        "\n",
        "#raw_tickets = load_jsonl(raw_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "raw_tickets[14]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "source": [
        "\n",
        "## 2.2 Data Anonymization\n",
        "\n",
        "The first step is protecting sensitive data by removing PII (Personally Identifiable Information) so that they never enter the system. PII includes details like names, emails, phone numbers, account numbers and IP addresses - anything that could identify a specific person. Our sanitizer replaces each sensitive element with a consistent placeholder. This way, the data still retains enough structure to be useful for analysis and retrieval.\n",
        "\n",
        "This approach has two key benefits:\n",
        "1. **Risk reduction**: even if a query slips through access control or a model tries to reveal more than it should, the sensitive data simply isn’t there to give away.\n",
        "2. **Utility preservation**: placeholders remain consistent across documents, so the system can still link related tickets or detect patterns without exposing private details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "# Define the token formats we’ll use for each PII type\n",
        "PLACEHOLDER_FMT = {\n",
        "    \"email\":  \"EMAIL_{:04d}\",\n",
        " #   \"phone\":  \"PHONE_{:04d}\",\n",
        "    \"account\":\"ACCT_{:04d}\",\n",
        "    \"ip\":     \"IP_{:04d}\",\n",
        "    \"person\": \"PERSON_{:04d}\",\n",
        "}\n",
        "\n",
        "# Global token vault:\n",
        "# Keeps a STABLE, GLOBAL mapping: original value -> placeholder (per type)\n",
        "# Ensures the same email, etc. always maps to the same token across tickets\n",
        "class TokenVault:\n",
        "    \"\"\"\n",
        "    Maps original values -> stable placeholders, per type.\n",
        "    Example: vault.get_token(\"email\", \"alice@example.com\") -> \"{{EMAIL_0001}}\"\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self._maps: Dict[str, Dict[str, str]] = {t:{} for t in PLACEHOLDER_FMT}\n",
        "        self._counters: Dict[str, int] = {t:0 for t in PLACEHOLDER_FMT}\n",
        "\n",
        "    def _normalize_key(self, typ: str, value: str) -> str:\n",
        "        # Normalize by type (e.g., emails to lowercase)\n",
        "        v = value.strip()\n",
        "        if typ == \"email\":\n",
        "            v = v.lower()\n",
        "        elif typ == \"ip\":\n",
        "            v = v.strip()\n",
        "        elif typ == \"account\":\n",
        "            v = re.sub(r\"\\D\", \"\", v)\n",
        "        elif typ == \"person\":\n",
        "            v = re.sub(r\"\\s+\", \" \", v)\n",
        "        return v\n",
        "\n",
        "    def get_token(self, typ: str, value: str) -> str:\n",
        "        # Return the existing token for (typ,value) OR create a new one\n",
        "        # Final tokens are wrapped like \"{{EMAIL_0001}}\"\n",
        "        assert typ in PLACEHOLDER_FMT, f\"Unknown token type: {typ}\"\n",
        "        key = self._normalize_key(typ, value)\n",
        "        if not key:\n",
        "            return value\n",
        "        if key not in self._maps[typ]:\n",
        "            self._counters[typ] += 1\n",
        "            token = \"{{\" + PLACEHOLDER_FMT[typ].format(self._counters[typ]) + \"}}\"\n",
        "            self._maps[typ][key] = token\n",
        "        return self._maps[typ][key]\n",
        "\n",
        "    def stats(self) -> Dict[str, int]:\n",
        "        return {k: len(v) for k, v in self._maps.items()}\n",
        "\n",
        "\n",
        "# Regexes (free-text pass)\n",
        "# -------------------------\n",
        "# Patterns to find PII inside unstructured text fields (body, messages)\n",
        "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
        "# Long numbers that might be account IDs\n",
        "ACCOUNT_RE = re.compile(r\"\\b\\d{8,12}\\b\")  # generic long number (use with caution in free text)\n",
        "# IPv4 candidates in text. We’ll validate them with ipaddress before replacing.\n",
        "IP_CANDIDATE_RE = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
        "\n",
        "\n",
        "# Helpers\n",
        "# -------------------------\n",
        "# Find/replace emails in free text\n",
        "def replace_emails(text: str, vault: TokenVault) -> str:\n",
        "    return EMAIL_RE.sub(lambda m: vault.get_token(\"email\", m.group(0)), text)\n",
        "\n",
        "def replace_accounts_text(text: str, vault: TokenVault) -> str:\n",
        "    # Find/replace long numeric sequences as account IDs in free text.\n",
        "    def repl(m):\n",
        "        num = m.group(0)\n",
        "        return vault.get_token(\"account\", num)\n",
        "    return ACCOUNT_RE.sub(repl, text)\n",
        "\n",
        "def replace_ips_text(text: str, vault: TokenVault) -> str:\n",
        "    # Find/validate/replace IPv4 addresses in free text.\n",
        "    def repl(m):\n",
        "        cand = m.group(0)\n",
        "        try:\n",
        "            ipaddress.IPv4Address(cand)\n",
        "            return vault.get_token(\"ip\", cand)\n",
        "        except Exception:\n",
        "            return cand\n",
        "    return IP_CANDIDATE_RE.sub(repl, text)\n",
        "\n",
        "def anonymize_free_text(text: str, vault: TokenVault) -> str:\n",
        "    # Apply free-text anonymization\n",
        "    t = text\n",
        "    t = replace_emails(t, vault)\n",
        " #   t = replace_phones_text(t, vault)\n",
        "    t = replace_ips_text(t, vault)\n",
        "    t = replace_accounts_text(t, vault)\n",
        "    return t\n",
        "\n",
        "\n",
        "# Field-level anonymization\n",
        "# -------------------------\n",
        "STRUCTURED_FIELD_RULES = [\n",
        "    # Define which structured fields to anonymize, with an optional validator\n",
        "    ((\"contact_email\",), \"email\",  None),\n",
        "    ((\"account_number\",),\"account\", None),\n",
        "    ((\"source_ip\",),     \"ip\",     lambda s: _is_ipv4(s:=str(s))),\n",
        "    ((\"contact_name\",),  \"person\", None),\n",
        "]\n",
        "\n",
        "def _is_ipv4(s: str) -> bool:\n",
        "    # True if s is a valid IPv4 address\n",
        "    try:\n",
        "        ipaddress.IPv4Address(s)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def set_in(obj: Dict[str, Any], path: tuple, value: Any) -> None:\n",
        "    # Write a value at a dotted path in a dict (shallow)\n",
        "    d = obj\n",
        "    for p in path[:-1]:\n",
        "        d = d.get(p, {})\n",
        "    d[path[-1]] = value\n",
        "\n",
        "def get_in(obj: Dict[str, Any], path: tuple) -> Any:\n",
        "    # Read a value at a dotted path in a dict; return None if missing\n",
        "    d = obj\n",
        "    for p in path:\n",
        "        if not isinstance(d, dict) or p not in d:\n",
        "            return None\n",
        "        d = d[p]\n",
        "    return d\n",
        "\n",
        "def anonymize_ticket(ticket: Dict[str, Any], vault: TokenVault) -> Dict[str, Any]:\n",
        "    # Produce an anonymized copy of a single ticket:\n",
        "    # 1) Structured fields: precise tokenization using rules above.\n",
        "    # 2) Free-text fields: regex-based replacement (body + conversation messages)\n",
        "    t = json.loads(json.dumps(ticket))\n",
        "\n",
        "    # 1) Structured fields (email, account, IP, person)\n",
        "    for path, typ, validator in STRUCTURED_FIELD_RULES:\n",
        "        val = get_in(t, path)\n",
        "        if not isinstance(val, str):\n",
        "            continue\n",
        "        if validator is None or validator(val):\n",
        "            token = vault.get_token(typ, val)\n",
        "            set_in(t, path, token)\n",
        "\n",
        "    # 2) Free text fields (body)\n",
        "    if isinstance(t.get(\"body\"), str):\n",
        "        t[\"body\"] = anonymize_free_text(t[\"body\"], vault)\n",
        "\n",
        "    # 2b) Free text in conversation messages\n",
        "    conv = t.get(\"conversation\", [])\n",
        "    if isinstance(conv, list):\n",
        "        new_conv = []\n",
        "        for turn in conv:\n",
        "            if not isinstance(turn, dict):\n",
        "                new_conv.append(turn); continue\n",
        "            msg = turn.get(\"message\")\n",
        "            if isinstance(msg, str):\n",
        "                turn = dict(turn)\n",
        "                turn[\"message\"] = anonymize_free_text(msg, vault)\n",
        "            new_conv.append(turn)\n",
        "        t[\"conversation\"] = new_conv\n",
        "\n",
        "    return t\n",
        "\n",
        "def anonymize_corpus(tickets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    # Anonymize a list of tickets\n",
        "    vault = TokenVault()\n",
        "    out = [anonymize_ticket(t, vault) for t in tickets]\n",
        "    # You can inspect vault.stats() if you want counts per type\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "source": [
        "Now, we'll call `anonymize_corpus` function on \"raw_tickets\". This will return a new list of tickets where PII fields have been replaced with placeholders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "outputs": [],
      "source": [
        "sanitized = anonymize_corpus(raw_tickets)\n",
        "out_path = Path(\"support_tickets_sanitized.jsonl\")\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for doc in sanitized:\n",
        "        f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote:\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "source": [
        "To persist the anonymized data, write it out in JSON Lines:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  sanitized_path = \"support_tickets_sanitized.jsonl\"\n",
        "\n",
        "  def load_jsonl(file_path):\n",
        "      \"\"\"Load JSONL file\"\"\"\n",
        "      with open(file_path, 'r', encoding='utf-8') as f:\n",
        "          return [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "  # Load the sanitized tickets\n",
        "  sanitized_tickets = load_jsonl(sanitized_path)\n",
        "\n",
        "  print(f\"✅ Loaded {len(sanitized_tickets)} sanitized tickets\")"
      ],
      "metadata": {
        "id": "WSYobXDPZreg"
      },
      "id": "WSYobXDPZreg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "source": [
        "And now let's load them back take a look at one support ticket to confirm placeholders look right:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "outputs": [],
      "source": [
        "raw_tickets[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "sanitized_tickets[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "\n",
        "## 2.3 Access Control - Preparing Metadata\n",
        "\n",
        "This step is about defining the rules that decide who can see what. In practice, access control comes down to a few common checks:\n",
        "\n",
        "1. Tenant isolation (multi-customer separation):\n",
        "- Each ticket belongs to an organization (org).\n",
        "- If the current user is from Zephyr Telecom, they should never see tickets from BlueShield Bank.\n",
        "- Policy: ticket.org == user.org (unless the user is an internal analyst).\n",
        "\n",
        "\n",
        "2. Department scoping\n",
        "- Some tickets are HR-related, others IT, Security, etc.\n",
        "- If the user is only allowed to see HR tickets, you filter on ticket.department.\n",
        "- Policy: ticket.department in user.allowed_departments.\n",
        "  \n",
        "3. Visibility flags (internal vs. customer-facing)\n",
        "  \n",
        "- Some data is internal-only, some can be shown to customers.\n",
        "- Example: add a customer_visible: true/false field.\n",
        "- Policy: if the user is a customer, they only see tickets where customer_visible == true.\n",
        "\n",
        "\n",
        "\n",
        "The below code will take sanitized tickets and attach access-control metadata that we'll need later for filtering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "def add_metadata(sanitized_tickets):\n",
        "    enriched = []\n",
        "    for t in sanitized_tickets:\n",
        "        t2 = dict(t)  # making a copy\n",
        "        t2[\"metadata\"] = {\n",
        "            \"org\": t.get(\"org\"),\n",
        "            \"department\": t.get(\"department\"),\n",
        "            \"customer_visible\": True,\n",
        "            \"tags\": t.get(\"tags\", []),\n",
        "        }\n",
        "        enriched.append(t2)\n",
        "    return enriched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "# Adding metadata to sanitized tickets\n",
        "enriched_tickets = add_metadata(sanitized_tickets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "outputs": [],
      "source": [
        "# Inspecting\n",
        "enriched_tickets[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "## 2.4 Preparing for Embeddings\n",
        "\n",
        "Now that our tickets are sanitized and enriched with metadata, the next step is to prepare them for embeddings. Instead of embedding an entire ticket as one big block of text, we’ll **break it into smaller, more meaningful pieces**.\n",
        "\n",
        "Each ticket will become multiple rows:\n",
        "- 1 row for the ticket body\n",
        "- 1 row for each message in the conversation\n",
        "\n",
        "Treating these smaller chunks as separate rows has two big benefits:\n",
        "1. it **improves retrieval accuracy**: we can return the exact part of a ticket that matches a query\n",
        "2. it **respects model limitations on input length**\n",
        "\n",
        "Along with the text, each row will carry its own metadata, such as organization, department and visibility, which will later let us enforce access control when querying the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "outputs": [],
      "source": [
        "## Break it into smaller, more meaningful pieces\n",
        "\n",
        "def ticket_to_rows(t):\n",
        "    rows = []\n",
        "    tid = t[\"ticket_id\"]\n",
        "\n",
        "    # this is the per-chunk \"catalog metadata\"\n",
        "    base_md = {\n",
        "        **t[\"metadata\"],            # org, department, tags, customer_visible\n",
        "        \"ticket_id\": tid,           # for provenance & catalog writes\n",
        "        \"created_at\": t.get(\"created_at\"),\n",
        "    }\n",
        "\n",
        "    body = (t.get(\"body\") or \"\").strip()\n",
        "    if body:\n",
        "        rows.append({\n",
        "            \"id\": f\"{tid}:body\",\n",
        "            \"text\": body,\n",
        "            \"metadata\": {**base_md, \"part\": \"body\"}\n",
        "        })\n",
        "\n",
        "    for i, turn in enumerate(t.get(\"conversation\", [])):\n",
        "        msg = (turn or {}).get(\"message\", \"\").strip()\n",
        "        if not msg:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"id\": f\"{tid}:conv:{i}\",\n",
        "            \"text\": msg,\n",
        "            \"metadata\": {**base_md, \"part\": \"conversation\", \"role\": turn.get(\"role\"), \"idx\": i}\n",
        "        })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "# Apply ticket_to_rows() function to every ticket in the dataset\n",
        "rows = [r for t in enriched_tickets for r in ticket_to_rows(t)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "# Peek at the first few rows\n",
        "for r in rows[:5]:\n",
        "    print(json.dumps(r, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "## 2.5 Creating Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "source": [
        "We will use embedding model `all-MiniLM-L6-v2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device = \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "texts = [r[\"text\"] for r in rows]\n",
        "\n",
        "# Embedding in batches\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size = 64,\n",
        "    show_progress_bar = True,\n",
        "    convert_to_numpy = True,\n",
        "    normalize_embeddings = True\n",
        ")\n",
        "\n",
        "# Attaching embeddings back to rows as Python lists\n",
        "for r, vec in zip(rows, embeddings):\n",
        "    r[\"values\"] = vec.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "vectors = [\n",
        "    {\"id\": r[\"id\"], \"values\": r[\"values\"], \"metadata\": r[\"metadata\"]}\n",
        "    for r in rows\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "## 2.6 Upserting Data into Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Load API key from Google Colab secrets\n",
        "pinecone_client = Pinecone(api_key=userdata.get('PINECONE_API_KEY'))"
      ],
      "metadata": {
        "id": "fTuXMSVr4daN"
      },
      "id": "fTuXMSVr4daN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Client\n",
        "#pinecone_client = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "# Creating an Index\n",
        "pinecone_client.create_index(name = \"support-tickets-demo\",\n",
        "                             dimension = 384,\n",
        "                             metric = \"cosine\",\n",
        "                             spec = ServerlessSpec(\n",
        "                                 cloud = \"aws\",\n",
        "                                 region = \"us-east-1\"\n",
        "                             ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "outputs": [],
      "source": [
        "index = pinecone_client.Index(\"support-tickets-demo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": [],
      "source": [
        "BATCH = 200\n",
        "total = len(vectors)\n",
        "\n",
        "for i in range(0, total, BATCH):\n",
        "    batch = vectors[i:i+BATCH]\n",
        "    index.upsert(vectors=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "source": [
        "### 2.6.1 Storing Raw Data with SQLite\n",
        "\n",
        "In production, the copy of our documents would be in a primary data store, usually a relational database like Postgres/MySQL or object storage (S3/GCS) referenced from a small catalog table. Pinecone holds only the embeddings plus minimal filterable metadata and a pointer back to the source.\n",
        "\n",
        "In this demo we use `sqlite3` to create a simple local data store.\n",
        "\n",
        "Let's understand how LLM will actually use it:\n",
        "1. We'll query Pinecone with ACL filters and get back chunk IDs/pointers.\n",
        "2. We'll fetch the raw text for those IDs from sqlite3.\n",
        "3. We'll pass only those snippets into the LLM to compose the answer.\n",
        "\n",
        "So this sets us up for the next section of this notebook - constraining the model - where we instruct the LLM to answer strictly from those snippets.\n",
        "\n",
        "Let's create a content catalog which stores the raw text for each chunk (plus basic provenance like ticket ID, org, department and timestamps):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "CATALOG_PATH = Path(\"data/chunk_catalog.sqlite\")\n",
        "\n",
        "def init_catalog(db_path=CATALOG_PATH):\n",
        "    # Open (or create) the SQLite database file\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.executescript(\"\"\"\n",
        "    PRAGMA journal_mode = WAL;\n",
        "    PRAGMA synchronous = NORMAL;\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS chunks (\n",
        "      id         TEXT PRIMARY KEY,   -- e.g. \"87428682:conv:2\"\n",
        "      ticket_id  TEXT,\n",
        "      part       TEXT,               -- 'body' | 'conversation'\n",
        "      idx        INTEGER,            -- turn index for conversation\n",
        "      org        TEXT,\n",
        "      department TEXT,\n",
        "      created_at TEXT,\n",
        "      text       TEXT                -- raw chunk text lives here (not in Pinecone)\n",
        "    );\n",
        "\n",
        "    CREATE INDEX IF NOT EXISTS idx_chunks_ticket ON chunks(ticket_id);\n",
        "    CREATE INDEX IF NOT EXISTS idx_chunks_org_dept ON chunks(org, department);\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def write_chunks_to_catalog(rows, db_path=CATALOG_PATH):\n",
        "    # Insert/overwrite chunk rows produced by ticket_to_rows()\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    data = []\n",
        "    for r in rows:\n",
        "        md = r[\"metadata\"]\n",
        "        data.append((\n",
        "            r[\"id\"],\n",
        "            md.get(\"ticket_id\"),\n",
        "            md.get(\"part\"),\n",
        "            md.get(\"idx\"),\n",
        "            md.get(\"org\"),\n",
        "            md.get(\"department\"),\n",
        "            md.get(\"created_at\"),\n",
        "            r[\"text\"],\n",
        "        ))\n",
        "    cur.executemany(\n",
        "        \"INSERT OR REPLACE INTO chunks (id, ticket_id, part, idx, org, department, created_at, text) VALUES (?,?,?,?,?,?,?,?)\",\n",
        "        data\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Initialization + load the current batch of chunks\n",
        "init_catalog()\n",
        "write_chunks_to_catalog(rows)\n",
        "print(\"Catalog ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "source": [
        "Before we run real queries, we sanity-check the vector index: confirm the index is ready and try fetching a single known vector by ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "outputs": [],
      "source": [
        "# Index readiness\n",
        "desc = pinecone_client.describe_index(\"support-tickets-demo\")\n",
        "print(\"Index ready:\", desc.status.get(\"ready\"))\n",
        "\n",
        "probe_id = rows[0][\"id\"]\n",
        "fetched = index.fetch(ids=[probe_id])\n",
        "\n",
        "exists = probe_id in (fetched.vectors or {})\n",
        "print(\"Fetch exists in Pinecone:\", exists)\n",
        "\n",
        "if exists:\n",
        "    vec_obj = fetched.vectors[probe_id]\n",
        "    # Values\n",
        "    vals = vec_obj.values\n",
        "    print(\"Dimensions:\", len(vals))\n",
        "\n",
        "    # Metadata\n",
        "    md = vec_obj.metadata\n",
        "    print(\"Metadata:\", md)\n",
        "\n",
        "    # ACL filters\n",
        "    if md:\n",
        "        print(\"organization:\", md.get(\"org\"), \"| department:\", md.get(\"department\"),\n",
        "              \"| part:\", md.get(\"part\"), \"| idx:\", md.get(\"idx\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "# Check some organization/department we expect to exist\n",
        "cur.execute(\"SELECT COUNT(*) FROM chunks WHERE org=? AND department=?\", (\"Zephyr Telecom\", \"Security\"))\n",
        "print(\"Zephyr/Security chunks:\", cur.fetchone()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "## 2.7 Querying the Database\n",
        "\n",
        "**Accesss Control List Filter**:\n",
        "Now, we’ll run semantic queries against Pinecone and inspect returned matches. We'll specify who the user is (`user_org`), which departments they’re allowed to see (`allowed_depts`) and require that chunks be marked `customer_visible=True`. This filter is sent to Pinecone so retrieval already respects access control before any text is fetched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": [],
      "source": [
        "# Building ACL filter\n",
        "user_org = \"Zephyr Telecom\"\n",
        "allowed_depts = {\"Security\"}\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "We'll encode the question into an embedding with the same model used for our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "# Encoding the question\n",
        "question = \"Okta group membership was wrong; what was the fix?\"\n",
        "q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "Finally, we query the Pinecone index to return the top 8 matches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "# Querying\n",
        "res = index.query(\n",
        "    vector=q_vec,\n",
        "    top_k=8,\n",
        "    include_metadata=True,\n",
        "    filter=acl_filter\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "source": [
        "Each match shows the metadata we filtered on and the chunk type. Similarity scores can look \"low\" with short snippets but what matters is the relative ranking (body > relevant support turns > others):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "outputs": [],
      "source": [
        "# Returned matches\n",
        "matches = res.get(\"matches\", [])\n",
        "print(f\"Got {len(matches)} match(es).\")\n",
        "for m in matches:\n",
        "    mid = m[\"id\"]\n",
        "    score = m[\"score\"]\n",
        "    md = m.get(\"metadata\", {})\n",
        "    print(f\"{mid:25s}  score={score:.3f}  org={md.get('org')}  dept={md.get('department')}  part={md.get('part')}  role={md.get('role')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "source": [
        "We can try the same query with a different organization in the filter, such as \"BlueShield Bank\". Pinecone returned different chunks from BlueShield Bank tickets. That means our ACL filter is working: we’re not seeing Zephyr data anymore. We’re seeing BlueShield’s own relevant tickets about Okta-like issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "outputs": [],
      "source": [
        "# Building ACL filter\n",
        "user_org = \"BlueShield Bank\"\n",
        "allowed_depts = {\"Security\"}  # tweak to test\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Encoding the question\n",
        "question = \"Okta group membership was wrong; what was the fix?\"\n",
        "q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()\n",
        "\n",
        "# Querying\n",
        "res = index.query(\n",
        "    vector=q_vec,\n",
        "    top_k=8,\n",
        "    include_metadata=True,\n",
        "    filter=acl_filter\n",
        ")\n",
        "\n",
        "# Returned matches\n",
        "matches = res.get(\"matches\", [])\n",
        "print(f\"Got {len(matches)} match(es).\")\n",
        "for m in matches:\n",
        "    mid = m[\"id\"]\n",
        "    score = m[\"score\"]\n",
        "    md = m.get(\"metadata\", {})\n",
        "    print(f\"{mid:25s}  score={score:.3f}  org={md.get('org')}  dept={md.get('department')}  part={md.get('part')}  role={md.get('role')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "\n",
        "## 2.8 Constraining the Model\n",
        "\n",
        "We'll use this strategy to **tell the LLM exactly how it’s allowed to behave** like use only retrieved snippets, summarize rather than quote, avoid PII entirely and prefer neutral, generic wording. These guardrails don’t replace access control or anonymization, but they sharply reduce the chance of the model inventing specifics or echoing sensitive details. Then, if anything still slips through, the downstream redactor layer can catch it - we will cover that later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "source": [
        "We’ll create a function that takes matched results from Pinecone (matched IDs) and returns their texts from SQLite, keeping the same order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57",
      "metadata": {
        "id": "57"
      },
      "outputs": [],
      "source": [
        "CATALOG_PATH = Path(\"data/chunk_catalog.sqlite\")\n",
        "\n",
        "def fetch_texts_by_ids(ids, db_path=CATALOG_PATH):\n",
        "    # Fetch raw chunk text for a list of chunk IDs, preserving input order.\"\"\"\n",
        "    if not ids:\n",
        "        return {}\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    qmarks = \",\".join(\"?\" * len(ids))\n",
        "    cur.execute(f\"SELECT id, text FROM chunks WHERE id IN ({qmarks})\", ids)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    # Map found ids -> text\n",
        "    found = dict(rows)\n",
        "    # Preserve Pinecone order; missing ids map to \"\"\n",
        "    return {i: found.get(i, \"\") for i in ids}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58",
      "metadata": {
        "id": "58"
      },
      "source": [
        "Next, we'll create another function `build_context_from_matches()` that turns each Pinecone match into a small, numbered snippet such as `[1 | f5bfddc9:body]\\n<text>`. The numbers preserve relevance order and give the model (and us) stable citation handles like `[1], [2]` to reference in the answer. We trim long snippets to keep the prompt compact so the LLM focuses on the most useful details, and we skip any IDs that didn’t resolve to text to avoid noise.\n",
        "\n",
        "The result is a clean context: answers can cite exactly which chunk supported each claim, making the system more explainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59",
      "metadata": {
        "id": "59"
      },
      "outputs": [],
      "source": [
        "def build_context_from_matches(matches, id_to_text, max_chars=4000):\n",
        "    \"\"\"\n",
        "    Create labeled context blocks like:\n",
        "    [1 | f5bfddc9:body]\n",
        "    <snippet>\n",
        "\n",
        "    Returns (context_str, used_ids)\n",
        "    \"\"\"\n",
        "    blocks = []\n",
        "    used_ids = []\n",
        "    total = 0\n",
        "    for i, m in enumerate(matches, start=1):\n",
        "        cid = m[\"id\"]\n",
        "        txt = (id_to_text.get(cid) or \"\").strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        # Light truncation to avoid oversized prompts\n",
        "        snippet = txt if len(txt) <= 800 else (txt[:800] + \" …\")\n",
        "        block = f\"[{i} | {cid}]\\n{snippet}\"\n",
        "        if total + len(block) + 2 > max_chars:\n",
        "            break\n",
        "        blocks.append(block)\n",
        "        used_ids.append(cid)\n",
        "        total += len(block) + 2\n",
        "    return \"\\n\\n\".join(blocks), used_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60",
      "metadata": {
        "id": "60"
      },
      "source": [
        "Now let's finally see how we can constrain the model - we will set boundaries on what the LLM can use and how it should respond. Instead of letting it generate freely from its pretraining, we guide it with a strict system prompt and a small, curated set of retrieved snippets.\n",
        "\n",
        "The rules for a model are simple:\n",
        "- rely only on these snippets,\n",
        "- summarize rather than quote,\n",
        "- leave out personal or sensitive details,\n",
        "- cite snippet labels,\n",
        "- and abstain if no evidence is available.\n",
        "\n",
        "We will be working with `gpt-4o-mini` and create the helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61",
      "metadata": {
        "id": "61"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def answer_with_constrained_model(question, context):\n",
        "    # Send the constrained prompt + context to the model.\n",
        "    # Returns text or a fallback message if no context.\n",
        "\n",
        "    if not context.strip():\n",
        "        return \"No eligible context was found for this user. Please refine the query or check permissions.\"\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model = \"gpt-4o-mini\",\n",
        "        temperature = 0,\n",
        "        max_tokens = 450,\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    # RULES\n",
        "                    \"You are a helpful security support assistant. \"\n",
        "                    \"Answer ONLY using the provided context snippets. \"\n",
        "                    \"Summarize; do not quote verbatim. \"\n",
        "                    \"Do NOT include PII or specifics (no names, emails, phone numbers, IPs, account numbers, dates, project codes, or dollar figures). \"\n",
        "                    \"If specifics are essential, use generic placeholders like [REDACTED]. \"\n",
        "                    \"If the context is insufficient, say so and suggest safe next steps.\"\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"SOURCES:\\n{context}\\n\\n\"\n",
        "                    f\"QUESTION: {question}\\n\\n\"\n",
        "                    \"Answer in 3-5 sentences and cite sources by their bracket labels, e.g., [1], [2].\"\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    return resp.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62",
      "metadata": {
        "id": "62"
      },
      "source": [
        "Now, let's prepare ACL filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63",
      "metadata": {
        "id": "63"
      },
      "outputs": [],
      "source": [
        "# Vector search with ACL\n",
        "user_org = \"Zephyr Telecom\"\n",
        "allowed_depts = {\"Security\"}\n",
        "acl_filter = {\n",
        "    \"$and\": [\n",
        "        {\"org\": {\"$eq\": user_org}},\n",
        "        {\"department\": {\"$in\": list(allowed_depts)}},\n",
        "        {\"customer_visible\": {\"$eq\": True}},\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64",
      "metadata": {
        "id": "64"
      },
      "source": [
        "Then we turn the question into an embedding and run a similarity search with `top_k=8`, asking Pinecone to return metadata and applying the ACL filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65",
      "metadata": {
        "id": "65"
      },
      "outputs": [],
      "source": [
        "# User question\n",
        "question = \"I clicked a phishing link and my endpoint is acting weird\"\n",
        "\n",
        "# Encoding\n",
        "q_vec = model.encode([question], convert_to_numpy = True, normalize_embeddings = False)[0].tolist()\n",
        "\n",
        "# Querying\n",
        "res = index.query(\n",
        "    vector = q_vec,\n",
        "    top_k = 8,\n",
        "    include_metadata = True,\n",
        "    filter = acl_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66",
      "metadata": {
        "id": "66"
      },
      "source": [
        "Finally, let's put that together: matches → context → answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67",
      "metadata": {
        "id": "67"
      },
      "outputs": [],
      "source": [
        "# 1. We grab the returned chunk IDs in ranked order. These are just pointers, they don’t contain the raw text.\n",
        "matches = res.get(\"matches\", [])\n",
        "ids = [m[\"id\"] for m in matches]\n",
        "\n",
        "# 2. Fetch raw text for those IDs\n",
        "id_to_text = fetch_texts_by_ids(ids)\n",
        "\n",
        "# 3. Build labeled, trimmed context blocks\n",
        "context, used_ids = build_context_from_matches(matches, id_to_text, max_chars=4000)\n",
        "print(\"Context used:\\n\", context[:600], \"...\\n\",\"\\n-----------------\\n\")\n",
        "\n",
        "# 4. Ask the model under strict rules\n",
        "final_answer = answer_with_constrained_model(question, context)\n",
        "print(\"Model's answer: \",\"\\n\", final_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68",
      "metadata": {
        "id": "68"
      },
      "source": [
        "## 2.9 Output Filtering and Guardrails\n",
        "\n",
        "As a final guard, we'll implement \"redactor layer\" which complements our earlier layers (ACL-filtered retrieval + constrained prompting). The function below check the model's final answer, just before it's shown to the user. Its job is to **catch and redact sensitive information that might have slipped through** such as:\n",
        "\n",
        "- real names,\n",
        "- emails,\n",
        "- 8-12 digit account numbers,\n",
        "- IPv4/IPv6,\n",
        "- customer IDs like CUST-2407,\n",
        "- asset IDs like WS-1054"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69",
      "metadata": {
        "id": "69"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Emails\n",
        "EMAIL_RE = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "\n",
        "# 8–12 digit account-like numbers\n",
        "ACCOUNT_RE = re.compile(r'(?<!\\d)\\d{8,12}(?!\\d)')\n",
        "\n",
        "# Customer IDs like CUST-2407\n",
        "CUSTOMER_ID_RE = re.compile(r'\\bCUST-\\d{3,6}\\b', re.IGNORECASE)\n",
        "\n",
        "# Asset IDs like WS-1054 (two letters + dash + 3–6 digits)\n",
        "ASSET_ID_RE = re.compile(r'\\b[A-Z]{2,4}-\\d{3,6}\\b')\n",
        "\n",
        "# IPv4 0–255 per octet\n",
        "IPV4_RE = re.compile(\n",
        "    r'\\b(?:(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\.){3}'\n",
        "    r'(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\b'\n",
        ")\n",
        "\n",
        "# Basic IPv6 (not exhaustive but useful)\n",
        "IPV6_RE = re.compile(r'\\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\\b')\n",
        "\n",
        "# Very light \"name-like\" pattern (two capitalized words).\n",
        "NAME_RE = re.compile(r'\\b([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\b')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70",
      "metadata": {
        "id": "70"
      },
      "source": [
        "> NOTE to \"NAME_RE\": It is a simple heuristic (two Capitalized Words are treated as a person name). This can also hit non-person phrases like \"Active Sessions\" or \"Conditional Access\". To reduce false positives, we could:\n",
        "> - add a small whitelist of domain phrases (e.g., {\"Conditional Access\",\"Active Directory\"})\n",
        "> - only apply when \"person cues\" appear nearby (e.g., 'contact', 'reported by', or an email in the sentence)\n",
        "> - replace with a lightweight NER (e.g., spaCy PERSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71",
      "metadata": {
        "id": "71"
      },
      "outputs": [],
      "source": [
        " def redact_output(output: str) -> str:\n",
        "    \"\"\"\n",
        "    Last-mile guard: replace PII-ish patterns in the model's output with placeholders.\n",
        "    Designed around your dataset: emails, phones, 8–12 digit accounts, IPs, customer & asset IDs,\n",
        "    and (optionally) name-like strings.\n",
        "    \"\"\"\n",
        "    s = output\n",
        "\n",
        "    # Order can reduce false positives: more specific first, then broader\n",
        "    s = EMAIL_RE.sub('[REDACTED_EMAIL]', s)\n",
        "    s = IPV4_RE.sub('[REDACTED_IP]', s)\n",
        "    s = IPV6_RE.sub('[REDACTED_IP]', s)\n",
        "\n",
        "    s = CUSTOMER_ID_RE.sub('[REDACTED_CUSTOMER_ID]', s)\n",
        "    s = ASSET_ID_RE.sub('[REDACTED_ASSET_ID]', s)\n",
        "\n",
        "    s = ACCOUNT_RE.sub('[REDACTED_ACCOUNT]', s)\n",
        "\n",
        "    s = NAME_RE.sub('[REDACTED_NAME]', s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72",
      "metadata": {
        "id": "72"
      },
      "source": [
        "Let's tie all pieces together and returns the redacted answer plus the IDs that were actually used. The function will:\n",
        "\n",
        "1. **Apply ACL filter to retrieval**\n",
        "2. **Fetch raw text from SQLite and build labeled context**\n",
        "3. **Constrain model when generating answer and redact it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73",
      "metadata": {
        "id": "73"
      },
      "outputs": [],
      "source": [
        "def secure_rag_answer(\n",
        "    *,\n",
        "    question: str,\n",
        "    index,\n",
        "    model,\n",
        "    acl_filter: dict,\n",
        "    top_k: int = 8,\n",
        "    namespace: str | None = None,\n",
        "    max_context_chars: int = 4000,\n",
        "):\n",
        "    \"\"\"\n",
        "    End-to-end solution: ACL-filtered search -> fetch texts -> labeled context -> constrained LLM -> redaction\n",
        "    Returns (context, safe_answer)\n",
        "    \"\"\"\n",
        "    # 1) Encode and query Pinecone with ACLs\n",
        "    q_vec = model.encode([question], convert_to_numpy=True, normalize_embeddings=False)[0].tolist()\n",
        "    res = index.query(\n",
        "        vector = q_vec,\n",
        "        top_k = top_k,\n",
        "        include_metadata = True,\n",
        "        filter = acl_filter,\n",
        "        **({\"namespace\": namespace} if namespace is not None else {})\n",
        "    )\n",
        "    matches = res.get(\"matches\", [])\n",
        "    ids = [m[\"id\"] for m in matches]\n",
        "\n",
        "    # 2) Fetch raw text from SQLite and build labeled context\n",
        "    id_to_text = fetch_texts_by_ids(ids)\n",
        "    context, _used_ids = build_context_from_matches(matches, id_to_text, max_chars=max_context_chars)\n",
        "\n",
        "    # 3) Constrained answer, then redact\n",
        "    draft = answer_with_constrained_model(question, context)\n",
        "    safe_answer = redact_output(draft)\n",
        "\n",
        "    return context, safe_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74",
      "metadata": {
        "id": "74"
      },
      "outputs": [],
      "source": [
        "# User question\n",
        "question = \"I clicked a phishing link and my endpoint is acting weird\"\n",
        "\n",
        "context, safe_answer = secure_rag_answer(\n",
        "    question = question,\n",
        "    index = index,\n",
        "    model = model,\n",
        "    acl_filter = acl_filter,\n",
        "    top_k = 8,\n",
        "    namespace = None,\n",
        "    max_context_chars = 4000\n",
        ")\n",
        "\n",
        "print(\"Context used:\\n\", context, \"...\\n\",\"\\n-----------------\\n\")\n",
        "print(\"Model's answer:\\n\", safe_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75",
      "metadata": {
        "id": "75"
      },
      "outputs": [],
      "source": [
        "# User question\n",
        "question = \"Hi, my workstation popped up a malware alert from the endpoint agent.\"\n",
        "\n",
        "context, safe_answer = secure_rag_answer(\n",
        "    question = question,\n",
        "    index = index,\n",
        "    model = model,\n",
        "    acl_filter = acl_filter,\n",
        "    top_k = 8,\n",
        "    namespace = None,\n",
        "    max_context_chars = 4000\n",
        ")\n",
        "\n",
        "print(\"Context used:\\n\", context, \"...\\n\",\"\\n-----------------\\n\")\n",
        "print(\"Model's answer:\\n\", safe_answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}